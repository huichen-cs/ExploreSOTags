diff --git a/tag_recommend_code/src/FormatData/FormatDataFromTagChangeData.java b/tag_recommend_code/src/FormatData/FormatDataFromTagChangeData.java
index aa9cefe..c473170 100644
--- a/tag_recommend_code/src/FormatData/FormatDataFromTagChangeData.java
+++ b/tag_recommend_code/src/FormatData/FormatDataFromTagChangeData.java
@@ -59,7 +59,7 @@ public class FormatDataFromTagChangeData {
 			if(!new File(output).isDirectory())
 				new File(output).mkdirs();
 			
-			HashMap<String, ArrayList<String>> tag_doc = new HashMap();
+			HashMap<String, ArrayList<String>> tag_doc = new HashMap<>();
 			
 			Reader in = new InputStreamReader(new FileInputStream(inputData));
 			CSVParser parser = CSVFormat.EXCEL.withHeader().parse(in);
@@ -223,7 +223,7 @@ public class FormatDataFromTagChangeData {
 					map.get(tag).add(docId);
 			}
 			else{
-				ArrayList<String> list = new ArrayList();
+				ArrayList<String> list = new ArrayList<>();
 				list.add(docId);
 				map.put(tag, list);
 			}
diff --git a/tag_recommend_code/src/FormatData/GenerateDataForTagChange.java b/tag_recommend_code/src/FormatData/GenerateDataForTagChange.java
index d1cfe91..6d558c6 100644
--- a/tag_recommend_code/src/FormatData/GenerateDataForTagChange.java
+++ b/tag_recommend_code/src/FormatData/GenerateDataForTagChange.java
@@ -1,10 +1,9 @@
 package FormatData;
 
-import java.io.BufferedReader;
+
 import java.io.BufferedWriter;
 import java.io.File;
 import java.io.FileNotFoundException;
-import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.util.ArrayList;
@@ -20,23 +19,21 @@ import org.apache.commons.csv.CSVParser;
 import org.apache.commons.csv.CSVRecord;
 
 import org.apache.commons.io.*;
-import org.netlib.util.booleanW;
-import org.netlib.util.intW;
-import org.preprocess.htmlFilter;
+import org.preprocess.HtmlFilter;
 import org.preprocess.rawTextDataPreprocessor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import evaluate_EnTagRec_UA.RunKtimesForEffectSizeTest_onTagChangeData_old;
 
-import LabeledLDA.clearData;
 import LabeledLDA.generateDataset;
 import TermTagIndex.POS;
 import TermTagIndex.TermTagIndexBuilder;
 
-import scala.actors.threadpool.Arrays;
 import tagRecommend.Doc;
 
 public class GenerateDataForTagChange {
-	
+	private final static Logger LOGGER = LoggerFactory.getLogger(GenerateDataForTagChange.class);
 	
 	static String rawdata = "rawdata/";
 	static String cleandata= "descriptionCleaned/";
@@ -53,65 +50,69 @@ public class GenerateDataForTagChange {
 	
 	
 	
-	static public void main(String[] args){
-//		String inputData = "/scratch4/shaowei/data/data_and_results/ChangeTagData/askDifferent/originalData.csv";
-//
-//		String root = "/scratch4/shaowei/data/data_and_results/ChangeTagData/askDifferent/";
-		if(args.length <1)
-		{
+    static public void main(String[] args) {
+        // String inputData =
+        // "/scratch4/shaowei/data/data_and_results/ChangeTagData/askDifferent/originalData.csv";
+        //
+        // String root =
+        // "/scratch4/shaowei/data/data_and_results/ChangeTagData/askDifferent/";
+        if (args.length < 1) {
             System.out.println("usage [root]");
             return;
         }
 
         String root = args[0];
-		String inputData = root+ "/originalData.csv";
+        String inputData = root + "/originalData.csv";
+
+        try {
 
             // generate raw data
 
-		if(rawDataGeneration){
-			FormatDataFromTagChangeData.formatFromCSV(root, inputData,50);
-			//clearData.clearTags(root+"tag_doc.txt", root+"tag_doc"+"_"+50+".txt", 50);
-			System.out.println("raw data generation done");
+            if (rawDataGeneration) {
+                FormatDataFromTagChangeData.formatFromCSV(root, inputData, 50);
+                // clearData.clearTags(root+"tag_doc.txt", root+"tag_doc"+"_"+50+".txt", 50);
+                LOGGER.info("raw data generation done");
 
             }
 
             // prepare tags assoication rule input data
-		getTagsForAR(inputData,root+RunKtimesForEffectSizeTest_onTagChangeData_old.ARinputFile);
+            getTagsForAR(inputData, root + RunKtimesForEffectSizeTest_onTagChangeData_old.ARinputFile);
             System.out.println("generate the tags file for assoication rule");
 
             // preprocess the text
-		if(cleanDataGeneration)
-		{
-			rawTextDataPreprocessor.tokenCleanForDir(root+rawdata, root+cleandata);
-			System.out.println("clean data generation done");
+            if (cleanDataGeneration) {
+                rawTextDataPreprocessor.tokenCleanForDir(root + rawdata, root + cleandata);
+                LOGGER.info("clean data generation done");
             }
 
             // remove html
-		if(htmlFreeDataGeneration){
-			htmlFilter.tokenCleanForDir(root+rawdata, root+htmlFreeData);
-			System.out.println("html tag free data generation done");
+            if (htmlFreeDataGeneration) {
+                HtmlFilter.tokenCleanForDir(root + rawdata, root + htmlFreeData);
+                LOGGER.info("html tag free data generation done");
             }
 
+            // pos tagger
 
-		
-		//pos tagger
-		
-		if(pos){
-			POS.posTagForFolder(root+htmlFreeData , root + posData);
-			System.out.println("pos tagging done");
+            if (pos) {
+                POS.posTagForFolder(root + htmlFreeData, root + posData);
+                LOGGER.info("pos tagging done");
             }
 
-		if(termIndexGenration){
+            if (termIndexGenration) {
                 TermTagIndexBuilder.runOnglobal(root);
-			System.out.println("term index generated!");
+                LOGGER.info("term index generated!");
             }
 
             // generate csv for experiment
 
-		if(finalCSVDataGeneration){
-			generateDataset.generateCSVDataset(root,finalCSVDataset);
+            if (finalCSVDataGeneration) {
+                generateDataset.generateCSVDataset(root, finalCSVDataset);
+            }
+        } catch (FileNotFoundException e) {
+            LOGGER.error(e.getMessage(), e);
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
         }
-		
 
     }
 	
@@ -124,12 +125,12 @@ public class GenerateDataForTagChange {
 			if(!new File(output).isDirectory())
 				new File(output).mkdirs();
 			
-			HashMap<String, ArrayList<String>> tag_doc = new HashMap();
+			HashMap<String, ArrayList<String>> tag_doc = new HashMap<>();
 			
 			File file = new File(inputData);
 			String fileContent = FileUtils.readFileToString(file);
 			CSVParser parser = CSVParser.parse(fileContent,CSVFormat.EXCEL);
-			Iterator it = parser.iterator();
+			Iterator<CSVRecord> it = parser.iterator();
 			
 			it.next();
 			while(it.hasNext()){
@@ -140,7 +141,7 @@ public class GenerateDataForTagChange {
 		        String title = csvRecord.get(3);
 		        String content = csvRecord.get(4);
 		        String finalTag = csvRecord.get(2);
-		        String originalTag = csvRecord.get(1);
+//		        String originalTag = csvRecord.get(1);
 		        finalTag = finalTag.replace("<", "");
 		        addToTagHashMap(finalTag,id,tag_doc,">");
 //		        
@@ -187,7 +188,7 @@ public class GenerateDataForTagChange {
 					map.get(tag).add(docId);
 			}
 			else{
-				ArrayList<String> list = new ArrayList();
+				ArrayList<String> list = new ArrayList<>();
 				list.add(docId);
 				map.put(tag, list);
 			}
@@ -207,7 +208,7 @@ public class GenerateDataForTagChange {
 			File file = new File(inputData);
 			String fileContent = FileUtils.readFileToString(file);
 			CSVParser parser = CSVParser.parse(fileContent,CSVFormat.EXCEL);
-			Iterator it = parser.iterator();
+			Iterator<CSVRecord> it = parser.iterator();
 			
 			BufferedWriter bw = new BufferedWriter(new FileWriter(outputData));
 			
@@ -238,13 +239,12 @@ public class GenerateDataForTagChange {
 	
 	public static void getGivenAndExclusiveTags(String inputData,
 			HashMap<String, List<String>> givenTags,
-			HashMap<String, List<String>> exclusiveTagOnEvaluation, HashMap<String, Doc> goldenSet) {
-		// TODO Auto-generated method stub
-		try{
+			HashMap<String, List<String>> exclusiveTagOnEvaluation, HashMap<String, Doc> goldenSet) throws FileNotFoundException, IOException {
+
 			File file = new File(inputData);
 			String fileContent = FileUtils.readFileToString(file);
 			CSVParser parser = CSVParser.parse(fileContent,CSVFormat.EXCEL);
-			Iterator it = parser.iterator();
+			Iterator<CSVRecord> it = parser.iterator();
 			
 			it.next();
 			int count =0;
@@ -254,7 +254,7 @@ public class GenerateDataForTagChange {
 		        String id= csvRecord.get(0);
 	//	        String package_name = tokens[1];
 
-		        String finalTagstr = csvRecord.get(2);
+//		        String finalTagstr = csvRecord.get(2);
 		        
 		        String originalTagStr = csvRecord.get(1);
 				 originalTagStr = originalTagStr.replace("<", "");
@@ -283,10 +283,5 @@ public class GenerateDataForTagChange {
 			}
 			System.out.println(count);
 			parser.close();
-		}catch(Exception e){
-			e.printStackTrace();
-		}
-		
-		
 	}
 }
diff --git a/tag_recommend_code/src/LabeledLDA/clearData.java b/tag_recommend_code/src/LabeledLDA/clearData.java
index 12c6693..19f7203 100644
--- a/tag_recommend_code/src/LabeledLDA/clearData.java
+++ b/tag_recommend_code/src/LabeledLDA/clearData.java
@@ -8,10 +8,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 
-import org.netlib.util.intW;
-
-import scala.tools.nsc.doc.model.DocTemplateEntity;
-
 public class clearData {
 
 	/**
@@ -29,7 +25,7 @@ public class clearData {
 	}
 	public static HashMap<String, ArrayList<String>> clearTags(String intput, String output, int tag_thres) {
 		// TODO Auto-generated method stub
-		HashMap<String, ArrayList<String>> docId_tag = new HashMap();
+		HashMap<String, ArrayList<String>> docId_tag = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(intput));
 			BufferedWriter bw = new BufferedWriter(new FileWriter(output));
@@ -46,7 +42,7 @@ public class clearData {
 					if(docId_tag.containsKey(docId)){
 						docId_tag.get(docId).add(tag);
 					}else{
-						ArrayList<String> tagList = new ArrayList();
+						ArrayList<String> tagList = new ArrayList<>();
 						tagList.add(tag);
 						docId_tag.put(docId, tagList);
 					}
diff --git a/tag_recommend_code/src/LabeledLDA/generateDataset.java b/tag_recommend_code/src/LabeledLDA/generateDataset.java
index bf166a6..aeeb17f 100644
--- a/tag_recommend_code/src/LabeledLDA/generateDataset.java
+++ b/tag_recommend_code/src/LabeledLDA/generateDataset.java
@@ -10,19 +10,14 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 
-import org.netlib.util.doubleW;
-
-import com.sun.org.glassfish.external.statistics.AverageRangeStatistic;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class generateDataset {
-
-	/**
-	 * @param args
-	 * @throws IOException 
-	 */
+    private final static Logger LOGGER = LoggerFactory.getLogger(generateDataset.class);
     
 	static int docNumberInTag = 50;
-	public static void main(String[] args) throws IOException {
+	public static void main(String[] args) {
 		// TODO Auto-generated method stub
 		//String root = "D:\\Shaowei\\research\\Folksonomies\\stackoverflow\\";
 		//String root = "D:\\Shaowei\\research\\Folksonomies\\stackoverflow\\";
@@ -30,45 +25,50 @@ public class generateDataset {
 		//String root = "O:\\shaowei\\folksonomy\\AlltheFourDataset\\appleSource\\";
 		//String root = "G:/research/tag_recommendation/data_and_results/AlltheFourDataset/SuperUserSource/";
 		
-		if(args.length < 1){
+        if (args.length < 1) {
             System.out.println("usage [root path]");
-			return ;
+            return;
         }
         String root = args[0];
 
-		HashMap<String, ArrayList<String>> tag_docs = loadTags_doc(root+"tag_doc_50.txt");
-		st(tag_docs);
+        try {
+            HashMap<String, ArrayList<String>> tagDocs = loadTagsDoc(root + "tag_doc_50.txt");
+            logStatistics(tagDocs);
 
             generateCVSDataSet(root);
+        } catch (FileNotFoundException e) {
+            LOGGER.error(e.getMessage(), e);
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
+        }
 
 	}
 	
-	public static void  st(HashMap<String, ArrayList<String>> id_tags){
+	public static void  logStatistics(HashMap<String, ArrayList<String>> idTags){
 		int sum = 0;
 		int max = 0;
-		for(ArrayList<String> docsArrayList : id_tags.values()){
+		for(ArrayList<String> docsArrayList : idTags.values()){
 			sum += docsArrayList.size();
 			if(docsArrayList.size() >max){
 				max = docsArrayList.size();
 			}
 		}
 		
-		System.out.println((double)sum/(double)id_tags.size());
-		System.out.println(max);
+		LOGGER.info("sum/idTags.size() = " + (double)sum/(double)idTags.size());
+		LOGGER.info("max = " + max);
 	}
 	
-	public static void  generateCVSDataSet(String root){
+	public static void  generateCVSDataSet(String root) throws FileNotFoundException, IOException {
 		//String root = "G:/research/tag_recommendation/data_and_results/ChangeTagData/stackoverflow/";
-		System.out.println("start loading....");
+		LOGGER.info("start loading....");
 		HashMap<String, ArrayList<String>> id_tags = loadTags(root+"tag_doc_50.txt");
-		try{
-			BufferedWriter outputCSV = new BufferedWriter(new FileWriter(root+"dataset.csv"));
+		try (BufferedWriter outputCSV = new BufferedWriter(new FileWriter(root+"dataset.csv"))) {
 			String docDir = root+"descriptionCleaned";
 			
 			File[] docList = new File(docDir).listFiles();
 			
 			for(File f : docList){
-				System.out.println(f.getAbsolutePath());
+				LOGGER.info(f.getAbsolutePath());
 				String id = f.getName();
 				String content = readContentIntoOneLine(f.getAbsolutePath()).trim().replace(" +", " ");
 				ArrayList<String> tagList = id_tags.get(id);
@@ -83,46 +83,37 @@ public class generateDataset {
 				outputCSV.write(id+","+tags+","+content);
 				outputCSV.write("\n");
 			}
-			outputCSV.close();
-		}catch(Exception e){
-			e.printStackTrace();
 		}
 	}
 	
 
-	private static HashMap<String, ArrayList<String>> loadTags(String file) {
+	private static HashMap<String, ArrayList<String>> loadTags(String file) throws FileNotFoundException, IOException {
 		// TODO Auto-generated method stub
-		HashMap<String, ArrayList<String>> docId_tag = new HashMap();
-		try {
-			BufferedReader br = new BufferedReader(new FileReader(file));
+		HashMap<String, ArrayList<String>> docIdTag = new HashMap<>();
+		try (BufferedReader br = new BufferedReader(new FileReader(file))) {
 			String line = null;
 			while((line = br.readLine())!=null){
 				String[] sparts = line.split(":");
 				String tag = sparts[0];
 				String[] docs = sparts[1].split(",");
 				for(String docId : docs){
-					if(docId_tag.containsKey(docId)){
-						docId_tag.get(docId).add(tag);
+					if(docIdTag.containsKey(docId)){
+						docIdTag.get(docId).add(tag);
 					}else{
-						ArrayList<String> tagList = new ArrayList();
+						ArrayList<String> tagList = new ArrayList<>();
 						tagList.add(tag);
-						docId_tag.put(docId, tagList);
+						docIdTag.put(docId, tagList);
 					}
 				}
 			}
-			br.close();
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
 		} 
-		return docId_tag;
+		return docIdTag;
 	}
 	
-	private static HashMap<String, ArrayList<String>> loadTags_doc(String file) {
+	private static HashMap<String, ArrayList<String>> loadTagsDoc(String file) throws FileNotFoundException, IOException {
 		// TODO Auto-generated method stub
-		HashMap<String, ArrayList<String>> tag_docs = new HashMap();
-		try {
-			BufferedReader br = new BufferedReader(new FileReader(file));
+		HashMap<String, ArrayList<String>> tag_docs = new HashMap<>();
+		try (BufferedReader br = new BufferedReader(new FileReader(file))) {
 			String line = null;
 			while((line = br.readLine())!=null){
 				String[] sparts = line.split(":");
@@ -131,48 +122,39 @@ public class generateDataset {
 				ArrayList<String> docsArrayList = new ArrayList<>();
 				for(String docId : docs){
 					docsArrayList.add(docId);
-					
 				}
 				tag_docs.put(tag, docsArrayList);
 			}
-			br.close();
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
 		} 
 		return tag_docs;
 	}
 	
 	
-	public static String readContentIntoOneLine(String file){
+	public static String readContentIntoOneLine(String file) throws FileNotFoundException, IOException {
 		StringBuilder sb = new StringBuilder();
-		BufferedReader br;
-		try {
-			br = new BufferedReader(new FileReader(file));
+		
+		try (FileReader reader = new FileReader(file);
+		    BufferedReader br = new BufferedReader(reader)) {
 			String line = null;
 			while((line = br.readLine())!=null){
 				sb.append(line+" ");
 			}
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
 		} 
 		return sb.toString();
 	}
 
-	public static void generateCSVDataset(String root, String finalCSVDataset) {
+	public static void generateCSVDataset(String root, String finalCSVDataset) throws FileNotFoundException, IOException {
 		// TODO Auto-generated method stub
 		//String root = "G:/research/tag_recommendation/data_and_results/ChangeTagData/stackoverflow/";
-				System.out.println("start loading....");
+		LOGGER.info("start loading....");
 		HashMap<String, ArrayList<String>> id_tags = loadTags(root+"tag_doc_50.txt");
-				try{
-					BufferedWriter outputCSV = new BufferedWriter(new FileWriter(root+finalCSVDataset));
+		try (BufferedWriter outputCSV = new BufferedWriter(new FileWriter(root+finalCSVDataset))) {
 			String docDir = root+"descriptionCleaned";
 			
 			File[] docList = new File(docDir).listFiles();
 			
 			for(File f : docList){
-						System.out.println(f.getAbsolutePath());
+				LOGGER.info(f.getAbsolutePath());
 				String id = f.getName();
 				String content = readContentIntoOneLine(f.getAbsolutePath()).trim().replace(" +", " ");
 				ArrayList<String> tagList = id_tags.get(id);
@@ -187,9 +169,6 @@ public class generateDataset {
 				outputCSV.write(id+","+tags+","+content);
 				outputCSV.write("\n");
 			}
-					outputCSV.close();
-				}catch(Exception e){
-					e.printStackTrace();
 		}
 	}
 }
diff --git a/tag_recommend_code/src/Network/Graph.java b/tag_recommend_code/src/Network/Graph.java
index 85ebfab..2e3ac6d 100644
--- a/tag_recommend_code/src/Network/Graph.java
+++ b/tag_recommend_code/src/Network/Graph.java
@@ -1,13 +1,10 @@
 package Network;
 
 import java.io.BufferedReader;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashMap;
-import java.util.List;
 
 public class Graph {
 	private HashMap<String, Node> index;
@@ -16,13 +13,13 @@ public class Graph {
 	private int PLUSE_STEP = 1;
 	private boolean pluseValue = true;
 	public Graph(int pluseV) {
-		this.index = new HashMap();
-		this.edgeList = new HashMap();
+		this.index = new HashMap<>();
+		this.edgeList = new HashMap<>();
 		this.PLUSE_STEP = pluseV;
 	}
 	public Graph() {
-		this.index = new HashMap();
-		this.edgeList = new HashMap();
+		this.index = new HashMap<>();
+		this.edgeList = new HashMap<>();
 		
 	}
 
@@ -49,8 +46,7 @@ public class Graph {
 	}
 
 	public void createFromEdgeFile(String path) {
-		try {
-			BufferedReader br = new BufferedReader(new FileReader(path));
+		try (BufferedReader br = new BufferedReader(new FileReader(path))) {
 			String line = null;
 			while ((line = br.readLine()) != null) {
 				String[] sparts = line.split("\t");
@@ -154,20 +150,20 @@ public class Graph {
 	//		System.out.println(n.getName() + ":" +n.getWeight());
 		try{
 		compareNode comparator = new compareNode();
-		ArrayList<Node> orlist = new ArrayList( this.index.values());
-		ArrayList<Node> list = new ArrayList();
+		ArrayList<Node> orlist = new ArrayList<>( this.index.values());
+		ArrayList<Node> list = new ArrayList<>();
 		for(Node n :orlist)
 		{
 			list.add(n.clone());
 		}
 		Collections.sort(list, comparator); 
 		if(topK<list.size())
-			return new ArrayList(list.subList(0, topK));
+			return new ArrayList<>(list.subList(0, topK));
 		else
-			return new ArrayList(list);
+			return new ArrayList<>(list);
 		}catch(Exception e){
 			e.printStackTrace();
-			return new ArrayList();
+			return new ArrayList<>();
 		}
 	
 		
@@ -181,12 +177,12 @@ public class Graph {
 			//ArrayList<Node> list = new ArrayList( this.index.values());
 			Collections.sort(list, comparator); 
 			if(list.size() >topK)
-				return new ArrayList(list.subList(0, topK));
+				return new ArrayList<>(list.subList(0, topK));
 			else
 				return list;
 			}catch(Exception e){
 				e.printStackTrace();
-				return new ArrayList();
+				return new ArrayList<>();
 			}
 		}
 	
diff --git a/tag_recommend_code/src/Network/Node.java b/tag_recommend_code/src/Network/Node.java
index 8ef6ad0..812236e 100644
--- a/tag_recommend_code/src/Network/Node.java
+++ b/tag_recommend_code/src/Network/Node.java
@@ -37,13 +37,13 @@ public class Node {
 	public Node(String name,double weight){
 		this.name = name;
 		this.Weight = weight;
-		this.incomeNodes = new ArrayList(); 
-		this.outgoNodes = new ArrayList();
+		this.incomeNodes = new ArrayList<>(); 
+		this.outgoNodes = new ArrayList<>();
 		this.flag = false;
 	}
 	
 	public void addNode(Node n, String type){
-		if(type.equals(this.OUT))
+		if(type.equals(OUT))
 			this.outgoNodes.add(n);
 		else
 			this.incomeNodes.add(n);
diff --git a/tag_recommend_code/src/Network/generateNetwork.java b/tag_recommend_code/src/Network/generateNetwork.java
index a1acae8..7be180a 100644
--- a/tag_recommend_code/src/Network/generateNetwork.java
+++ b/tag_recommend_code/src/Network/generateNetwork.java
@@ -24,14 +24,14 @@ public class generateNetwork {
 
 	public generateNetwork(String path) {
 		this.datasetPath = path;
-		this.similarities = new HashMap();
-		this.tag_docs = new HashMap();
+		this.similarities = new HashMap<>();
+		this.tag_docs = new HashMap<>();
 	}
 
 	public void getNetwork() {
 		load();
 		// dump the tags into arraylist
-		ArrayList<Tag> tagList = new ArrayList();
+		ArrayList<Tag> tagList = new ArrayList<>();
 	
 		for(String tag : this.tag_docs.keySet()){
 			Tag t = new Tag(tag, this.tag_docs.get(tag));
@@ -78,7 +78,7 @@ public class generateNetwork {
 					if (this.tag_docs.containsKey(tag)) {
 						this.tag_docs.get(tag).add(docId);
 					} else {
-						ArrayList<String> docList = new ArrayList();
+						ArrayList<String> docList = new ArrayList<>();
 						docList.add(docId);
 						this.tag_docs.put(tag, docList);
 					}
diff --git a/tag_recommend_code/src/TagInferBasedonUser/InferBasedOnUser.java b/tag_recommend_code/src/TagInferBasedonUser/InferBasedOnUser.java
index 86ecee7..8465599 100644
--- a/tag_recommend_code/src/TagInferBasedonUser/InferBasedOnUser.java
+++ b/tag_recommend_code/src/TagInferBasedonUser/InferBasedOnUser.java
@@ -1,19 +1,29 @@
 package TagInferBasedonUser;
 
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.HashMap;
 
 import org.preprocess.QuestionInformationManager;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import queryExpansion.Query;
 import tagRecommend.Doc;
 
 public class InferBasedOnUser {
-	public QuestionInformationManager qiManager;
+    private final static Logger LOGGER = LoggerFactory.getLogger(InferBasedOnUser.class);
+    
+	private QuestionInformationManager qiManager;
 	
-	private String projctDir;
 	public InferBasedOnUser(String projectDir){
-		this.projctDir = projectDir;
 		//String projectDir = "F:\\shaowei\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\appleSource\\";
 		String postFile = projectDir + "posts.xml";
 		String metaFile = projectDir + "meta.txt";
@@ -25,14 +35,19 @@ public class InferBasedOnUser {
 		qiManager = new QuestionInformationManager();
 		try{
 			if(projectDir.contains("freecode")){
+			    LOGGER.info("freecode: processing " + userInfoFile);
 				this.qiManager.getContentAndTag(userInfoFile);
 			}else{
+			    LOGGER.info("non-freecode: processing " + metaFile + ", " + postFile);
 				this.qiManager.getRowIdToFileMap(metaFile);
 				this.qiManager.getContentAndTag(postFile, projectDir, readLineN);
 			}
 		}catch(Exception e){
-			e.printStackTrace();
+			LOGGER.error(e.getMessage(), e);
+		}
 	}
+	
+	public InferBasedOnUser() {
     }
 
     public void infer(ArrayList<Query> queryListForTraining, ArrayList<Query> queryListForTesting, HashMap<String,Doc> goldenSet){
@@ -48,7 +63,7 @@ public class InferBasedOnUser {
 			ArrayList<Query> listForTraining, HashMap<String,Doc> goldenSet) {
 		// TODO Auto-generated method stub
 		// get the tags of userid in training data
-		HashMap<String,Integer> tag_count = new HashMap();
+		HashMap<String,Integer> tag_count = new HashMap<>();
 		
 		int sum = 0;
 		String userId = this.qiManager.getDocId_User_Map().get(q.query_id);
@@ -87,7 +102,7 @@ public class InferBasedOnUser {
 	}
 
 	public HashMap<String, ArrayList<Integer>> buildeIndexFromUserToQuestion(ArrayList<Query> queryList){
-		HashMap<String, ArrayList<Integer>> index  = new HashMap();
+		HashMap<String, ArrayList<Integer>> index  = new HashMap<>();
 		
 		for(int i = 0; i < queryList.size();i++){
 			String userId = this.qiManager.getDocId_User_Map().get(queryList.get(i).query_id);
@@ -96,14 +111,34 @@ public class InferBasedOnUser {
 			if(index.containsKey(userId)){
 				index.get(userId).add(i);
 			}else{
-				ArrayList<Integer> list = new ArrayList();
+				ArrayList<Integer> list = new ArrayList<>();
 				list.add(i);
 				index.put(userId, list);
 			}
 		}
 
 		return index;
+	}
 
+    public static InferBasedOnUser loadFromFile(String root) throws IOException, ClassNotFoundException {
+        Path path = Paths.get(root, "qiManager.ser");
+        InferBasedOnUser obj = new InferBasedOnUser();
+        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(path.toString()))) {
+            obj.qiManager = (QuestionInformationManager) ois.readObject();
+        }
+        return obj;
     }
 
+    public void saveToFile(String root) throws IOException {
+        Path path = Paths.get(root, "qiManager.ser");
+        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(path.toString()))) {
+            oos.writeObject(qiManager);
+        }
+        LOGGER.info("Saved qiManager object to " + path.toString());
+    }
+    
+	public static boolean fileExists(String root) {
+        Path path = Paths.get(root, "qiManager.ser");
+        return Files.exists(path);
+	}
 }
diff --git a/tag_recommend_code/src/TagInferBasedonUser/userAges.java b/tag_recommend_code/src/TagInferBasedonUser/userAges.java
index d964439..b3f5cdc 100644
--- a/tag_recommend_code/src/TagInferBasedonUser/userAges.java
+++ b/tag_recommend_code/src/TagInferBasedonUser/userAges.java
@@ -1,7 +1,6 @@
 package TagInferBasedonUser;
 
 import java.text.DateFormat;
-import java.text.ParseException;
 import java.text.SimpleDateFormat;
 import java.util.Date;
 import java.util.HashMap;
@@ -17,10 +16,10 @@ public class userAges {
 		// TODO Auto-generated method stub
 		String projectDir = args[0];
 		//String projectDir = "F:\\shaowei\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\appleSource\\";
-		String postFile = projectDir + "posts.xml";
+//		String postFile = projectDir + "posts.xml";
 		String metaFile = projectDir + "meta.txt";
 		String userInfoFile = projectDir + "Users.xml";
-		int readLineN = 1000000;
+//		int readLineN = 1000000;
 		
 		 String sDate= "2008-12-31";
          DateFormat format = new SimpleDateFormat("yyyy-MM-dd");
diff --git a/tag_recommend_code/src/TermTagIndex/cmdProcessor.java b/tag_recommend_code/src/TermTagIndex/CmdProcessor.java
similarity index 77%
rename from tag_recommend_code/src/TermTagIndex/cmdProcessor.java
rename to tag_recommend_code/src/TermTagIndex/CmdProcessor.java
index 14262b3..1c15a5d 100644
--- a/tag_recommend_code/src/TermTagIndex/cmdProcessor.java
+++ b/tag_recommend_code/src/TermTagIndex/CmdProcessor.java
@@ -2,12 +2,12 @@ package TermTagIndex;
 
 import java.util.concurrent.Callable;
 
-public class cmdProcessor implements Callable {
+public class CmdProcessor implements Callable<Object> {
 
 	
 	String cmd ;
 	int id;
-	public cmdProcessor(String cmd, int id){
+	public CmdProcessor(String cmd, int id){
 		this.cmd = cmd;
 		this.id = id;
 	}
diff --git a/tag_recommend_code/src/TermTagIndex/POS.java b/tag_recommend_code/src/TermTagIndex/POS.java
index ee1bac5..8c0c5e9 100644
--- a/tag_recommend_code/src/TermTagIndex/POS.java
+++ b/tag_recommend_code/src/TermTagIndex/POS.java
@@ -6,31 +6,29 @@ package TermTagIndex;
 import java.io.File;
 import java.io.IOException;
 
-import java.util.concurrent.CompletionService;
-import java.util.concurrent.ExecutorCompletionService;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-
 import org.apache.commons.io.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import edu.stanford.nlp.tagger.maxent.MaxentTagger;
 
 public class POS {
+    private final static Logger LOGGER = LoggerFactory.getLogger(POS.class);
 	
     public static void main(String[] args) {
-		//String root = "O:\\shaowei\\folksonomy\\freecode\\";
-		//String root = "O:\\shaowei\\folksonomy\\AlltheFourDataset\\appleSource\\";
+        // String root = "O:\\shaowei\\folksonomy\\freecode\\";
+        // String root = "O:\\shaowei\\folksonomy\\AlltheFourDataset\\appleSource\\";
 
-		if(args.length < 1){
+        if (args.length < 1) {
             System.out.println("usage [root path]");
-			return ;
+            return;
         }
         String root = args[0];
+        String rawData = root + "htmlFilterred/";
+        String posData = root + "posdata/";
 
-		String rawData = root +"htmlFilterred/";
-		String posData = root +"posdata/";
-		
-		if(!new File(posData).isDirectory())
+        try {
+            if (!new File(posData).isDirectory())
                 new File(posData).mkdirs();
 
             MaxentTagger tagger = new MaxentTagger(
@@ -38,56 +36,53 @@ public class POS {
 
             File[] files = new File(rawData).listFiles();
 
-		for(File f : files){
-			try {
+            for (File f : files) {
                 String content = FileUtils.readFileToString(f);
 
-				String tagged = posTag(content,tagger);
+                String tagged = posTag(content, tagger);
 
-				String output = posData+f.getName();
+                String output = posData + f.getName();
 
                 FileUtils.writeStringToFile(new File(output), tagged, false);
-			} catch (IOException e) {
-				// TODO Auto-generated catch block
-				e.printStackTrace();
-			}
+                LOGGER.info(f.toString() + " -> " + output);
             }
 
             System.out.println("done");
-		
-		
-	}
-	
-	public static void posTagCMD(String posData, String rawData){
-		String POSroot = new File("tools\\stanford-postagger-2013-06-20\\").getAbsolutePath()+"\\";
-		
-		if(!new File(posData).isDirectory())
-			new File(posData).mkdirs();
-		
-		String prefixcmd = "java -mx300m -classpath "+POSroot+
-				"stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model "+
-				POSroot+"models\\english-left3words-distsim.tagger -textFile ";
-		//int nrOfProcessors = Runtime.getRuntime().availableProcessors();
-		ExecutorService eservice = Executors.newFixedThreadPool(5);
-		CompletionService < Object > cservice = new ExecutorCompletionService < Object > (eservice);
-	
-		
-		File[] files = new File(rawData).listFiles();
-		int id = 0;
-		for(File f : files){
-			System.out.println(id++);
-			
-			String input = f.getAbsolutePath();
-			String output = posData+f.getName();
-			if(new File(output).exists()&& new File(output).length() >0)
-				continue;
-			System.out.println(id++);
-			String cmd  ="cmd /c " + prefixcmd +"\"" +input +"\""+">"+"\"" + output +"\"";
-			System.out.println(cmd);
-			cservice.submit(new cmdProcessor(cmd,id));
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
         }
 	}
 	
+//	public static void posTagCMD(String posData, String rawData){
+//		String POSroot = new File("tools\\stanford-postagger-2013-06-20\\").getAbsolutePath()+"\\";
+//		
+//		if(!new File(posData).isDirectory())
+//			new File(posData).mkdirs();
+//		
+//		String prefixcmd = "java -mx300m -classpath "+POSroot+
+//				"stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model "+
+//				POSroot+"models\\english-left3words-distsim.tagger -textFile ";
+//		//int nrOfProcessors = Runtime.getRuntime().availableProcessors();
+//		ExecutorService eservice = Executors.newFixedThreadPool(5);
+//		CompletionService < Object > cservice = new ExecutorCompletionService < Object > (eservice);
+//	
+//		
+//		File[] files = new File(rawData).listFiles();
+//		int id = 0;
+//		for(File f : files){
+//			System.out.println(id++);
+//			
+//			String input = f.getAbsolutePath();
+//			String output = posData+f.getName();
+//			if(new File(output).exists()&& new File(output).length() >0)
+//				continue;
+//			System.out.println(id++);
+//			String cmd  ="cmd /c " + prefixcmd +"\"" +input +"\""+">"+"\"" + output +"\"";
+//			System.out.println(cmd);
+//			cservice.submit(new CmdProcessor(cmd,id));
+//		}
+//	}
+	
 	
 	public static String posTag(String input, MaxentTagger tagger){
 		 // Initialize the tagger
@@ -101,10 +96,7 @@ public class POS {
         return tagged;
 	}
 
-	public static void posTagForFolder(String rawData, String posData) {
-		// TODO Auto-generated method stub
-		
-		
+	public static void posTagForFolder(String rawData, String posData) throws IOException {
 		if(!new File(posData).isDirectory())
 			new File(posData).mkdirs();
 		
@@ -114,7 +106,6 @@ public class POS {
 		File[] files = new File(rawData).listFiles();
 		
 		for(File f : files){
-			try {
 			String content  = FileUtils.readFileToString(f);
 			
 			String tagged = posTag(content,tagger);
@@ -122,12 +113,8 @@ public class POS {
 			String output = posData+f.getName();
 			
 			FileUtils.writeStringToFile(new File(output), tagged, false);
-			} catch (IOException e) {
-				// TODO Auto-generated catch block
-				e.printStackTrace();
-			}
 		}
 		
-		System.out.println("done");
+		LOGGER.info("done");
 	}
 }
diff --git a/tag_recommend_code/src/TermTagIndex/Term.java b/tag_recommend_code/src/TermTagIndex/Term.java
index 909871f..348720b 100644
--- a/tag_recommend_code/src/TermTagIndex/Term.java
+++ b/tag_recommend_code/src/TermTagIndex/Term.java
@@ -10,12 +10,12 @@ public class Term {
 	
 	public Term(String term){
 		this.name = term;
-		this.tag_occur = new HashMap();
+		this.tag_occur = new HashMap<>();
 	}
 	
 	public double getTerm_IDF(){
 		if(this.term_IDF==0)
-			this.term_IDF= Math.log10(this.totalTag/this.tag_occur.size());
+			this.term_IDF= Math.log10(totalTag/this.tag_occur.size());
 		return this.term_IDF;
 	}
 	
diff --git a/tag_recommend_code/src/TermTagIndex/TermTagIndex.java b/tag_recommend_code/src/TermTagIndex/TermTagIndex.java
index 9ad32c8..ea9effc 100644
--- a/tag_recommend_code/src/TermTagIndex/TermTagIndex.java
+++ b/tag_recommend_code/src/TermTagIndex/TermTagIndex.java
@@ -1,7 +1,6 @@
 package TermTagIndex;
 
 import java.io.BufferedReader;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.IOException;
 import java.util.ArrayList;
@@ -19,10 +18,10 @@ public class TermTagIndex {
 	public static String INFER_NON_UNIQUE_TOKEN = "INFER_NON_UNIQUE_TOKEN";
 	
 	public TermTagIndex(){
-		this.termTag = new HashMap();
+		this.termTag = new HashMap<String, Term>();
 	}
 	
-	public void LoadIndexFromFile(String path){
+	public void loadIndexFromFile(String path){
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -30,7 +29,7 @@ public class TermTagIndex {
 				String[] temp = line.split(":");
 				String token = temp[0];
 				Term term = new Term(token);
-				HashMap<String, Integer> tags_occur_map = new HashMap();
+				HashMap<String, Integer> tags_occur_map = new HashMap<>();
 				String[] tags = temp[1].split(",");
 				for(String tag_occur : tags){
 					String[] splitStr = tag_occur.split("@");
@@ -46,9 +45,9 @@ public class TermTagIndex {
 		}
 	}
 	
-	public void AssignTags(Query q, String approach, int Topk) {
+	public void assignTags(Query q, String approach, int Topk) {
 		// TODO Auto-generated method stub
-		HashMap<String, Node> tagCands = new HashMap();
+		HashMap<String, Node> tagCands = new HashMap<>();
 		String queryText = q.getText();
 		double max = 1;
 		if(approach.equals(TermTagIndex.INFER_NON_UNIQUE_TOKEN)){
@@ -87,7 +86,7 @@ public class TermTagIndex {
 			n.setWeight(n.getWeight()/max);
 		
 		
-		ArrayList<Node> tagList = new ArrayList( tagCands.values());
+		ArrayList<Node> tagList = new ArrayList<>( tagCands.values());
 		ArrayList<Node> topK_tags = Graph.getTopKTags(Topk, tagList);
 		
 		for(Node tag: topK_tags){
diff --git a/tag_recommend_code/src/TermTagIndex/TermTagIndexBuilder.java b/tag_recommend_code/src/TermTagIndex/TermTagIndexBuilder.java
index 03d7cb4..068e241 100644
--- a/tag_recommend_code/src/TermTagIndex/TermTagIndexBuilder.java
+++ b/tag_recommend_code/src/TermTagIndex/TermTagIndexBuilder.java
@@ -3,24 +3,27 @@ package TermTagIndex;
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
 import java.io.File;
+import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 
-import org.preprocess.dataFilter;
-
-import scala.actors.threadpool.Arrays;
+import org.preprocess.DataFilter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class TermTagIndexBuilder {
+    private final static Logger LOGGER = LoggerFactory.getLogger(TermTagIndexBuilder.class);
 
 	/**
 	 * @param args
 	 */
 	String root;
 	static String[] tmp = { "NN", "NNP", "NNS", "NNPS" };
-	static ArrayList<String> remainPOS = new ArrayList(Arrays.asList(tmp));
+	static ArrayList<String> remainPOS = new ArrayList<String>(Arrays.asList(tmp));
 	static boolean preprocess = true;
 
 	public static void main(String[] args) {
@@ -29,30 +32,36 @@ public class TermTagIndexBuilder {
 		// String root =
 		// "O:/shaowei/folksonomy/AlltheFourDataset/appleSource/";
 		//String root = "G:/research/tag_recommendation/data_and_results/ChangeTagData/stackoverflow/";
-		if(args.length < 1){
+        if (args.length < 1) {
             System.out.println("usage [root path]");
-			return ;
+            return;
         }
         String root = args[0];
 
-		String tag_docFile = root + "tag_doc_50.txt";
+        String tagDocFile = root + "tag_doc_50.txt";
         // "test.txt";
         String dataDir = root + "posdata/";
         String dataDirAfterProcess = root + "posdata_preprocessed/";
+
+        try {
             if (preprocess) {
                 if (!new File(dataDirAfterProcess).isDirectory())
                     new File(dataDirAfterProcess).mkdirs();
                 preprocess(dataDir, dataDirAfterProcess);
             }
-		System.out.println(System.currentTimeMillis());
-		HashMap<String, HashMap<String, Integer>> term_tag_index = buildTermTagIndex(
-				1, tag_docFile, dataDirAfterProcess);
+            LOGGER.info("Preprocessed from " + dataDir + " to " + dataDirAfterProcess + " at "
+                    + System.currentTimeMillis() + " ms");
+            HashMap<String, HashMap<String, Integer>> termTagIndex = buildTermTagIndex(1, tagDocFile,
+                    dataDirAfterProcess);
             String outputPath = root + "term_tag_index.txt";
-		output(term_tag_index, outputPath);
-		System.out.println(System.currentTimeMillis());
+            output(termTagIndex, outputPath);
+            LOGGER.info("Output term tag index to " + outputPath + " at " + System.currentTimeMillis() + " ms");
+        } catch (FileNotFoundException e) {
+            LOGGER.error(e.getMessage(), e);
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
+        }
     }
-	
-	
 	
     public TermTagIndexBuilder(String root) {
         this.root = root;
@@ -60,15 +69,14 @@ public class TermTagIndexBuilder {
 
 	// remove the tokens don't need to be consided and do stemmeing, stop words
 	// removal
-	public static void preprocess(String inputDir, String outputDir) {
+    public static void preprocess(String inputDir, String outputDir) throws FileNotFoundException, IOException {
         File[] fileList = new File(inputDir).listFiles();
         int count = 1;
         for (File f : fileList) {
-			System.out.println(count++);
+            LOGGER.info("counter + " + count++);
             String content = readContentIntoOneLine(f.getAbsolutePath());
             String outFile = outputDir + "/" + f.getName();
-			try {
-				BufferedWriter bw = new BufferedWriter(new FileWriter(outFile));
+            try (BufferedWriter bw = new BufferedWriter(new FileWriter(outFile))) {
                 String[] terms = content.split(" ");
                 StringBuilder sb = new StringBuilder();
                 for (String term : terms) {
@@ -78,25 +86,19 @@ public class TermTagIndexBuilder {
                         sb.append(token + " ");
                     }
                 }
-				String resultStr = dataFilter.filter_Code(sb.toString());
+                String resultStr = DataFilter.filter_Code(sb.toString());
                 bw.write(resultStr);
-				bw.close();
-			} catch (Exception e) {
-				e.printStackTrace();
             }
         }
 
     }
 
-	public static void output(
-			HashMap<String, HashMap<String, Integer>> term_tag_index,
-			String outputPath) {
-		try {
-			BufferedWriter bw = new BufferedWriter(new FileWriter(outputPath));
+    public static void output(HashMap<String, HashMap<String, Integer>> term_tag_index, String outputPath)
+            throws FileNotFoundException, IOException {
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(outputPath))) {
             for (String term : term_tag_index.keySet()) {
                 bw.write(term + ":");
-				HashMap<String, Integer> tag_occurence = term_tag_index
-						.get(term);
+                HashMap<String, Integer> tag_occurence = term_tag_index.get(term);
 
                 for (String tag : tag_occurence.keySet()) {
                     int occur = tag_occurence.get(tag);
@@ -104,76 +106,65 @@ public class TermTagIndexBuilder {
                 }
                 bw.newLine();
             }
-
-			bw.close();
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-	}
-
-	public static HashMap<String, HashMap<String, Integer>> buildTermTagIndex(
-			String path, String dataDir) {
-		// token_ <tag, occurence>
-		HashMap<String, HashMap<String, Integer>> TermTagIndex = new HashMap();
-		try {
-			BufferedReader br = new BufferedReader(new FileReader(path));
-			String line = null;
-			while ((line = br.readLine()) != null) {
-
-				String[] sparts = line.split(":");
-				String tag = sparts[0];
-				System.out.println(tag);
-				String[] docs = sparts[1].split(",");
-				for (String docId : docs) {
-					String inputPath = dataDir + docId;
-					String content = readContentIntoOneLine(inputPath);
-					String[] terms = content.split(" +");
-					for (String term : terms) {
-						// if(isReamined(term)){
-						if (true) {
-							String[] tmp = term.split("_");
-							String token = tmp[0];
-							if (token.equals(""))
-								continue;
-							// if contain this term
-							if (TermTagIndex.containsKey(token)) {
-								HashMap<String, Integer> tagMap = TermTagIndex
-										.get(token);
-								// count the occurnece of the tag for the term
-								if (tagMap.containsKey(tag)) {
-									int nextOccur = tagMap.get(tag) + 1;
-									tagMap.put(tag, nextOccur++);
-								} else {
-									tagMap.put(tag, 1);
-								}
         }
-							// if don't contain this term
-							else {
-								HashMap<String, Integer> tagMap = new HashMap();
-								tagMap.put(tag, 1);
-								TermTagIndex.put(token, tagMap);
-							}
-						}
-					}
-				}
-			}
-			br.close();
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
     }
 
-		return TermTagIndex;
-	}
+//	public static HashMap<String, HashMap<String, Integer>> buildTermTagIndex(
+//			String path, String dataDir) throws FileNotFoundException, IOException {
+//		// token_ <tag, occurence>
+//		HashMap<String, HashMap<String, Integer>> termTagIndex = new HashMap<>();
+//		try (BufferedReader br = new BufferedReader(new FileReader(path))) {
+//			String line = null;
+//			while ((line = br.readLine()) != null) {
+//
+//				String[] sparts = line.split(":");
+//				String tag = sparts[0];
+//				System.out.println(tag);
+//				String[] docs = sparts[1].split(",");
+//				for (String docId : docs) {
+//					String inputPath = dataDir + docId;
+//					String content = readContentIntoOneLine(inputPath);
+//					String[] terms = content.split(" +");
+//					for (String term : terms) {
+//						// if(isReamined(term)){
+//						if (true) {
+//							String[] tmp = term.split("_");
+//							String token = tmp[0];
+//							if (token.equals(""))
+//								continue;
+//							// if contain this term
+//							if (termTagIndex.containsKey(token)) {
+//								HashMap<String, Integer> tagMap = termTagIndex
+//										.get(token);
+//								// count the occurnece of the tag for the term
+//								if (tagMap.containsKey(tag)) {
+//									int nextOccur = tagMap.get(tag) + 1;
+//									tagMap.put(tag, nextOccur++);
+//								} else {
+//									tagMap.put(tag, 1);
+//								}
+//							}
+//							// if don't contain this term
+//							else {
+//								HashMap<String, Integer> tagMap = new HashMap<>();
+//								tagMap.put(tag, 1);
+//								termTagIndex.put(token, tagMap);
+//							}
+//						}
+//					}
+//				}
+//			}
+//		}
+//
+//		return termTagIndex;
+//	}
 
 	public static HashMap<String, HashMap<String, Integer>> buildTermTagIndex(
-			int i, String path, String dataDir) {
+			int i, String path, String dataDir) throws FileNotFoundException, IOException {
 		// token_ <tag, occurence>
-		HashMap<String, HashMap<String, Integer>> TermTagIndex = new HashMap();
-		HashMap<String, ArrayList<String>> doc_tags = new HashMap();
-		try {
-			BufferedReader br = new BufferedReader(new FileReader(path));
+		HashMap<String, HashMap<String, Integer>> termTagIndex = new HashMap<>();
+		HashMap<String, ArrayList<String>> docTags = new HashMap<>();
+		try (BufferedReader br = new BufferedReader(new FileReader(path))) {
 			String line = null;
 			while ((line = br.readLine()) != null) {
 
@@ -182,22 +173,22 @@ public class TermTagIndexBuilder {
 				// System.out.println(tag);
 				String[] docs = sparts[1].split(",");
 				for (String docId : docs) {
-					if (doc_tags.containsKey(docId)) {
-						doc_tags.get(docId).add(tag);
+					if (docTags.containsKey(docId)) {
+						docTags.get(docId).add(tag);
 					} else {
-						ArrayList<String> tags = new ArrayList();
+						ArrayList<String> tags = new ArrayList<>();
 						tags.add(tag);
-						doc_tags.put(docId, tags);
+						docTags.put(docId, tags);
 					}
 				}
 			}
-			br.close();
+
 			File[] docs = new File(dataDir).listFiles();
-			for (File docId : docs) {
-				String inputPath = docId.getAbsolutePath();
+			for (File doc : docs) {
+				String inputPath = doc.getAbsolutePath();
 				String content = readContentIntoOneLine(inputPath);
 				String[] terms = content.split(" +");
-				ArrayList<String> tags = doc_tags.get(docId.getName());
+				ArrayList<String> tags = docTags.get(doc.getName());
 				if (tags == null)
 					continue;
 				for (String tag : tags) {
@@ -210,8 +201,8 @@ public class TermTagIndexBuilder {
 								continue;
 							// if contain this term
 
-							if (TermTagIndex.containsKey(token)) {
-								HashMap<String, Integer> tagMap = TermTagIndex
+							if (termTagIndex.containsKey(token)) {
+								HashMap<String, Integer> tagMap = termTagIndex
 										.get(token);
 								// count the occurnece of the tag for the term
 
@@ -225,21 +216,18 @@ public class TermTagIndexBuilder {
 							}
 							// if don't contain this term
 							else {
-								HashMap<String, Integer> tagMap = new HashMap();
+								HashMap<String, Integer> tagMap = new HashMap<>();
 								tagMap.put(tag, 1);
-								TermTagIndex.put(token, tagMap);
+								termTagIndex.put(token, tagMap);
 							}
 						}
 					}
 				}
 			}
 
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
 		} 
 
-		return TermTagIndex;
+		return termTagIndex;
 	}
 
 	private static boolean isReamined(String term) {
@@ -273,7 +261,7 @@ public class TermTagIndexBuilder {
 
 
 
-	public static void runOnglobal(String root) {
+	public static void runOnglobal(String root) throws FileNotFoundException, IOException {
 		// TODO Auto-generated method stub
 		String tag_docFile = root + "tag_doc_50.txt";
 		// "test.txt";
@@ -284,12 +272,14 @@ public class TermTagIndexBuilder {
 				new File(dataDirAfterProcess).mkdirs();
 			preprocess(dataDir, dataDirAfterProcess);
 		}
-		System.out.println(System.currentTimeMillis());
+        LOGGER.info("Preprocessed from " + dataDir + " to " + dataDirAfterProcess + " at "
+                + System.currentTimeMillis() + " ms");
 		HashMap<String, HashMap<String, Integer>> term_tag_index = buildTermTagIndex(
 				1, tag_docFile, dataDirAfterProcess);
 		String outputPath = root + "term_tag_index.txt";
 		output(term_tag_index, outputPath);
-		System.out.println(System.currentTimeMillis());
+        LOGGER.info("Output term tag index to " + outputPath + " at " + System.currentTimeMillis() + " ms");
+		
 	}
 
 }
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U.java b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U.java
index c43c98b..c0249f3 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U.java
@@ -24,7 +24,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import scala.actors.Exit;
 import topicinfer.*;
 import tagRecommend.*;
@@ -60,7 +60,7 @@ public class RunKtimesForEffectSizeTest_U {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 		// TODO Auto-generated method stub
@@ -88,7 +88,7 @@ public class RunKtimesForEffectSizeTest_U {
 
 			logFile = TestDir + "log_" + dateFormat.format(date) + ".txt";
 
-			HashMap<String, Double> final_results = new HashMap();
+			HashMap<String, Double> final_results = new HashMap<>();
 			// String root =
 			// "G:/research/tag_recommendation/folksonomy/freecode/";
 			// generate cross data
@@ -112,12 +112,12 @@ public class RunKtimesForEffectSizeTest_U {
 					String posPreprecessedDir = root + "posdata_preprocessed/";
 					
 
-					HashMap<String, Double> result_recall = new HashMap();
-					HashMap<String, Double> result_precision = new HashMap();
+					HashMap<String, Double> result_recall = new HashMap<>();
+					HashMap<String, Double> result_precision = new HashMap<>();
 
 					InferBasedOnUser inferBOU = new InferBasedOnUser(root);
 
-					// HashMap<String,Double> result_10= new HashMap();
+					// HashMap<String,Double> result_10= new HashMap<>();
 					// perform cross validation
 					try {
 						for (int i = 0; i < crossNumber; i++) {
@@ -147,7 +147,7 @@ public class RunKtimesForEffectSizeTest_U {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									trainingData, inferTopicForTrainingPath,
 									infer_topK);
-							ArrayList<Query> queryListForTraining = run
+							ArrayList<Query> queryListForTraining = Run
 									.loadQueryFromFile(
 											inferTopicForTrainingPath,
 											trainingData);
@@ -159,7 +159,7 @@ public class RunKtimesForEffectSizeTest_U {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									testData, inferTopicPathForTesting,
 									infer_topK);
-							ArrayList<Query> queryListForTesting = run
+							ArrayList<Query> queryListForTesting = Run
 									.loadQueryFromFile(
 											inferTopicPathForTesting, testData);
 
@@ -189,9 +189,9 @@ public class RunKtimesForEffectSizeTest_U {
 
 							// infer topic llda
 							if (topicInfer) {
-								run.loadInferredTopic(inferTopicPathForTesting,
+								Run.loadInferredTopic(inferTopicPathForTesting,
 										queryListForTesting);
-								run.loadInferredTopic(
+								Run.loadInferredTopic(
 										inferTopicForTrainingPath,
 										queryListForTraining);
 							}
@@ -199,17 +199,17 @@ public class RunKtimesForEffectSizeTest_U {
 							// infer topic from term_tag_index
 							if (termTagIndex) {
 								TermTagIndex tti = new TermTagIndex();
-								tti.LoadIndexFromFile(root
+								tti.loadIndexFromFile(root
 										+ "term_tag_index.txt");
 								for (Query q : queryListForTesting) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
 								}
 
 								for (Query q : queryListForTraining) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
@@ -385,7 +385,7 @@ public class RunKtimesForEffectSizeTest_U {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -394,7 +394,7 @@ public class RunKtimesForEffectSizeTest_U {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -435,7 +435,7 @@ public class RunKtimesForEffectSizeTest_U {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -446,7 +446,7 @@ public class RunKtimesForEffectSizeTest_U {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -483,8 +483,8 @@ public class RunKtimesForEffectSizeTest_U {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -545,8 +545,8 @@ public class RunKtimesForEffectSizeTest_U {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -570,7 +570,7 @@ public class RunKtimesForEffectSizeTest_U {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -579,7 +579,7 @@ public class RunKtimesForEffectSizeTest_U {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -619,7 +619,7 @@ public class RunKtimesForEffectSizeTest_U {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U_onPlusestep.java b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U_onPlusestep.java
index aea917b..3a9efc5 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U_onPlusestep.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_U_onPlusestep.java
@@ -3,7 +3,6 @@ package evaluate_EnTagRec_U;
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
 import java.io.File;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
@@ -13,19 +12,14 @@ import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashMap;
 
-import org.preprocess.QuestionInformationManager;
-
-import edu.stanford.nlp.tmt.model.llda.*;
 import Network.Graph;
-import Network.Node;
 import Network.Tag;
 import Network.generateNetwork;
 import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
-import scala.actors.Exit;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -60,7 +54,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 		// TODO Auto-generated method stub
@@ -93,7 +87,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 
 			logFile = TestDir + "log_" + dateFormat.format(date) + ".txt";
 
-			HashMap<String, Double> final_results = new HashMap();
+			HashMap<String, Double> final_results = new HashMap<>();
 			// String root =
 			// "G:/research/tag_recommendation/folksonomy/freecode/";
 			// generate cross data
@@ -117,12 +111,12 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 					String posPreprecessedDir = root + "posdata_preprocessed/";
 					
 
-					HashMap<String, Double> result_recall = new HashMap();
-					HashMap<String, Double> result_precision = new HashMap();
+					HashMap<String, Double> result_recall = new HashMap<>();
+					HashMap<String, Double> result_precision = new HashMap<>();
 
 					InferBasedOnUser inferBOU = new InferBasedOnUser(root);
 
-					// HashMap<String,Double> result_10= new HashMap();
+					// HashMap<String,Double> result_10= new HashMap<>();
 					// perform cross validation
 					try {
 						for (int i = 0; i < crossNumber; i++) {
@@ -152,7 +146,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									trainingData, inferTopicForTrainingPath,
 									infer_topK);
-							ArrayList<Query> queryListForTraining = run
+							ArrayList<Query> queryListForTraining = Run
 									.loadQueryFromFile(
 											inferTopicForTrainingPath,
 											trainingData);
@@ -164,7 +158,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									testData, inferTopicPathForTesting,
 									infer_topK);
-							ArrayList<Query> queryListForTesting = run
+							ArrayList<Query> queryListForTesting = Run
 									.loadQueryFromFile(
 											inferTopicPathForTesting, testData);
 
@@ -194,9 +188,9 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 
 							// infer topic llda
 							if (topicInfer) {
-								run.loadInferredTopic(inferTopicPathForTesting,
+								Run.loadInferredTopic(inferTopicPathForTesting,
 										queryListForTesting);
-								run.loadInferredTopic(
+								Run.loadInferredTopic(
 										inferTopicForTrainingPath,
 										queryListForTraining);
 							}
@@ -204,17 +198,17 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 							// infer topic from term_tag_index
 							if (termTagIndex) {
 								TermTagIndex tti = new TermTagIndex();
-								tti.LoadIndexFromFile(root
+								tti.loadIndexFromFile(root
 										+ "term_tag_index.txt");
 								for (Query q : queryListForTesting) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
 								}
 
 								for (Query q : queryListForTraining) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
@@ -391,7 +385,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -400,7 +394,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -441,7 +435,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -452,7 +446,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -489,8 +483,8 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -551,8 +545,8 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -576,7 +570,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -585,7 +579,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -625,7 +619,7 @@ public class RunKtimesForEffectSizeTest_U_onPlusestep {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_on_b_f_individual.java b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_on_b_f_individual.java
index 5a1ffff..3f765a0 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_on_b_f_individual.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U/RunKtimesForEffectSizeTest_on_b_f_individual.java
@@ -24,7 +24,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -54,7 +54,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 		// TODO Auto-generated method stub
@@ -82,7 +82,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 
 			logFile = TestDir + "log_" + dateFormat.format(date) + ".txt";
 
-			HashMap<String, Double> final_results = new HashMap();
+			HashMap<String, Double> final_results = new HashMap<>();
 			// String root =
 			// "G:/research/tag_recommendation/folksonomy/freecode/";
 			// generate cross data
@@ -108,11 +108,11 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 				
 				String posPreprecessedDir = root + "posdata_preprocessed/";
 
-				HashMap<String, Double> result_recall_5 = new HashMap();
-				HashMap<String, Double> result_precision_5 = new HashMap();
-				HashMap<String, Double> result_recall_10 = new HashMap();
-				HashMap<String, Double> result_precision_10 = new HashMap();
-				// HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result_recall_5 = new HashMap<>();
+				HashMap<String, Double> result_precision_5 = new HashMap<>();
+				HashMap<String, Double> result_recall_10 = new HashMap<>();
+				HashMap<String, Double> result_precision_10 = new HashMap<>();
+				// HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -142,7 +142,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -152,7 +152,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 								+ "/query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -182,24 +182,24 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -381,7 +381,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -390,7 +390,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -431,7 +431,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -442,7 +442,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -479,8 +479,8 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -541,8 +541,8 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -566,7 +566,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -575,7 +575,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -615,7 +615,7 @@ public class RunKtimesForEffectSizeTest_on_b_f_individual {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U/TestonKvary_recall_precision.java b/tag_recommend_code/src/evaluate_EnTagRec_U/TestonKvary_recall_precision.java
index 3e2858c..eb997d3 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_U/TestonKvary_recall_precision.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U/TestonKvary_recall_precision.java
@@ -24,7 +24,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -53,11 +53,11 @@ public class TestonKvary_recall_precision {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
-		HashMap<String, Double> final_results = new HashMap();
+		HashMap<String, Double> final_results = new HashMap<>();
 		//String root = "G:\\research\\tag_recommendation\\folksonomy\\freecode\\";
 		String root =		 "G:\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\appleSource\\";
 		//String root =		 "G:\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\askubuntuSource\\";
@@ -86,13 +86,13 @@ public class TestonKvary_recall_precision {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result_recall = new HashMap();
-				HashMap<String,Double> result_precision = new HashMap();
+				HashMap<String, Double> result_recall = new HashMap<>();
+				HashMap<String,Double> result_precision = new HashMap<>();
 				
 				InferBasedOnUser inferBOU = new InferBasedOnUser(root);
 				
 				
-				//HashMap<String,Double> result_10= new HashMap();
+				//HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -121,7 +121,7 @@ public class TestonKvary_recall_precision {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -131,7 +131,7 @@ public class TestonKvary_recall_precision {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -161,24 +161,24 @@ public class TestonKvary_recall_precision {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -331,7 +331,7 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -340,7 +340,7 @@ public class TestonKvary_recall_precision {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -381,7 +381,7 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -392,7 +392,7 @@ public class TestonKvary_recall_precision {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -429,8 +429,8 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -491,8 +491,8 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -516,7 +516,7 @@ public class TestonKvary_recall_precision {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -525,7 +525,7 @@ public class TestonKvary_recall_precision {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -565,7 +565,7 @@ public class TestonKvary_recall_precision {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_U2.java b/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_U2.java
new file mode 100644
index 0000000..01a1c5b
--- /dev/null
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_U2.java
@@ -0,0 +1,693 @@
+package evaluate_EnTagRec_U2;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Properties;
+import java.util.Set;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import Network.Graph;
+import Network.Tag;
+import Network.generateNetwork;
+import TagInferBasedonUser.InferBasedOnUser;
+import TermTagIndex.TermTagIndex;
+import TermTagIndex.TermTagIndexBuilder;
+import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
+import evaluate_EnTagRec_U.IOManager;
+import queryExpansion.Query;
+import queryExpansion.Run;
+import tagRecommend.Doc;
+import tagRecommend.LinearCombinationTrainerAndTester;
+import tagRecommend.tenCrossValidation;
+import topicinfer.TopicInfer;
+import topicinfer.topicEstimate;
+
+public class RunKtimesForEffectSizeTest_U2 {
+    private final static Logger LOGGER = LoggerFactory.getLogger(RunKtimesForEffectSizeTest_UH.class);
+    /**
+     * @param args
+     */
+    static String logFile = null;
+    // generate 10 cross fold for cross validation test
+    static boolean generateCrossData = true;
+    static boolean buildInferBasedOnUserFromFile = true;
+    // estimated the Labeled-LDA from the training data
+    static boolean estimated = true;
+    // generate the network
+    static boolean generateNetwork = true;
+
+    // query expansion or not
+    static boolean expansion = true;
+    // infer tag from llda
+    static boolean topicInfer = true;
+    // infer tag from frequentist
+    static boolean termTagIndex = true;
+    // infer tag from user information
+    static final int crossNumber = 10;
+    static int infer_topK = 70;
+    static int termTagIndex_topK = 70;
+    // static String train = "trainDataset.csv";
+    static String train = "trainDataset_distr.csv";
+    static String test = "testDataset.csv";
+    static String golden = "goldenSet.csv";
+    static ArrayList<Tag> tag_doc = new ArrayList<>();
+    static int numOfRepetitions = 2;
+    static int[] returnTopKs = { 5, 10 };
+    
+    static String linearCombineParasFile = null;
+    static double[] linearCombineParas = {1.0, 0.1, 0.05, 0};
+
+    public static void main(String[] args) throws IOException, ClassNotFoundException {
+
+        if (args.length < 1) {
+            System.out.println("usage [root path]");
+            return;
+        }
+        String root = args[0];
+        
+        if (args.length >= 2) {
+            settingParametersFromPropertyFile(args[1]);
+        }
+        
+        if (linearCombineParasFile != null) {
+            setLinearCombineParasFromFile(linearCombineParasFile);
+        }
+
+        boolean[] userCom = { true, false };
+
+        boolean headerWritten = false;
+        try (BufferedWriter bwRepeat = new BufferedWriter(new FileWriter(root + "repeat_precision_recall.csv"))) {
+            // precision/5/true recall/5/true; p/5/false r/5 false; p/10/true r/10/true; p/10/false r/10/false
+            // bwRepeat.write("id,EnRecTagU_P@5,r@5,EnRecTag_P@5,r@5,EnRecTagU_P@10,r@10,EnRecTag_P@10,r@10\n");
+            for (int repeatI = 0; repeatI < numOfRepetitions; repeatI++) {
+                
+                HashMap<String, Double> iterResults = runExperiment(root, repeatI, userCom, returnTopKs);
+                Set<String> keySet = iterResults.keySet();
+                if (!headerWritten) {
+                    bwRepeat.write("id,");
+                    for (String key: keySet) {
+                        bwRepeat.write(key + ",");
+                    }
+                    bwRepeat.write("\n");
+                    headerWritten = true;
+                }
+                bwRepeat.write(repeatI+",");
+                for (String key : keySet) {
+                    bwRepeat.write(iterResults.get(key)+ ",");
+                }
+                bwRepeat.write("\n"); 
+            }
+        }
+    }
+    
+    public static HashMap<String, Double> runExperiment(String root, int repeatI, boolean[] userCom,
+            int[] returnTopKs) throws IOException, ClassNotFoundException {
+        HashMap<String, Double> iterResultMap = new LinkedHashMap<String, Double>();
+        
+        LOGGER.info("Experiment repetition no = " + repeatI);
+        
+        String inputData = root + "dataset.csv";
+        String testDir = root + "testcase_repeat" +repeatI  +"/";
+        LOGGER.info("Starting with root = " + root + " and inputData = " + inputData);
+
+        // load golden set
+        HashMap<String, Doc> goldenSet = loadGoldset(inputData);
+        LOGGER.info("loaded gold dataset from " + inputData);
+
+        //  load user info
+        InferBasedOnUser inferBOU = null;
+        if (buildInferBasedOnUserFromFile || !InferBasedOnUser.fileExists(root)) {
+            if (!buildInferBasedOnUserFromFile && !InferBasedOnUser.fileExists(root)) {
+                LOGGER.info("InferBaseOnUser seralized object does not exist, rebuilding ...");
+            }
+            inferBOU = new InferBasedOnUser(root);
+            inferBOU.saveToFile(root);
+        }        
+        LOGGER.info("Created or loaded the InferBasedOnUser object");
+
+        
+        // generate cross data
+        if (generateCrossData) {
+            generateTestDataDistribution(inputData,
+                    tenCrossValidation.crossNumber, testDir);
+        }
+        LOGGER.info("Generated cross validation data sets from " + inputData + " at " + testDir);
+
+
+        
+        // run both inferBasedOnUser setting to false and true
+        HashMap<String, Double> final_results = new HashMap<>();
+        for (int returnTopK : returnTopKs) {
+            for (boolean inferBasedOnUser: userCom) {
+                LOGGER.info("Run experiment with returnTopK = " + returnTopK + " userCom = " + userCom);
+                
+                // recover inferBOU object
+                inferBOU = InferBasedOnUser.loadFromFile(root);
+                
+                // perform cross validation
+                HashMap<String, Double> result_recall = new HashMap<>();
+                HashMap<String, Double> result_precision = new HashMap<>();
+                String posPreprecessedDir = root + "posdata_preprocessed/";
+                performCrossValidation(root, testDir, posPreprecessedDir, goldenSet, inferBOU, returnTopK,
+                        inferBasedOnUser, result_recall, result_precision);
+                LOGGER.info("Completed the cross validation.");
+                
+                HashMap<String, Double> averageMap = computeAverageRecallPrecision(result_recall, result_precision);
+                String key = "EnRecTag" +  (inferBasedOnUser?"U":"") + "_P@" + returnTopK;
+                iterResultMap.put(key, averageMap.get("precision"));
+                key = "EnRecTag" +  (inferBasedOnUser?"U":"") + "_R@" + returnTopK;
+                iterResultMap.put(key, averageMap.get("recall"));
+                LOGGER.info("final average precision and recall:" + averageMap.get("precision") + "\t"
+                        + averageMap.get("recall"));
+                
+                String resultOutput = testDir + "/ubf_recall_precision"
+                        + "@" + returnTopK + "_" + infer_topK + "_"
+                        + termTagIndex_topK + ".csv";
+                outputUbfRecallPrecisionForParas(resultOutput, result_recall, result_precision);
+                
+
+                final_results.put(resultOutput, averageMap.get("recall"));
+            }
+        }
+
+        for (String key : final_results.keySet()){
+            LOGGER.info(key + "\t" + final_results.get(key));
+            
+        }
+        return iterResultMap;
+    }
+    
+    private static void performCrossValidation(String root, String testDir, String posPreprecessedDir,
+            HashMap<String, Doc> goldenSet, InferBasedOnUser inferBOU, int returnTopK, boolean inferBasedOnUser,
+            HashMap<String, Double> result_recall, HashMap<String, Double> result_precision) throws IOException {
+        for (int i = 0; i < crossNumber; i++) {
+            String trainingData = testDir + i + "/" + train;
+            String testData = testDir + i + "/" + golden;
+            String outputModelDir = testDir + i + "/model";
+
+            // estimate labeled LDA
+            if (estimated) {
+                LOGGER.info("Estimating labeled LDA model for training data at " + trainingData + " to " + outputModelDir);
+                if (!new File(outputModelDir).isDirectory())
+                    topicEstimate.estimateTopic(outputModelDir, trainingData);
+            }
+
+            // TOPIC MODEL --------------------->
+            // load llda model
+            LOGGER.info("loading labeled LDA model from " + outputModelDir);
+            CVB0LabeledLDA model = TopicInfer.getmodel(outputModelDir);
+            String currentTestDir = testDir + i;
+            LOGGER.info("Set current test dir at " + currentTestDir);
+
+            // construct training query
+            String inferTopicForTrainingPath = currentTestDir
+                    + "/query-training-out.csv";            
+            LOGGER.info("Inferring top " + returnTopK + " topics from " + trainingData + " to " + inferTopicForTrainingPath);
+            TopicInfer.getTopKTopic(model, outputModelDir,
+                    trainingData, inferTopicForTrainingPath,
+                    infer_topK);
+            ArrayList<Query> queryListForTraining = Run
+                    .loadQueryFromFile(
+                            inferTopicForTrainingPath,
+                            trainingData);
+
+            // construct testing query
+            String inferTopicPathForTesting = currentTestDir
+                    + "/query-testing-out.csv";
+            LOGGER.info("Inferring top " + returnTopK + " topics from " + testData + " to " + inferTopicPathForTesting);
+            TopicInfer.getTopKTopic(model, outputModelDir,
+                    testData, inferTopicPathForTesting,
+                    infer_topK);
+            ArrayList<Query> queryListForTesting = Run
+                    .loadQueryFromFile(
+                            inferTopicPathForTesting, testData);
+            //<-------------------------- TOPIC MODEL
+
+            // load testing data text
+            LOGGER.info("loading document text for each test document");
+            for (Query q : queryListForTesting) {
+                String id = q.query_id;
+                Doc d = goldenSet.get(id);
+                String input = posPreprecessedDir + d.getName();
+                String content = TermTagIndexBuilder
+                        .readContentIntoOneLine(input);
+                q.setText(content);
+                q.trueTags = d.getTaglist();
+                // q.query_id = d.getName();
+            }
+
+            // load training data text
+            LOGGER.info("loading document text for each test document");            
+            for (Query q : queryListForTraining) {
+                String id = q.query_id;
+                Doc d = goldenSet.get(id);
+                String input = posPreprecessedDir + d.getName();
+                String content = TermTagIndexBuilder
+                        .readContentIntoOneLine(input);
+                q.setText(content);
+                q.trueTags = d.getTaglist();
+                // q.query_id = d.getName();
+            }
+
+            // infer topic llda
+            if (topicInfer) {
+                LOGGER.info("Loading inferred topics for testing at " + queryListForTesting);
+                Run.loadInferredTopic(inferTopicPathForTesting, queryListForTesting);
+                LOGGER.info("Loading inferred topics for training at " + inferTopicForTrainingPath);
+                Run.loadInferredTopic(inferTopicForTrainingPath, queryListForTraining);
+            }
+
+            // infer topic from term_tag_index
+            if (termTagIndex) {
+                TermTagIndex tti = new TermTagIndex();
+                tti.loadIndexFromFile(root
+                        + "term_tag_index.txt");
+                LOGGER.info("Inferring topic for testing from term_tag_index at " + root
+                        + "term_tag_index.txt");
+                for (Query q : queryListForTesting) {
+                    tti.assignTags(
+                            q,
+                            TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+                            termTagIndex_topK);
+                }
+                LOGGER.info("Inferring topic for training from term_tag_index at " + root
+                        + "term_tag_index.txt");
+                for (Query q : queryListForTraining) {
+                    tti.assignTags(
+                            q,
+                            TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+                            termTagIndex_topK);
+                }
+            }
+
+            if (inferBasedOnUser) {
+                LOGGER.info("inferring topics based on user for training dataset");
+                inferBOU.infer(queryListForTraining, queryListForTraining, goldenSet);
+                LOGGER.info("inferring topics based on user for testing dataset");
+                inferBOU.infer(queryListForTraining, queryListForTesting, goldenSet);
+            }
+
+
+            // query expansion
+            if (expansion) {
+                LOGGER.info("expanding query results based on tag graph");
+                queryExpansion(currentTestDir, trainingData, testData, queryListForTraining,
+                        queryListForTesting);
+            }
+
+            // evaluate
+            // output trained result
+            LOGGER.info("computing precision and recall");
+            outputRecallPrecision(i, currentTestDir, trainingData, testData, goldenSet, queryListForTraining,
+                    queryListForTesting, returnTopK, result_recall, result_precision);
+        }
+    }
+    
+    private static HashMap<String, Double> computeAverageRecallPrecision(HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) {
+        double precision_sum = 0;
+        double recall_sum = 0;
+        for (String key : result_recall.keySet()) {
+            precision_sum += result_precision.get(key);
+            recall_sum += result_recall.get(key);
+        }
+        
+        double recall= (double) recall_sum
+                / (double) result_recall.size();
+        double precision = (double) precision_sum
+                / (double) result_precision.size();
+        
+        HashMap<String, Double> averageMap =  new HashMap<>();
+        averageMap.put("recall",  recall);
+        averageMap.put("precision",  precision);
+        
+        return averageMap;
+    }
+    
+    private static void outputUbfRecallPrecisionForParas(String resultOutput, HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) throws IOException {
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(resultOutput))) {
+
+            // output recall and precision
+            bw.write("id,recall,precision\n");
+            for (String key : result_recall.keySet()) {
+                bw.write(key + "," + result_recall.get(key) + "," + result_precision.get(key) + "\n");
+            }
+        }
+    }
+    
+
+    private static void outputRecallPrecision(int crossNumberIdx, String currentTestDir, String trainingData,
+            String testData, HashMap<String, Doc> goldenSet, ArrayList<Query> queryListForTraining,
+            ArrayList<Query> queryListForTesting, int returnTopK, HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) throws IOException {
+        try (BufferedWriter bwLocal = new BufferedWriter(new FileWriter(currentTestDir + "/recall_withTraining.csv"))) {
+            LinearCombinationTrainerAndTester crossValidation = new LinearCombinationTrainerAndTester(
+                    queryListForTesting, goldenSet, returnTopK);
+
+            // output candidate tags of each query
+            IOManager.writeToLogFile(currentTestDir + "/detailedInferTagForEachComponent.txt", queryListForTesting, 50);
+
+            double[] paras = new double[4];
+//          paras[0] = 1.0;
+//          paras[1] = 0.1;
+//          paras[2] = 0.05;
+//          paras[3] = 0;
+            for (int i = 0; i < 4; i++) {
+                paras[i] = linearCombineParas[i];
+            }
+            LOGGER.info("Linear Combine Paras are : " + Arrays.toString(paras));
+
+            // retu rn recall
+            HashMap<String, Double> recalls = crossValidation.calculateRecalls(paras, queryListForTesting);
+
+            // return precision
+            HashMap<String, Double> precisions = crossValidation.calculatPrecision(paras, queryListForTesting);
+            double sum = 0;
+            for (String key : recalls.keySet()) {
+                bwLocal.write(key + "," + recalls.get(key));
+                result_recall.put(key, recalls.get(key));
+                result_precision.put(key, precisions.get(key));
+                sum += recalls.get(key);
+                bwLocal.newLine();
+            }
+            double averageRecall = sum / (double) (recalls.size());
+
+            LOGGER.info("iter" + crossNumberIdx + "\t" + paras[0] + "\t" + paras[1] + "\t" + paras[2] + "\t"
+                    + averageRecall);
+        }
+    }
+
+    
+    private static void queryExpansion(String currentTestDir, String trainingData, String testData,
+            ArrayList<Query> queryListForTraining, ArrayList<Query> queryListForTesting) {
+        // generate network
+        String graphInputForTesting = currentTestDir
+                + "/GraphresultForTesting.txt";
+        String graphInputForTraining = currentTestDir
+                + "/GraphresultForTraining.txt";
+        if (generateNetwork) {
+            // output for training
+            generateNetwork gn = new generateNetwork(
+                    trainingData);
+            gn.getNetwork();
+            gn.output(graphInputForTraining);
+            // output for testing
+            gn = new generateNetwork(testData);
+            // gn.load();
+            gn.getNetwork();
+            gn.output(graphInputForTesting);
+        }
+        // for training data
+        Graph g = new Graph();
+        g.createFromEdgeFile(graphInputForTraining);
+        for (Query q : queryListForTraining) {
+            q.extendTagWithTagGraph(g, 200);
+        }
+
+        // for testing data
+        g = new Graph();
+        g.createFromEdgeFile(graphInputForTesting);
+        for (Query q : queryListForTesting) {
+            q.extendTagWithTagGraph(g, 200);
+        }
+    }
+
+    private static HashMap<String, Doc> loadGoldset(String path) throws IOException {
+        HashMap<String, Doc> result = new HashMap<>();
+        try (BufferedReader br = new BufferedReader(new FileReader(path))) {
+            String line = null;
+            while ((line = br.readLine()) != null) {
+                String[] sparts = line.split(",");
+                Doc d = new Doc(sparts[0]);
+                String[] tagstr = sparts[1].split(" ");
+                ArrayList<String> tags = new ArrayList<>();
+                for (int i = 0; i < tagstr.length; i++) {
+                    tags.add(tagstr[i]);
+                }
+                d.setTaglist(tags);
+                result.put(d.getName(), d);
+            }
+        }
+
+        return result;
+    }
+
+    private static void generateTestDataDistribution(String dataset, int crossnumber, String testDir) throws FileNotFoundException, IOException {
+        HashMap<String, Doc> docList = loadDatasetbasedOnTag(dataset);
+        @SuppressWarnings("unchecked")
+        HashMap<String, Doc> remainingTest = (HashMap<String, Doc>) docList.clone();
+        int[] testCaseNumberPerCross = new int[10];
+        int average = docList.size() / crossnumber;
+
+        // get the number of each cross test
+        for (int i = 0; i < crossnumber; i++) {
+            if (i < crossnumber - 1) {
+                testCaseNumberPerCross[i] = average;
+            } else
+                testCaseNumberPerCross[i] = docList.size() - average * (crossnumber - 1);
+        }
+        for (int i = 0; i < crossnumber; i++) {
+            String subTestDir = testDir + i;
+            if (!new File(subTestDir).isDirectory())
+                new File(subTestDir).mkdirs();
+
+            ArrayList<Doc> currentTestset = new ArrayList<>();
+            ArrayList<Doc> currentTrainset = new ArrayList<>();
+            if (i == crossnumber - 1) {
+                // dump the remained doc to test case
+                for (Doc d : remainingTest.values())
+                    currentTestset.add(d);
+
+                for (String key : docList.keySet()) {
+                    if (!currentTestset.contains(docList.get(key))) {
+                        currentTrainset.add(docList.get(key));
+                    }
+                }
+            } else {
+                // pick test set from each tag
+
+                int count = 0;
+
+                // get training set
+                boolean notFull = true;
+                while (notFull) {
+                    for (int j = 0; j < tag_doc.size(); j++) {
+                        Tag tag = tag_doc.get(j);
+                        if (!tag.getDocMap().isEmpty()) {
+                            String d_id = tag.getDocMap().keySet().iterator().next();
+                            Doc d = docList.get(d_id);
+                            if (!currentTestset.contains(d)) {
+                                currentTestset.add(d);
+                                count++;
+                                // remove this id in the doc_tag
+                                tag.getDocMap().remove(d_id);
+
+                                // remove in remainingTest
+                                remainingTest.remove(d_id);
+                            } else {
+                                LOGGER.info("duplica");
+                            }
+                            if (count == testCaseNumberPerCross[i]) {
+                                notFull = false;
+                                break;
+                            }
+
+                        }
+
+                    }
+                }
+
+                for (String key : docList.keySet()) {
+                    if (!currentTestset.contains(docList.get(key))) {
+                        currentTrainset.add(docList.get(key));
+                    }
+                }
+
+            }
+            // output test and train set
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " training data to " + subTestDir + "/" + train);
+            output(subTestDir + "/" + train, currentTrainset, true);
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " test data to " + subTestDir + "/" + test);
+            output(subTestDir + "/" + test, currentTestset, false);
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " gold data to " + subTestDir + "/" + golden);
+            output(subTestDir + "/" + golden, currentTestset, true);
+        }
+    }
+
+    private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
+        HashMap<String, Doc> list = new HashMap<>();
+        HashMap<String, ArrayList<String>> tmp = new HashMap<>();
+        try {
+            BufferedReader br = new BufferedReader(new FileReader(dataset));
+            String line = null;
+            while ((line = br.readLine()) != null) {
+                String[] sparts = line.split(",");
+                if (sparts.length < 3)
+                    LOGGER.error("bug");
+                String id = sparts[0];
+                String tags = sparts[1];
+                String connent = sparts[2];
+                Doc d = new Doc(id, tags, connent);
+                list.put(id, d);
+                // add tag doc
+                String[] tagList = tags.split(" ");
+                // take the first tag
+                String tag = tagList[0];
+
+                if (tmp.containsKey(tag)) {
+                    if (!tmp.get(tag).contains(id))
+                        tmp.get(tag).add(id);
+                    else
+                        LOGGER.info(tag + "," + id);
+                } else {
+                    ArrayList<String> newlist = new ArrayList<>();
+                    newlist.add(id);
+                    tmp.put(tag, newlist);
+                }
+
+            }
+            br.close();
+            // dump to tag_doc
+            for (String key : tmp.keySet()) {
+                HashMap<String, String> docmap = new HashMap<>();
+                for (String docId : tmp.get(key)) {
+                    docmap.put(docId, "1");
+                }
+
+                Tag t = new Tag(key, docmap);
+                tag_doc.add(t);
+            }
+
+        } catch (Exception e) {
+            // TODO Auto-generated catch block
+            e.printStackTrace();
+        }
+        return list;
+    }
+
+    private static void output(String path, ArrayList<Doc> dataset,
+            boolean outputTag) throws FileNotFoundException, IOException {
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(path))) {
+            for (Doc d : dataset) {
+                String line = "";
+                if (outputTag)
+                    line = d.getName() + "," + d.getTags() + ","
+                            + d.getWordSet();
+                else
+                    line = d.getName() + "," + d.getWordSet();
+                bw.write(line);
+                bw.write("\n");
+            }
+        } 
+    }
+    
+    private static void settingParametersFromPropertyFile(String propertyFile) throws FileNotFoundException, IOException {
+        Properties props = new Properties();
+        try (FileInputStream in = new FileInputStream(propertyFile)) {
+            props.load(in);
+        }
+
+        String value = props.getProperty("generateCrossData");
+        if (value != null) {
+            generateCrossData = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": generateCrossData = " + generateCrossData);
+
+        value = props.getProperty("buildInferBasedOnUserFromFile");
+        if (value != null) {
+            buildInferBasedOnUserFromFile = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": buildInferBasedOnUserFromFile = " + buildInferBasedOnUserFromFile);
+
+        value = props.getProperty("estimated");
+        if (value != null) {
+            estimated = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": estimated = " + estimated);
+
+        value = props.getProperty("topicInfer");
+        if (value != null) {
+            topicInfer = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": topicInfer = " + topicInfer);
+
+        value = props.getProperty("termTagIndex");
+        if (value != null) {
+            termTagIndex = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": termTagIndex = " + termTagIndex);
+
+        value = props.getProperty("expansion");
+        if (value != null) {
+            expansion = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": expansion = " + expansion);
+        
+        value = props.getProperty("numOfRepetitions");
+        if (value != null) {
+            numOfRepetitions = Integer.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": numOfRepetitions = " + numOfRepetitions);
+        
+        value = props.getProperty("returnTopKs");
+        if (value != null) {
+            String[] parts = value.split(",");
+            returnTopKs = new int[parts.length];
+            for (int i=0; i<parts.length; i++) {
+                returnTopKs[i] = Integer.valueOf(parts[i]);
+            }
+        }
+        LOGGER.info("From " + propertyFile + ": returnTopKs = " + Arrays.toString(returnTopKs));
+        
+        value = props.getProperty("linearCombineParasFile");
+        if (value != null) {
+            linearCombineParasFile = value;
+        }
+        LOGGER.info("From " + propertyFile + ": linearCombineParasFile = " + linearCombineParasFile);
+    }
+    
+    private static void setLinearCombineParasFromFile(String fn) throws IOException {
+        Path path = Paths.get(fn);
+        if (!Files.exists(path)) {
+            LOGGER.info("Linear Combine Parameter File " + fn + " does not exists.");
+            return;
+        }
+        
+        double[] paras = new double[4]; 
+        try (BufferedReader reader = Files.newBufferedReader(path)) {
+            String line = reader.readLine();
+            paras[0] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[1] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[2] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[3] = Double.parseDouble(line);
+        }
+        for (int i=0; i<4; i++) {
+            linearCombineParas[i] = paras[i];
+        }
+        LOGGER.info("loaded linearCombineParas: " + Arrays.toString(linearCombineParas));
+    }
+}
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_UH.java b/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_UH.java
new file mode 100644
index 0000000..c7cd3ed
--- /dev/null
+++ b/tag_recommend_code/src/evaluate_EnTagRec_U2/RunKtimesForEffectSizeTest_UH.java
@@ -0,0 +1,701 @@
+package evaluate_EnTagRec_U2;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Properties;
+import java.util.Set;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import Network.Graph;
+import Network.Tag;
+import Network.generateNetwork;
+import TagInferBasedonUser.InferBasedOnUser;
+import TermTagIndex.TermTagIndex;
+import TermTagIndex.TermTagIndexBuilder;
+//import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
+import evaluate_EnTagRec_U.IOManager;
+import queryExpansion.Query;
+import queryExpansion.Run;
+import tagRecommend.Doc;
+import tagRecommend.LinearCombinationTrainerAndTester;
+import tagRecommend.tenCrossValidation;
+//import topicinfer.TopicInfer;
+//import topicinfer.topicEstimate;
+
+public class RunKtimesForEffectSizeTest_UH {
+    private final static Logger LOGGER = LoggerFactory.getLogger(RunKtimesForEffectSizeTest_UH.class);
+    /**
+     * @param args
+     */
+    static String logFile = null;
+    // generate 10 cross fold for cross validation test
+    static boolean generateCrossData = true;
+    static boolean buildInferBasedOnUserFromFile = true;
+    // estimated the Labeled-LDA from the training data
+    static boolean estimated = true;
+    // generate the network
+    static boolean generateNetwork = true;
+
+    // query expansion or not
+    static boolean expansion = true;
+    // infer tag from llda
+    static boolean topicInfer = true;
+    // infer tag from frequentist
+    static boolean termTagIndex = true;
+    // infer tag from user information
+    static final int crossNumber = 10;
+    static int infer_topK = 70;
+    static int termTagIndex_topK = 70;
+    // static String train = "trainDataset.csv";
+    static String train = "trainDataset_distr.csv";
+    static String test = "testDataset.csv";
+    static String golden = "goldenSet.csv";
+    static ArrayList<Tag> tag_doc = new ArrayList<>();
+    static int numOfRepetitions = 2;
+    static int[] returnTopKs = { 5, 10 };
+    
+    static String linearCombineParasFile = null;
+    static double[] linearCombineParas = {1.0, 0.1, 0.05, 0};
+
+    public static void main(String[] args) throws IOException, ClassNotFoundException {
+
+        if (args.length < 1) {
+            System.out.println("usage [root path]");
+            return;
+        }
+        String root = args[0];
+        
+        if (args.length >= 2) {
+            settingParametersFromPropertyFile(args[1]);
+        }
+        
+        if (linearCombineParasFile != null) {
+            setLinearCombineParasFromFile(linearCombineParasFile);
+        }
+        
+
+        boolean[] userCom = { true, false };
+
+        boolean headerWritten = false;
+        try (BufferedWriter bwRepeat = new BufferedWriter(new FileWriter(root + "repeat_precision_recall.csv"))) {
+            // precision/5/true recall/5/true; p/5/false r/5 false; p/10/true r/10/true; p/10/false r/10/false
+            // bwRepeat.write("id,EnRecTagU_P@5,r@5,EnRecTag_P@5,r@5,EnRecTagU_P@10,r@10,EnRecTag_P@10,r@10\n");
+            for (int repeatI = 0; repeatI < numOfRepetitions; repeatI++) {
+                
+                HashMap<String, Double> iterResults = runExperiment(root, repeatI, userCom, returnTopKs);
+                Set<String> keySet = iterResults.keySet();
+                if (!headerWritten) {
+                    bwRepeat.write("id,");
+                    for (String key: keySet) {
+                        bwRepeat.write(key + ",");
+                    }
+                    bwRepeat.write("\n");
+                    headerWritten = true;
+                }
+                bwRepeat.write(repeatI+",");
+                for (String key : keySet) {
+                    bwRepeat.write(iterResults.get(key)+ ",");
+                }
+                bwRepeat.write("\n"); 
+            }
+        }
+    }
+    
+    public static HashMap<String, Double> runExperiment(String root, int repeatI, boolean[] userCom,
+            int[] returnTopKs) throws IOException, ClassNotFoundException {
+        HashMap<String, Double> iterResultMap = new LinkedHashMap<String, Double>();
+        
+        LOGGER.info("Experiment repetition no = " + repeatI);
+        
+        String inputData = root + "dataset.csv";
+        String testDir = root + "testcase_repeat" +repeatI  +"/";
+        LOGGER.info("Starting with root = " + root + " and inputData = " + inputData);
+
+        // load golden set
+        HashMap<String, Doc> goldenSet = loadGoldset(inputData);
+        LOGGER.info("loaded gold dataset from " + inputData);
+
+        //  load user info
+        InferBasedOnUser inferBOU = null;
+        if (buildInferBasedOnUserFromFile || !InferBasedOnUser.fileExists(root)) {
+            if (!buildInferBasedOnUserFromFile && !InferBasedOnUser.fileExists(root)) {
+                LOGGER.info("InferBaseOnUser seralized object does not exist, rebuilding ...");
+            }
+            inferBOU = new InferBasedOnUser(root);
+            inferBOU.saveToFile(root);
+        }
+        LOGGER.info("Created or loaded the InferBasedOnUser object");
+
+        
+        // generate cross data
+        if (generateCrossData) {
+            generateTestDataDistribution(inputData,
+                    tenCrossValidation.crossNumber, testDir);
+        }
+        LOGGER.info("Generated cross validation data sets from " + inputData + " at " + testDir);
+
+
+        
+        // run both inferBasedOnUser setting to false and true
+        HashMap<String, Double> final_results = new HashMap<>();
+        for (int returnTopK : returnTopKs) {
+            for (boolean inferBasedOnUser: userCom) {
+                LOGGER.info("Run experiment with returnTopK = " + returnTopK + " userCom = " + userCom);
+                
+                // recover inferBOU object
+                inferBOU = InferBasedOnUser.loadFromFile(root);
+                
+                // perform cross validation
+                HashMap<String, Double> result_recall = new HashMap<>();
+                HashMap<String, Double> result_precision = new HashMap<>();
+                String posPreprecessedDir = root + "posdata_preprocessed/";
+                performCrossValidation(root, testDir, posPreprecessedDir, goldenSet, inferBOU, returnTopK,
+                        inferBasedOnUser, result_recall, result_precision);
+                LOGGER.info("Completed the cross validation.");
+                
+                HashMap<String, Double> averageMap = computeAverageRecallPrecision(result_recall, result_precision);
+                String key = "EnRecTag" +  (inferBasedOnUser?"U":"") + "_P@" + returnTopK;
+                iterResultMap.put(key, averageMap.get("precision"));
+                key = "EnRecTag" +  (inferBasedOnUser?"U":"") + "_R@" + returnTopK;
+                iterResultMap.put(key, averageMap.get("recall"));
+                LOGGER.info("final average precision and recall:" + averageMap.get("precision") + "\t"
+                        + averageMap.get("recall"));
+                
+                String resultOutput = testDir + "/ubf_recall_precision"
+                        + "@" + returnTopK + "_" + infer_topK + "_"
+                        + termTagIndex_topK + ".csv";
+                outputUbfRecallPrecisionForParas(resultOutput, result_recall, result_precision);
+                
+
+                final_results.put(resultOutput, averageMap.get("recall"));
+            }
+        }
+
+        for (String key : final_results.keySet()){
+            LOGGER.info(key + "\t" + final_results.get(key));
+            
+        }
+        return iterResultMap;
+    }
+    
+    private static void performCrossValidation(String root, String testDir, String posPreprecessedDir,
+            HashMap<String, Doc> goldenSet, InferBasedOnUser inferBOU, int returnTopK, boolean inferBasedOnUser,
+            HashMap<String, Double> result_recall, HashMap<String, Double> result_precision) throws IOException {
+        for (int i = 0; i < crossNumber; i++) {
+            String trainingData = testDir + i + "/" + train;
+            String testData = testDir + i + "/" + golden;
+//            String outputModelDir = testDir + i + "/model";
+
+//            // estimate labeled LDA
+//            if (estimated) {
+//                LOGGER.info("Estimating labeled LDA model for training data at " + trainingData + " to " + outputModelDir);
+//                if (!new File(outputModelDir).isDirectory())
+//                    topicEstimate.estimateTopic(outputModelDir, trainingData);
+//            }
+
+//            // TOPIC MODEL --------------------->
+//            // load llda model
+//            LOGGER.info("loading labeled LDA model from " + outputModelDir);
+//            CVB0LabeledLDA model = TopicInfer.getmodel(outputModelDir);
+            
+            LOGGER.info("Attempt to reload pre-built L2H models to conduct the cross validation ...");
+            String currentTestDir = testDir + i;
+            LOGGER.info("Set current test dir at " + currentTestDir);
+
+//            // construct training query
+            String inferTopicForTrainingPath = currentTestDir
+                    + "/query-training-out.csv";            
+//            LOGGER.info("Inferring top " + returnTopK + " topics from " + trainingData + " to " + inferTopicForTrainingPath);
+//            TopicInfer.getTopKTopic(model, outputModelDir,
+//                    trainingData, inferTopicForTrainingPath,
+//                    infer_topK);
+            ArrayList<Query> queryListForTraining = Run
+                    .loadQueryFromFile(
+                            inferTopicForTrainingPath,
+                            trainingData);
+
+            // construct testing query
+            String inferTopicPathForTesting = currentTestDir
+                    + "/query-testing-out.csv";
+//            LOGGER.info("Inferring top " + returnTopK + " topics from " + testData + " to " + inferTopicPathForTesting);
+//            TopicInfer.getTopKTopic(model, outputModelDir,
+//                    testData, inferTopicPathForTesting,
+//                    infer_topK);
+            ArrayList<Query> queryListForTesting = Run
+                    .loadQueryFromFile(
+                            inferTopicPathForTesting, testData);
+            //<-------------------------- TOPIC MODEL
+            LOGGER.info("queryListForTesting has " + queryListForTesting.size() + " from " + inferTopicPathForTesting +
+                    " and " + testData);
+
+            // load testing data text
+            LOGGER.info("loading document text for each test document");
+//            int qNum = 0;
+            for (Query q : queryListForTesting) {
+                String id = q.query_id;
+                Doc d = goldenSet.get(id);
+//                LOGGER.info("qNum = " + qNum + " Doc d = " + d + " from id = " + id);
+                String input = posPreprecessedDir + d.getName();
+                String content = TermTagIndexBuilder
+                        .readContentIntoOneLine(input);
+                q.setText(content);
+                q.trueTags = d.getTaglist();
+//                qNum ++;
+                // q.query_id = d.getName();
+            }
+
+            // load training data text
+            LOGGER.info("loading document text for each test document");            
+            for (Query q : queryListForTraining) {
+                String id = q.query_id;
+                Doc d = goldenSet.get(id);
+                String input = posPreprecessedDir + d.getName();
+                String content = TermTagIndexBuilder
+                        .readContentIntoOneLine(input);
+                q.setText(content);
+                q.trueTags = d.getTaglist();
+                // q.query_id = d.getName();
+            }
+
+            // infer topic llda
+            if (topicInfer) {
+                LOGGER.info("Loading inferred topics for testing at " + inferTopicPathForTesting);
+                Run.loadInferredTopic(inferTopicPathForTesting, queryListForTesting);
+                LOGGER.info("Loading inferred topics for training at " + inferTopicForTrainingPath);
+                Run.loadInferredTopic(inferTopicForTrainingPath, queryListForTraining);
+            }
+
+            // infer topic from term_tag_index
+            if (termTagIndex) {
+                TermTagIndex tti = new TermTagIndex();
+                tti.loadIndexFromFile(root
+                        + "term_tag_index.txt");
+                LOGGER.info("Inferring topic for testing from term_tag_index at " + root
+                        + "term_tag_index.txt");
+                for (Query q : queryListForTesting) {
+                    tti.assignTags(
+                            q,
+                            TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+                            termTagIndex_topK);
+                }
+                LOGGER.info("Inferring topic for training from term_tag_index at " + root
+                        + "term_tag_index.txt");
+                for (Query q : queryListForTraining) {
+                    tti.assignTags(
+                            q,
+                            TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+                            termTagIndex_topK);
+                }
+            }
+
+            if (inferBasedOnUser) {
+                LOGGER.info("inferring topics based on user for training dataset");
+                inferBOU.infer(queryListForTraining, queryListForTraining, goldenSet);
+                LOGGER.info("inferring topics based on user for testing dataset");
+                inferBOU.infer(queryListForTraining, queryListForTesting, goldenSet);
+            }
+
+
+            // query expansion
+            if (expansion) {
+                LOGGER.info("expanding query results based on tag graph");
+                queryExpansion(currentTestDir, trainingData, testData, queryListForTraining,
+                        queryListForTesting);
+            }
+
+            // evaluate
+            // output trained result
+            LOGGER.info("computing precision and recall");
+            outputRecallPrecision(i, currentTestDir, trainingData, testData, goldenSet, queryListForTraining,
+                    queryListForTesting, returnTopK, result_recall, result_precision);
+        }
+    }
+    
+    private static HashMap<String, Double> computeAverageRecallPrecision(HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) {
+        double precision_sum = 0;
+        double recall_sum = 0;
+        for (String key : result_recall.keySet()) {
+            precision_sum += result_precision.get(key);
+            recall_sum += result_recall.get(key);
+        }
+        
+        double recall= (double) recall_sum
+                / (double) result_recall.size();
+        double precision = (double) precision_sum
+                / (double) result_precision.size();
+        
+        HashMap<String, Double> averageMap =  new HashMap<>();
+        averageMap.put("recall",  recall);
+        averageMap.put("precision",  precision);
+        
+        return averageMap;
+    }
+    
+    private static void outputUbfRecallPrecisionForParas(String resultOutput, HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) throws IOException {
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(resultOutput))) {
+
+            // output recall and precision
+            bw.write("id,recall,precision\n");
+            for (String key : result_recall.keySet()) {
+                bw.write(key + "," + result_recall.get(key) + "," + result_precision.get(key) + "\n");
+            }
+        }
+    }
+    
+
+    private static void outputRecallPrecision(int crossNumberIdx, String currentTestDir, String trainingData,
+            String testData, HashMap<String, Doc> goldenSet, ArrayList<Query> queryListForTraining,
+            ArrayList<Query> queryListForTesting, int returnTopK, HashMap<String, Double> result_recall,
+            HashMap<String, Double> result_precision) throws IOException {
+        try (BufferedWriter bwLocal = new BufferedWriter(new FileWriter(currentTestDir + "/recall_withTraining.csv"))) {
+            LinearCombinationTrainerAndTester crossValidation = new LinearCombinationTrainerAndTester(
+                    queryListForTesting, goldenSet, returnTopK);
+
+            // output candidate tags of each query
+            IOManager.writeToLogFile(currentTestDir + "/detailedInferTagForEachComponent.txt", queryListForTesting, 50);
+
+            double[] paras = new double[4];
+//            paras[0] = 1.0;
+//            paras[1] = 0.1;
+//            paras[2] = 0.05;
+//            paras[3] = 0;
+            for (int i=0; i<4; i++) {
+                paras[i] = linearCombineParas[i];
+            }
+            LOGGER.info("Linear Combine Paras are : " + Arrays.toString(paras));
+
+            // retu rn recall
+            HashMap<String, Double> recalls = crossValidation.calculateRecalls(paras, queryListForTesting);
+
+            // return precision
+            HashMap<String, Double> precisions = crossValidation.calculatPrecision(paras, queryListForTesting);
+            double sum = 0;
+            for (String key : recalls.keySet()) {
+                bwLocal.write(key + "," + recalls.get(key));
+                result_recall.put(key, recalls.get(key));
+                result_precision.put(key, precisions.get(key));
+                sum += recalls.get(key);
+                bwLocal.newLine();
+            }
+            double averageRecall = sum / (double) (recalls.size());
+
+            LOGGER.info("iter" + crossNumberIdx + "\t" + paras[0] + "\t" + paras[1] + "\t" + paras[2] + "\t"
+                    + averageRecall);
+        }
+    }
+
+    
+    private static void queryExpansion(String currentTestDir, String trainingData, String testData,
+            ArrayList<Query> queryListForTraining, ArrayList<Query> queryListForTesting) {
+        // generate network
+        String graphInputForTesting = currentTestDir
+                + "/GraphresultForTesting.txt";
+        String graphInputForTraining = currentTestDir
+                + "/GraphresultForTraining.txt";
+        if (generateNetwork) {
+            // output for training
+            generateNetwork gn = new generateNetwork(
+                    trainingData);
+            gn.getNetwork();
+            gn.output(graphInputForTraining);
+            // output for testing
+            gn = new generateNetwork(testData);
+            // gn.load();
+            gn.getNetwork();
+            gn.output(graphInputForTesting);
+        }
+        // for training data
+        Graph g = new Graph();
+        g.createFromEdgeFile(graphInputForTraining);
+        for (Query q : queryListForTraining) {
+            q.extendTagWithTagGraph(g, 200);
+        }
+
+        // for testing data
+        g = new Graph();
+        g.createFromEdgeFile(graphInputForTesting);
+        for (Query q : queryListForTesting) {
+            q.extendTagWithTagGraph(g, 200);
+        }
+    }
+
+    private static HashMap<String, Doc> loadGoldset(String path) throws IOException {
+        HashMap<String, Doc> result = new HashMap<>();
+        try (BufferedReader br = new BufferedReader(new FileReader(path))) {
+            String line = null;
+            while ((line = br.readLine()) != null) {
+                String[] sparts = line.split(",");
+                Doc d = new Doc(sparts[0]);
+                String[] tagstr = sparts[1].split(" ");
+                ArrayList<String> tags = new ArrayList<>();
+                for (int i = 0; i < tagstr.length; i++) {
+                    tags.add(tagstr[i]);
+                }
+                d.setTaglist(tags);
+                result.put(d.getName(), d);
+            }
+        }
+
+        return result;
+    }
+
+    private static void generateTestDataDistribution(String dataset, int crossnumber, String testDir) throws FileNotFoundException, IOException {
+        HashMap<String, Doc> docList = loadDatasetbasedOnTag(dataset);
+        @SuppressWarnings("unchecked")
+        HashMap<String, Doc> remainingTest = (HashMap<String, Doc>) docList.clone();
+        int[] testCaseNumberPerCross = new int[10];
+        int average = docList.size() / crossnumber;
+
+        // get the number of each cross test
+        for (int i = 0; i < crossnumber; i++) {
+            if (i < crossnumber - 1) {
+                testCaseNumberPerCross[i] = average;
+            } else
+                testCaseNumberPerCross[i] = docList.size() - average * (crossnumber - 1);
+        }
+        for (int i = 0; i < crossnumber; i++) {
+            String subTestDir = testDir + i;
+            if (!new File(subTestDir).isDirectory())
+                new File(subTestDir).mkdirs();
+
+            ArrayList<Doc> currentTestset = new ArrayList<>();
+            ArrayList<Doc> currentTrainset = new ArrayList<>();
+            if (i == crossnumber - 1) {
+                // dump the remained doc to test case
+                for (Doc d : remainingTest.values())
+                    currentTestset.add(d);
+
+                for (String key : docList.keySet()) {
+                    if (!currentTestset.contains(docList.get(key))) {
+                        currentTrainset.add(docList.get(key));
+                    }
+                }
+            } else {
+                // pick test set from each tag
+
+                int count = 0;
+
+                // get training set
+                boolean notFull = true;
+                while (notFull) {
+                    for (int j = 0; j < tag_doc.size(); j++) {
+                        Tag tag = tag_doc.get(j);
+                        if (!tag.getDocMap().isEmpty()) {
+                            String d_id = tag.getDocMap().keySet().iterator().next();
+                            Doc d = docList.get(d_id);
+                            if (!currentTestset.contains(d)) {
+                                currentTestset.add(d);
+                                count++;
+                                // remove this id in the doc_tag
+                                tag.getDocMap().remove(d_id);
+
+                                // remove in remainingTest
+                                remainingTest.remove(d_id);
+                            } else {
+                                LOGGER.info("duplica");
+                            }
+                            if (count == testCaseNumberPerCross[i]) {
+                                notFull = false;
+                                break;
+                            }
+
+                        }
+
+                    }
+                }
+
+                for (String key : docList.keySet()) {
+                    if (!currentTestset.contains(docList.get(key))) {
+                        currentTrainset.add(docList.get(key));
+                    }
+                }
+
+            }
+            // output test and train set
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " training data to " + subTestDir + "/" + train);
+            output(subTestDir + "/" + train, currentTrainset, true);
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " test data to " + subTestDir + "/" + test);
+            output(subTestDir + "/" + test, currentTestset, false);
+            LOGGER.info("Writing the cross validation folder #" + crossnumber + " gold data to " + subTestDir + "/" + golden);
+            output(subTestDir + "/" + golden, currentTestset, true);
+        }
+    }
+
+    private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
+        HashMap<String, Doc> list = new HashMap<>();
+        HashMap<String, ArrayList<String>> tmp = new HashMap<>();
+        try {
+            BufferedReader br = new BufferedReader(new FileReader(dataset));
+            String line = null;
+            while ((line = br.readLine()) != null) {
+                String[] sparts = line.split(",");
+                if (sparts.length < 3)
+                    LOGGER.error("bug");
+                String id = sparts[0];
+                String tags = sparts[1];
+                String connent = sparts[2];
+                Doc d = new Doc(id, tags, connent);
+                list.put(id, d);
+                // add tag doc
+                String[] tagList = tags.split(" ");
+                // take the first tag
+                String tag = tagList[0];
+
+                if (tmp.containsKey(tag)) {
+                    if (!tmp.get(tag).contains(id))
+                        tmp.get(tag).add(id);
+                    else
+                        LOGGER.info(tag + "," + id);
+                } else {
+                    ArrayList<String> newlist = new ArrayList<>();
+                    newlist.add(id);
+                    tmp.put(tag, newlist);
+                }
+
+            }
+            br.close();
+            // dump to tag_doc
+            for (String key : tmp.keySet()) {
+                HashMap<String, String> docmap = new HashMap<>();
+                for (String docId : tmp.get(key)) {
+                    docmap.put(docId, "1");
+                }
+
+                Tag t = new Tag(key, docmap);
+                tag_doc.add(t);
+            }
+
+        } catch (Exception e) {
+            // TODO Auto-generated catch block
+            e.printStackTrace();
+        }
+        return list;
+    }
+
+    private static void output(String path, ArrayList<Doc> dataset,
+            boolean outputTag) throws FileNotFoundException, IOException {
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(path))) {
+            for (Doc d : dataset) {
+                String line = "";
+                if (outputTag)
+                    line = d.getName() + "," + d.getTags() + ","
+                            + d.getWordSet();
+                else
+                    line = d.getName() + "," + d.getWordSet();
+                bw.write(line);
+                bw.write("\n");
+            }
+        } 
+    }
+    
+    private static void settingParametersFromPropertyFile(String propertyFile) throws FileNotFoundException, IOException {
+        Properties props = new Properties();
+        try (FileInputStream in = new FileInputStream(propertyFile)) {
+            props.load(in);
+        }
+
+        String value = props.getProperty("generateCrossData");
+        if (value != null) {
+            generateCrossData = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": generateCrossData = " + generateCrossData);
+
+        value = props.getProperty("buildInferBasedOnUserFromFile");
+        if (value != null) {
+            buildInferBasedOnUserFromFile = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": buildInferBasedOnUserFromFile = " + buildInferBasedOnUserFromFile);
+
+        value = props.getProperty("estimated");
+        if (value != null) {
+            estimated = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": estimated = " + estimated);
+
+        value = props.getProperty("topicInfer");
+        if (value != null) {
+            topicInfer = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": topicInfer = " + topicInfer);
+
+        value = props.getProperty("termTagIndex");
+        if (value != null) {
+            termTagIndex = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": termTagIndex = " + termTagIndex);
+
+        value = props.getProperty("expansion");
+        if (value != null) {
+            expansion = Boolean.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": expansion = " + expansion);
+        
+        value = props.getProperty("numOfRepetitions");
+        if (value != null) {
+            numOfRepetitions = Integer.valueOf(value);
+        }
+        LOGGER.info("From " + propertyFile + ": numOfRepetitions = " + numOfRepetitions);
+        
+        value = props.getProperty("returnTopKs");
+        if (value != null) {
+            String[] parts = value.split(",");
+            returnTopKs = new int[parts.length];
+            for (int i=0; i<parts.length; i++) {
+                returnTopKs[i] = Integer.valueOf(parts[i]);
+            }
+        }
+        LOGGER.info("From " + propertyFile + ": returnTopKs = " + Arrays.toString(returnTopKs));
+        
+        value = props.getProperty("linearCombineParasFile");
+        if (value != null) {
+            linearCombineParasFile = value;
+        }
+        LOGGER.info("From " + propertyFile + ": linearCombineParasFile = " + linearCombineParasFile);
+    }
+    
+    private static void setLinearCombineParasFromFile(String fn) throws IOException {
+        Path path = Paths.get(fn);
+        if (!Files.exists(path)) {
+            LOGGER.info("Linear Combine Parameter File " + fn + " does not exists.");
+            return;
+        }
+        
+        double[] paras = new double[4]; 
+        try (BufferedReader reader = Files.newBufferedReader(path)) {
+            String line = reader.readLine();
+            paras[0] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[1] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[2] = Double.parseDouble(line);
+            
+            line = reader.readLine();
+            paras[3] = Double.parseDouble(line);
+        }
+        for (int i=0; i<4; i++) {
+            linearCombineParas[i] = paras[i];
+        }
+        LOGGER.info("loaded linearCombineParas: " + Arrays.toString(linearCombineParas));
+    }
+}
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA.java b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA.java
index 650204c..fb91a5b 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA.java
@@ -25,7 +25,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -66,7 +66,7 @@ public class RunKtimesForEffectSizeTest_UA {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 
@@ -79,7 +79,7 @@ public class RunKtimesForEffectSizeTest_UA {
 		// "G:\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\askubuntuSource\\";
 		// String root=
 		// "G:\\research\\tag_recommendation\\folksonomy\\freecode\\";
-		HashMap<String, Double> final_results = new HashMap();
+		HashMap<String, Double> final_results = new HashMap<>();
 		// String root  ="G:\\research\\tag_recommendation\\data_and_results\\AlltheFourDataset\\StackOverFlowForShaowei\\";
 		String inputData = root + "dataset.csv";
 		
@@ -129,12 +129,12 @@ public class RunKtimesForEffectSizeTest_UA {
 									tenCrossValidation.crossNumber, TestDir);
 						}
 
-						HashMap<String, Double> result_recall = new HashMap();
-						HashMap<String, Double> result_precision = new HashMap();
+						HashMap<String, Double> result_recall = new HashMap<>();
+						HashMap<String, Double> result_precision = new HashMap<>();
 
 						InferBasedOnUser inferBOU = new InferBasedOnUser(root);
 
-						// HashMap<String,Double> result_10= new HashMap();
+						// HashMap<String,Double> result_10= new HashMap<>();
 						// perform cross validation
 						try {
 							for (int i = 0; i < crossNumber; i++) {
@@ -168,7 +168,7 @@ public class RunKtimesForEffectSizeTest_UA {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										trainingData,
 										inferTopicForTrainingPath, infer_topK);
-								ArrayList<Query> queryListForTraining = run
+								ArrayList<Query> queryListForTraining = Run
 										.loadQueryFromFile(
 												inferTopicForTrainingPath,
 												trainingData);
@@ -180,7 +180,7 @@ public class RunKtimesForEffectSizeTest_UA {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										testData, inferTopicPathForTesting,
 										infer_topK);
-								ArrayList<Query> queryListForTesting = run
+								ArrayList<Query> queryListForTesting = Run
 										.loadQueryFromFile(
 												inferTopicPathForTesting,
 												testData);
@@ -213,10 +213,10 @@ public class RunKtimesForEffectSizeTest_UA {
 
 								// infer topic llda
 								if (topicInfer) {
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicPathForTesting,
 											queryListForTesting);
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicForTrainingPath,
 											queryListForTraining);
 								}
@@ -224,17 +224,17 @@ public class RunKtimesForEffectSizeTest_UA {
 								// infer topic from term_tag_index
 								if (termTagIndex) {
 									TermTagIndex tti = new TermTagIndex();
-									tti.LoadIndexFromFile(root
+									tti.loadIndexFromFile(root
 											+ "term_tag_index.txt");
 									for (Query q : queryListForTesting) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
 									}
 
 									for (Query q : queryListForTraining) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
@@ -429,7 +429,7 @@ public class RunKtimesForEffectSizeTest_UA {
 	
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -438,7 +438,7 @@ public class RunKtimesForEffectSizeTest_UA {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -479,7 +479,7 @@ public class RunKtimesForEffectSizeTest_UA {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -490,7 +490,7 @@ public class RunKtimesForEffectSizeTest_UA {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -527,8 +527,8 @@ public class RunKtimesForEffectSizeTest_UA {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -589,8 +589,8 @@ public class RunKtimesForEffectSizeTest_UA {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -614,7 +614,7 @@ public class RunKtimesForEffectSizeTest_UA {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -623,7 +623,7 @@ public class RunKtimesForEffectSizeTest_UA {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -663,7 +663,7 @@ public class RunKtimesForEffectSizeTest_UA {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA_omitTagOneByOne.java b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA_omitTagOneByOne.java
index dc6a2a7..96e3634 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA_omitTagOneByOne.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_UA_omitTagOneByOne.java
@@ -25,7 +25,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -66,7 +66,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 
@@ -79,7 +79,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 		// "G:\\research\\tag_recommendation\\folksonomy\\AlltheFourDataset\\askubuntuSource\\";
 		// String root=
 		// "G:\\research\\tag_recommendation\\folksonomy\\freecode\\";
-		HashMap<String, Double> final_results = new HashMap();
+		HashMap<String, Double> final_results = new HashMap<>();
 		// String root  ="G:\\research\\tag_recommendation\\data_and_results\\AlltheFourDataset\\StackOverFlowForShaowei\\";
 		String inputData = root + "dataset.csv";
 		
@@ -130,14 +130,14 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 									tenCrossValidation.crossNumber, TestDir);
 						}
 
-						HashMap<String, Double> result_recall = new HashMap();
-						HashMap<String, Double> result_precision = new HashMap();
+						HashMap<String, Double> result_recall = new HashMap<>();
+						HashMap<String, Double> result_precision = new HashMap<>();
 
 						InferBasedOnUser inferBOU =null;
 						if(inferBasedOnUser)
 							inferBOU = new InferBasedOnUser(root);
 
-						// HashMap<String,Double> result_10= new HashMap();
+						// HashMap<String,Double> result_10= new HashMap<>();
 						// perform cross validation
 						try {
 							for (int i = 0; i < crossNumber; i++) {
@@ -171,7 +171,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										trainingData,
 										inferTopicForTrainingPath, infer_topK);
-								ArrayList<Query> queryListForTraining = run
+								ArrayList<Query> queryListForTraining = Run
 										.loadQueryFromFile(
 												inferTopicForTrainingPath,
 												trainingData);
@@ -183,7 +183,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										testData, inferTopicPathForTesting,
 										infer_topK);
-								ArrayList<Query> queryListForTesting = run
+								ArrayList<Query> queryListForTesting = Run
 										.loadQueryFromFile(
 												inferTopicPathForTesting,
 												testData);
@@ -216,10 +216,10 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 
 								// infer topic llda
 								if (topicInfer) {
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicPathForTesting,
 											queryListForTesting);
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicForTrainingPath,
 											queryListForTraining);
 								}
@@ -227,17 +227,17 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 								// infer topic from term_tag_index
 								if (termTagIndex) {
 									TermTagIndex tti = new TermTagIndex();
-									tti.LoadIndexFromFile(root
+									tti.loadIndexFromFile(root
 											+ "term_tag_index.txt");
 									for (Query q : queryListForTesting) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
 									}
 
 									for (Query q : queryListForTraining) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
@@ -435,7 +435,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 	
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -444,7 +444,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -485,7 +485,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -496,7 +496,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -533,8 +533,8 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -595,8 +595,8 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -620,7 +620,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -629,7 +629,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -669,7 +669,7 @@ public class RunKtimesForEffectSizeTest_UA_omitTagOneByOne {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData.java b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData.java
index a43072c..167195b 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData.java
@@ -27,7 +27,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import scala.reflect.generic.Trees.This;
 import topicinfer.*;
 import tagRecommend.*;
@@ -72,7 +72,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 
@@ -93,7 +93,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 		String root = args[0];
 		int repeatN  = Integer.parseInt(args[1]);
 		 
-		HashMap<String, Double> final_results = new HashMap();
+		HashMap<String, Double> final_results = new HashMap<>();
 		// String root  ="G:/research/tag_recommendation/data_and_results/AlltheFourDataset/StackOverFlowForShaowei/";
 		String inputData = root + "dataset.csv";
 		
@@ -144,14 +144,14 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 									tenCrossValidation.crossNumber, TestDir);
 						}
 
-						HashMap<String, Double> result_recall = new HashMap();
-						HashMap<String, Double> result_precision = new HashMap();
+						HashMap<String, Double> result_recall = new HashMap<>();
+						HashMap<String, Double> result_precision = new HashMap<>();
 						
 						InferBasedOnUser inferBOU =null;
 						if(inferBasedOnUser)
 							inferBOU= new InferBasedOnUser(root);
 
-						// HashMap<String,Double> result_10= new HashMap();
+						// HashMap<String,Double> result_10= new HashMap<>();
 						// perform cross validation
 						try {
 							for (int i = 0; i < crossNumber; i++) {
@@ -195,7 +195,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										trainingData,
 										inferTopicForTrainingPath, infer_topK);
-								ArrayList<Query> queryListForTraining = run
+								ArrayList<Query> queryListForTraining = Run
 										.loadQueryFromFile(
 												inferTopicForTrainingPath,
 												trainingData);
@@ -207,7 +207,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										testData, inferTopicPathForTesting,
 										infer_topK);
-								ArrayList<Query> queryListForTesting = run
+								ArrayList<Query> queryListForTesting = Run
 										.loadQueryFromFile(
 												inferTopicPathForTesting,
 												testData);
@@ -240,10 +240,10 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 
 								// infer topic llda
 								if (topicInfer) {
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicPathForTesting,
 											queryListForTesting);
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicForTrainingPath,
 											queryListForTraining);
 								}
@@ -251,17 +251,17 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 								// infer topic from term_tag_index
 								if (termTagIndex) {
 									TermTagIndex tti = new TermTagIndex();
-									tti.LoadIndexFromFile(root
+									tti.loadIndexFromFile(root
 											+ "term_tag_index.txt");
 									for (Query q : queryListForTesting) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
 									}
 
 									for (Query q : queryListForTraining) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
@@ -278,8 +278,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 								}
 
 								// the set storing the tags need to be excluded  in evaluation and the given tags
-								HashMap<String, List<String>> givenTags = new HashMap();
-								HashMap<String, List<String>> exclusiveTagOnEvaluation = new HashMap();
+								HashMap<String, List<String>> givenTags = new HashMap<>();
+								HashMap<String, List<String>> exclusiveTagOnEvaluation = new HashMap<>();
 								
 								GenerateDataForTagChange.getGivenAndExclusiveTags(root+"originalData_tag50.csv", givenTags, exclusiveTagOnEvaluation, goldenSet);
 								
@@ -455,7 +455,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 	
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -464,7 +464,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -505,7 +505,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -516,7 +516,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -553,8 +553,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -615,8 +615,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -640,7 +640,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -649,7 +649,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -689,7 +689,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData_old.java b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData_old.java
index 55d328d..5ff6917 100644
--- a/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData_old.java
+++ b/tag_recommend_code/src/evaluate_EnTagRec_UA/RunKtimesForEffectSizeTest_onTagChangeData_old.java
@@ -26,10 +26,11 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
+
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
 import evaluate_EnTagRec_U.IOManager;
 import getAssociatePattern.InferenceWithAssocRules;
@@ -68,7 +69,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 
@@ -89,7 +90,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 		String root = args[0];
 		int repeatN  = Integer.parseInt(args[1]);
 		 
-		HashMap<String, Double> final_results = new HashMap();
+		HashMap<String, Double> final_results = new HashMap<>();
 		// String root  ="G:/research/tag_recommendation/data_and_results/AlltheFourDataset/StackOverFlowForShaowei/";
 		String inputData = root + "dataset.csv";
 		
@@ -139,14 +140,14 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 									tenCrossValidation.crossNumber, TestDir);
 						}
 
-						HashMap<String, Double> result_recall = new HashMap();
-						HashMap<String, Double> result_precision = new HashMap();
+						HashMap<String, Double> result_recall = new HashMap<>();
+						HashMap<String, Double> result_precision = new HashMap<>();
 						
 						InferBasedOnUser inferBOU =null;
 						if(inferBasedOnUser)
 							inferBOU= new InferBasedOnUser(root);
 
-						// HashMap<String,Double> result_10= new HashMap();
+						// HashMap<String,Double> result_10= new HashMap<>();
 						// perform cross validation
 						try {
 							for (int i = 0; i < crossNumber; i++) {
@@ -180,7 +181,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										trainingData,
 										inferTopicForTrainingPath, infer_topK);
-								ArrayList<Query> queryListForTraining = run
+								ArrayList<Query> queryListForTraining = Run
 										.loadQueryFromFile(
 												inferTopicForTrainingPath,
 												trainingData);
@@ -192,7 +193,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 								TopicInfer.getTopKTopic(model, outputModelDir,
 										testData, inferTopicPathForTesting,
 										infer_topK);
-								ArrayList<Query> queryListForTesting = run
+								ArrayList<Query> queryListForTesting = Run
 										.loadQueryFromFile(
 												inferTopicPathForTesting,
 												testData);
@@ -225,10 +226,10 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 
 								// infer topic llda
 								if (topicInfer) {
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicPathForTesting,
 											queryListForTesting);
-									run.loadInferredTopic(
+									Run.loadInferredTopic(
 											inferTopicForTrainingPath,
 											queryListForTraining);
 								}
@@ -236,17 +237,17 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 								// infer topic from term_tag_index
 								if (termTagIndex) {
 									TermTagIndex tti = new TermTagIndex();
-									tti.LoadIndexFromFile(root
+									tti.loadIndexFromFile(root
 											+ "term_tag_index.txt");
 									for (Query q : queryListForTesting) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
 									}
 
 									for (Query q : queryListForTraining) {
-										tti.AssignTags(
+										tti.assignTags(
 												q,
 												TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 												termTagIndex_topK);
@@ -263,8 +264,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 								}
 
 								// the set storing the tags need to be excluded  in evaluation and the given tags
-								HashMap<String, List<String>> givenTags = new HashMap();
-								HashMap<String, List<String>> exclusiveTagOnEvaluation = new HashMap();
+								HashMap<String, List<String>> givenTags = new HashMap<>();
+								HashMap<String, List<String>> exclusiveTagOnEvaluation = new HashMap<>();
 								
 								FormatDataFromTagChangeData.getGivenAndExclusiveTags(root+"originalData_tag50.csv", givenTags, exclusiveTagOnEvaluation);
 								
@@ -443,7 +444,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 	
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -452,7 +453,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -493,7 +494,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -504,7 +505,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -541,8 +542,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -603,8 +604,8 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -628,7 +629,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -637,7 +638,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -677,7 +678,7 @@ public class RunKtimesForEffectSizeTest_onTagChangeData_old {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/getAssociatePattern/AssoicatedRuleMining.java b/tag_recommend_code/src/getAssociatePattern/AssoicatedRuleMining.java
index 004d326..b74ce3c 100644
--- a/tag_recommend_code/src/getAssociatePattern/AssoicatedRuleMining.java
+++ b/tag_recommend_code/src/getAssociatePattern/AssoicatedRuleMining.java
@@ -50,7 +50,7 @@ public class AssoicatedRuleMining {
 		this.minsup = minsup;
 		this.minconf = minconf;
 		this.minlift = minlift;
-		this.RulesHashMap = new HashMap();
+		this.RulesHashMap = new HashMap<>();
 	}
 	
 	
@@ -101,7 +101,7 @@ public class AssoicatedRuleMining {
 			if(r.getItemset2().length >1)
 				continue;
 			String tags = "";
-			ArrayList<String> tagList = new ArrayList();
+			ArrayList<String> tagList = new ArrayList<>();
 			for(int tagId : set1){
 				tagList.add(this.tagTrans.getId_text_map().get(tagId));
 			}
@@ -114,7 +114,7 @@ public class AssoicatedRuleMining {
 			if(this.RulesHashMap.containsKey(tags)){
 				this.RulesHashMap.get(tags).add(r);
 			}else{
-				ArrayList<AssocRule> list = new ArrayList();
+				ArrayList<AssocRule> list = new ArrayList<>();
 				list.add(r);
 				
 				this.RulesHashMap.put(tags, list);
diff --git a/tag_recommend_code/src/getAssociatePattern/InferenceWithAssocRules.java b/tag_recommend_code/src/getAssociatePattern/InferenceWithAssocRules.java
index e90b0f4..968f5ad 100644
--- a/tag_recommend_code/src/getAssociatePattern/InferenceWithAssocRules.java
+++ b/tag_recommend_code/src/getAssociatePattern/InferenceWithAssocRules.java
@@ -25,7 +25,7 @@ public class InferenceWithAssocRules {
 	
 	public int hit = 0;
 	public int miss = 0;
-	public HashMap<String, List<Node>> hitQuery = new HashMap();
+	public HashMap<String, List<Node>> hitQuery = new HashMap<>();
 	
 	static int k=100;
 	public InferenceWithAssocRules(String dir, double minsup, double minconf, double minlift){
@@ -34,7 +34,7 @@ public class InferenceWithAssocRules {
 		this.minconf = minconf;
 		this.minlift = minlift;
 		this.dir = dir;
-		//this.exclusiveTags = new HashMap();
+		//this.exclusiveTags = new HashMap<>();
 		
 	}
 	/*
@@ -152,7 +152,7 @@ public class InferenceWithAssocRules {
 			if(this.hitQuery.containsKey(q.query_id)){
 				this.hitQuery.get(q.query_id).add(new Node(tag,confidence));
 			}else{
-				List<Node> list = new ArrayList();
+				List<Node> list = new ArrayList<>();
 				list.add(new Node(tag,confidence));
 				this.hitQuery.put(q.query_id, list );
 			}
@@ -162,7 +162,7 @@ public class InferenceWithAssocRules {
 	
 	public static HashMap<String, List<String>> getExclusiveTags(HashMap<String,Doc> goldenSet, int givenTagN) {
 		// TODO Auto-generated method stub
-		HashMap<String, List<String>> exclusiveTags = new HashMap();	
+		HashMap<String, List<String>> exclusiveTags = new HashMap<>();	
 		for(String query_id : goldenSet.keySet()){
 			Doc doc = goldenSet.get(query_id);
 			int size= doc.getTaglist().size();
@@ -177,11 +177,11 @@ public class InferenceWithAssocRules {
 	
 	public static HashMap<String, List<String>> getExclusiveTags(HashMap<String,Doc> goldenSet, int givenTagN, int whichOne) {
 		// TODO Auto-generated method stub
-		HashMap<String, List<String>> exclusiveTags = new HashMap();	
+		HashMap<String, List<String>> exclusiveTags = new HashMap<>();	
 		for(String query_id : goldenSet.keySet()){
 			Doc doc = goldenSet.get(query_id);
 			int size= doc.getTaglist().size();
-			List<String> topKTags = new ArrayList();
+			List<String> topKTags = new ArrayList<>();
 			int index = whichOne%doc.getTaglist().size();
 			topKTags.add(doc.getTaglist().get(index));
 			// sort tags to make the representation of tags are unique when the eles in the set are unique
diff --git a/tag_recommend_code/src/getAssociatePattern/tagTransformation.java b/tag_recommend_code/src/getAssociatePattern/tagTransformation.java
index 8f8f4b8..778d0bf 100644
--- a/tag_recommend_code/src/getAssociatePattern/tagTransformation.java
+++ b/tag_recommend_code/src/getAssociatePattern/tagTransformation.java
@@ -74,8 +74,8 @@ public class tagTransformation {
 	
 	
 	private void createMap(String dataFile){
-		this.text_id_map = new HashMap();
-		this.id_text_map = new HashMap();
+		this.text_id_map = new HashMap<>();
+		this.id_text_map = new HashMap<>();
 		int id= 1;
 		try{
 			System.out.println("loading file");
diff --git a/tag_recommend_code/src/log4j2.xml b/tag_recommend_code/src/log4j2.xml
new file mode 100644
index 0000000..3d3e328
--- /dev/null
+++ b/tag_recommend_code/src/log4j2.xml
@@ -0,0 +1,25 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<Configuration status="WARN">
+	<Appenders>
+		<Console name="Console" target="SYSTEM_OUT">
+			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} %l - %msg%n" />
+		</Console>
+        <File name="MyFile" fileName="EnTagRec.log" immediateFlush="true" append="false">
+            <PatternLayout pattern="%d{yyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} %l - %msg%n"/>
+        </File>	
+        <!--  includeLocation="true" must be included below; otherwise %l does not work in the above  -->
+		<Async name="ASYNCFILE" includeLocation="true">
+			<AppenderRef ref="MyFile" />
+		</Async>        
+	</Appenders>
+	<Loggers>
+		<Logger name="SOMOdelTool" level="ALL" additivity="false">
+			<AppenderRef ref="Console" />
+            <AppenderRef ref="ASYNCFILE"/>
+		</Logger>
+		<Root level="debug">
+			<AppenderRef ref="Console" />
+			<AppenderRef ref="ASYNCFILE"/>
+		</Root>
+	</Loggers>
+</Configuration>
diff --git a/tag_recommend_code/src/newRQ/TestonKvary_recall_precision.java b/tag_recommend_code/src/newRQ/TestonKvary_recall_precision.java
index 5650b68..5f559e1 100644
--- a/tag_recommend_code/src/newRQ/TestonKvary_recall_precision.java
+++ b/tag_recommend_code/src/newRQ/TestonKvary_recall_precision.java
@@ -21,7 +21,7 @@ import TagInferBasedonUser.InferBasedOnUser;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -48,7 +48,7 @@ public class TestonKvary_recall_precision {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -76,13 +76,13 @@ public class TestonKvary_recall_precision {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result_recall = new HashMap();
-				HashMap<String,Double> result_precision = new HashMap();
+				HashMap<String, Double> result_recall = new HashMap<>();
+				HashMap<String,Double> result_precision = new HashMap<>();
 				
 				InferBasedOnUser inferBOU = new InferBasedOnUser(root);
 				
 				
-				//HashMap<String,Double> result_10= new HashMap();
+				//HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -111,7 +111,7 @@ public class TestonKvary_recall_precision {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -121,7 +121,7 @@ public class TestonKvary_recall_precision {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -149,24 +149,24 @@ public class TestonKvary_recall_precision {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -324,7 +324,7 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -333,7 +333,7 @@ public class TestonKvary_recall_precision {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -374,7 +374,7 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -385,7 +385,7 @@ public class TestonKvary_recall_precision {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -422,8 +422,8 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -484,8 +484,8 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -509,7 +509,7 @@ public class TestonKvary_recall_precision {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -518,7 +518,7 @@ public class TestonKvary_recall_precision {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -558,7 +558,7 @@ public class TestonKvary_recall_precision {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/org/preprocess/collectInformationFromPosthistory.java b/tag_recommend_code/src/org/preprocess/CollectInformationFromPostHistory.java
similarity index 89%
rename from tag_recommend_code/src/org/preprocess/collectInformationFromPosthistory.java
rename to tag_recommend_code/src/org/preprocess/CollectInformationFromPostHistory.java
index 3408e66..7bf9ab2 100644
--- a/tag_recommend_code/src/org/preprocess/collectInformationFromPosthistory.java
+++ b/tag_recommend_code/src/org/preprocess/CollectInformationFromPostHistory.java
@@ -2,20 +2,18 @@ package org.preprocess;
 
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 
 import org.dom4j.Document;
 import org.dom4j.DocumentHelper;
 import org.dom4j.Element;
 
-import scala.actors.threadpool.Arrays;
-
-public class collectInformationFromPosthistory {
+public class CollectInformationFromPostHistory {
 
 	/**
 	 * @param args
@@ -31,7 +29,7 @@ public class collectInformationFromPosthistory {
 		String metaFile = projectDir + "meta.txt";
 		String datasetFile = projectDir + "dataset.csv";
 		String outputFile = projectDir +"dataset_origTag.csv";
-		collectInformationFromPosthistory factory = new collectInformationFromPosthistory();
+		CollectInformationFromPostHistory factory = new CollectInformationFromPostHistory();
 		try{
 			factory.getRowIdToFileMap(metaFile);
 			factory.collectOriginalTags(postFile);
@@ -44,9 +42,9 @@ public class collectInformationFromPosthistory {
 		
 	}
 	
-	public collectInformationFromPosthistory(){
-		this.postId_docId_map = new HashMap();
-		this.docId_tags = new HashMap();	
+	public CollectInformationFromPostHistory(){
+		this.postId_docId_map = new HashMap<>();
+		this.docId_tags = new HashMap<>();	
 		
 	}
 	
diff --git a/tag_recommend_code/src/org/preprocess/dataFilter.java b/tag_recommend_code/src/org/preprocess/DataFilter.java
similarity index 95%
rename from tag_recommend_code/src/org/preprocess/dataFilter.java
rename to tag_recommend_code/src/org/preprocess/DataFilter.java
index f640b43..c074ce4 100644
--- a/tag_recommend_code/src/org/preprocess/dataFilter.java
+++ b/tag_recommend_code/src/org/preprocess/DataFilter.java
@@ -14,7 +14,7 @@ import org.dbrd.preprocessor.filters.StemmingWordFilter;
 import org.dbrd.preprocessor.filters.StopWordFilter;
 import org.dbrd.preprocessor.filters.TokenizerWordFilter;
 
-public class dataFilter {
+public class DataFilter {
 	
 	
 	static String[] keywords = {"return ","while(","for(","switch(","if(","int ","char ","double","float","const",
@@ -105,7 +105,7 @@ public class dataFilter {
 		{
 			s = s.replace("_", " ");
 		}
-		ArrayList<Integer> pos = new ArrayList();
+//		ArrayList<Integer> pos = new ArrayList<>();
 		
 		
 		//
diff --git a/tag_recommend_code/src/org/preprocess/getTagRawNumber.java b/tag_recommend_code/src/org/preprocess/GetTagRawNumber.java
similarity index 82%
rename from tag_recommend_code/src/org/preprocess/getTagRawNumber.java
rename to tag_recommend_code/src/org/preprocess/GetTagRawNumber.java
index 736b038..923a3fc 100644
--- a/tag_recommend_code/src/org/preprocess/getTagRawNumber.java
+++ b/tag_recommend_code/src/org/preprocess/GetTagRawNumber.java
@@ -1,12 +1,11 @@
 package org.preprocess;
 
 import java.io.BufferedReader;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.IOException;
 import java.util.HashMap;
 
-public class getTagRawNumber {
+public class GetTagRawNumber {
 
 	/**
 	 * @param args
@@ -17,7 +16,7 @@ public class getTagRawNumber {
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(inputFile));
 			String line = null;
-			HashMap<String, Integer> pool = new HashMap();
+			HashMap<String, Integer> pool = new HashMap<>();
 			while((line = br.readLine())!=null){
 				String[] strs = line.split("\t");
 				for(int i =3; i< strs.length; i++){
diff --git a/tag_recommend_code/src/org/preprocess/HtmlFilter.java b/tag_recommend_code/src/org/preprocess/HtmlFilter.java
new file mode 100644
index 0000000..84d02f1
--- /dev/null
+++ b/tag_recommend_code/src/org/preprocess/HtmlFilter.java
@@ -0,0 +1,85 @@
+package org.preprocess;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HtmlFilter {
+    private final static Logger LOGGER = LoggerFactory.getLogger(HtmlFilter.class);
+
+    /**
+     * @param args
+     */
+    public static void main(String[] args) {
+        // TODO Auto-generated method stub
+        // tokenCleanForDir("O:/shaowei/folksonomy/freecode/rawdata/",
+        // "O:/shaowei/folksonomy/freecode/htmlFilterred");
+        // tokenCleanForDir("G:/research/tag_recommendation/data_and_results/ChangeTagData/rawdata",
+        // "G:/research/tag_recommendation/data_and_results/ChangeTagData/htmlFilterred");
+
+        if (args.length < 1) {
+            System.out.println("usage [root path]");
+            return;
+        }
+        String root = args[0];
+
+        try {
+            tokenCleanForDir(root + "rawdata", root + "htmlFilterred");
+        } catch (FileNotFoundException e) {
+            LOGGER.error(e.getMessage(), e);
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
+        }
+    }
+
+    public static void tokenCleanForDir(String inDir, String outDir) throws FileNotFoundException, IOException {
+        if (!Files.exists(Paths.get(inDir)) || !Files.isDirectory(Paths.get(inDir))) {
+            LOGGER.error("Directory at " + inDir + " expected");
+            throw new IllegalStateException("Directory at " + inDir + " expected");
+        }
+
+        File[] fileList = new File(inDir).listFiles();
+
+        if (fileList == null) {
+            LOGGER.error("Failed to list directories or files in " + inDir);
+            throw new IllegalStateException("Failed to list directories or files in " + inDir);
+        }
+        
+        if (!Files.exists(Paths.get(outDir))) {
+            Files.createDirectories(Paths.get(outDir));
+        }
+
+        int count = 0;
+        for (File f : fileList) {
+            tokenCleanerForFile(f.getAbsolutePath(), outDir + "/" + f.getName());
+            LOGGER.info("counter = " + count++);
+        }
+    }
+
+    public static void tokenCleanerForFile(String inFile, String outFile) throws FileNotFoundException, IOException {
+        byte[] readBytes = null;
+
+        try (FileInputStream in = new FileInputStream(inFile)) {
+            readBytes = new byte[in.available()];
+            in.read(readBytes);
+        }
+
+        String s = new String(readBytes);
+        s = s.replace("\n", " ").replace("\r", " ").replace(" +", " ");
+        String result = DataFilter.filter_html(s);
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(outFile))) {
+            bw.write(result);
+        }
+
+    }
+
+}
diff --git a/tag_recommend_code/src/org/preprocess/QuestionInformationManager.java b/tag_recommend_code/src/org/preprocess/QuestionInformationManager.java
index 0021005..3330ea7 100644
--- a/tag_recommend_code/src/org/preprocess/QuestionInformationManager.java
+++ b/tag_recommend_code/src/org/preprocess/QuestionInformationManager.java
@@ -2,26 +2,28 @@ package org.preprocess;
 
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
-import java.io.File;
 import java.io.FileNotFoundException;
-import java.io.FileOutputStream;
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
+import java.io.Serializable;
 import java.text.DateFormat;
 import java.text.SimpleDateFormat;
 import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashMap;
-import java.util.Iterator;
 
 import org.dom4j.Document;
 import org.dom4j.DocumentException;
 import org.dom4j.DocumentHelper;
 import org.dom4j.Element;
-import org.netlib.util.intW;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
-public class QuestionInformationManager {
+public class QuestionInformationManager implements Serializable {
+    private static final long serialVersionUID = -297855154422585089L;
+
+    private final static Logger LOGGER = LoggerFactory.getLogger(QuestionInformationManager.class);
 	
 	private HashMap<Integer, Integer> questionId_docId_map;
 	private HashMap<String, Integer> docMap;
@@ -84,7 +86,6 @@ public class QuestionInformationManager {
 		QuestionInformationManager fac = new QuestionInformationManager();
 		fac.getRowIdToFileMap(metaFile);
 		fac.getContentAndTag(postFile, projectDir,readLineN);
-		 
 	}
 	
 	public QuestionInformationManager(){
@@ -97,82 +98,71 @@ public class QuestionInformationManager {
 	 * input: metaFile 
 	 * return a hashmap contains the rowid and fileid
 	 */
-	public void getRowIdToFileMap(String metaFile) throws IOException{
-		System.out.println("load map from: " + metaFile);
-		BufferedReader br = new BufferedReader(new FileReader(metaFile));
+    public void getRowIdToFileMap(String metaFile) throws FileNotFoundException, IOException {
+        LOGGER.info("load map from: " + metaFile);
+        try (BufferedReader br = new BufferedReader(new FileReader(metaFile))) {
+            LOGGER.info("Read meta data (docId[TAB]questionId[TAB]userId from " + metaFile);
             this.questionId_docId_map = new HashMap<Integer, Integer>();
             this.user = new ArrayList<>();
-		while(br.ready()){
+            while (br.ready()) {
                 String line = br.readLine();
                 String[] tmp = line.split("\t");
                 Integer docId = Integer.parseInt(tmp[0]);
                 Integer questionId = Integer.parseInt(tmp[1]);
-			questionId_docId_map.put( questionId, docId);
+                questionId_docId_map.put(questionId, docId);
 
                 String userId = tmp[2];
                 this.user.add(userId);
-			
-			
             }
-		br.close();
-		
-		
+        }
     }
 	
 	// for freecode input format
-	public void getContentAndTag(String userInfoFile){
-		userId_questionList_map = new HashMap();
-		docId_User_Map = new HashMap();
-		try{
-		BufferedReader br = new BufferedReader(new FileReader(userInfoFile));
-		while(br.ready()){
+    public void getContentAndTag(String userInfoFile) throws FileNotFoundException, IOException {
+        userId_questionList_map = new HashMap<>();
+        docId_User_Map = new HashMap<>();
+        try (BufferedReader br = new BufferedReader(new FileReader(userInfoFile))) {
+            LOGGER.info("Read meta data (docId[TAB]questionId[TAB]userId from " + userInfoFile);
+            while (br.ready()) {
                 String line = br.readLine();
 
                 String[] ele = line.split("\t");
                 String userId = ele[1];
                 String docId = ele[0];
                 /*
-			Question q = new Question(docId);
-			if(!userId_questionList_map.containsKey(userId)){
-				ArrayList<Question> questionList = new ArrayList();
-				questionList.add(q);
-				userId_questionList_map.put(userId, questionList );
-			}else{
-				userId_questionList_map.get(userId).add(q);
-			}
+                 * Question q = new Question(docId);
+                 * if(!userId_questionList_map.containsKey(userId)){ ArrayList<Question>
+                 * questionList = new ArrayList<>(); questionList.add(q);
+                 * userId_questionList_map.put(userId, questionList ); }else{
+                 * userId_questionList_map.get(userId).add(q); }
                  */
                 docId_User_Map.put(docId, userId);
             }
-		}catch(Exception e){
-			e.printStackTrace();
         }
     }
 	
 	
 	// for project except for freecode
-	public void getContentAndTag(String postFile, String outputDir,int readlineN) throws IOException{
-		System.out.println("Start parsing xml file............");
-		BufferedReader br = new BufferedReader(new FileReader(postFile));
+	public void getContentAndTag(String postFile, String outputDir,int readlineN) throws FileNotFoundException, IOException{
+        LOGGER.info("Start parsing xml file " + postFile);
+
         docMap = new HashMap<String, Integer>();
-		docId_question_map = new HashMap();
-		docId_User_Map = new HashMap();
-		userId_questionList_map = new HashMap();
+        docId_question_map = new HashMap<>();
+        docId_User_Map = new HashMap<>();
+        userId_questionList_map = new HashMap<>();
         String temp = null;
         int j = 0;
         int fileNum = 1;
 		
-		
-		while (((temp = br.readLine()) != null) ) 
-		
-		{
+        try (BufferedReader br = new BufferedReader(new FileReader(postFile))) {
+            while (((temp = br.readLine()) != null))  {
                 j++;
                 if (j == 1 || j == 2) {
                     continue;
                 }
-			if(fileNum>readlineN)
+                if (fileNum > readlineN)
                     break;
 
-
                 StringBuffer sb = new StringBuffer();
 
                 sb.append("<?xml version=\"1.0\" encoding=\"utf-8\"?>\n");
@@ -187,101 +177,97 @@ public class QuestionInformationManager {
                     Element ele = rootElt.element("row");
                     int questionId = Integer.parseInt(ele.attribute("Id").getText());
 
-				if(questionId== 38386)
-					System.out.println();
-				//skip the one not in our dataset.
-				if(!this.questionId_docId_map.containsKey(questionId))
+                    if (questionId == 38386)
+                        LOGGER.info("questionId = " + questionId);
+                    // skip the one not in our dataset.
+                    if (!this.questionId_docId_map.containsKey(questionId))
                         continue;
                     int docId = this.questionId_docId_map.get(questionId);
 
-				int PostTypeId = Integer.parseInt(ele.attribute("PostTypeId")
-						.getText());
-                String sDate=ele.attribute("CreationDate").getText().replace("T", " ");
+                    int PostTypeId = Integer.parseInt(ele.attribute("PostTypeId").getText());
+                    String sDate = ele.attribute("CreationDate").getText().replace("T", " ");
                     DateFormat format = new SimpleDateFormat("yyyy-MM-dd H:m:s.S");
                     Date date = format.parse(sDate);
                     String tags = null;
                     String userId = null;
                     if (PostTypeId == 1) {
 
-					
                         try {
-						tags = ele.attribute("Tags").getText().replace("<", "").replace(">", " ") ;
+                            tags = ele.attribute("Tags").getText().replace("<", "").replace(">", " ");
                             userId = ele.attribute("OwnerUserId").getText();
                         } catch (Exception e) {
 
                             continue;
                         }
 
-					//File tmp = new File(outputDir
+                        // File tmp = new File(outputDir
                         // + String.valueOf(String.valueOf(fileNum)));
-					//if (!tmp.exists()) {
+                        // if (!tmp.exists()) {
                         // tmp.createNewFile();
-					//}
+                        // }
                         String ID = ele.attribute("Id").getText();
-					String Title = ele.attribute("Title").getText().replace("\n", "") ;
-					String Body = ele.attribute("Body").getText().replace("\n","") ;
-					//FileOutputStream tmpout = new FileOutputStream(tmp, true);
-					//tmpout.write(Title.getBytes("utf-8"));
-					//tmpout.write(Tags.getBytes("utf-8"));
-					//tmpout.write(Body.getBytes("utf-8"));
-					//tmpout.close();
-					if(!this.questionId_docId_map.containsKey(questionId))
+                        String Title = ele.attribute("Title").getText().replace("\n", "");
+                        String Body = ele.attribute("Body").getText().replace("\n", "");
+                        // FileOutputStream tmpout = new FileOutputStream(tmp, true);
+                        // tmpout.write(Title.getBytes("utf-8"));
+                        // tmpout.write(Tags.getBytes("utf-8"));
+                        // tmpout.write(Body.getBytes("utf-8"));
+                        // tmpout.close();
+                        if (!this.questionId_docId_map.containsKey(questionId))
                             continue;
 
                         // add question to docid map
-					Question q = new Question(docId, questionId, tags,date,Body, Title);
+                        Question q = new Question(docId, questionId, tags, date, Body, Title);
                         docId_question_map.put(docId, q);
 
                         // build userid and quesiton association
-					if(!userId_questionList_map.containsKey(userId)){
-						ArrayList<Question> questionList = new ArrayList();
+                        if (!userId_questionList_map.containsKey(userId)) {
+                            ArrayList<Question> questionList = new ArrayList<>();
                             questionList.add(q);
-						userId_questionList_map.put(userId, questionList );
-					}else{
+                            userId_questionList_map.put(userId, questionList);
+                        } else {
                             userId_questionList_map.get(userId).add(q);
                         }
                         StringBuffer metaSb = new StringBuffer();
                         metaSb.append(fileNum + "\t");
                         metaSb.append(ID + "\t");
                         metaSb.append("\n");
-					//outInfo.write(metaSb.toString().getBytes("utf-8"));
+                        // outInfo.write(metaSb.toString().getBytes("utf-8"));
                         docMap.put(ID, fileNum);
 
                         fileNum++;
                         if (fileNum % 1000 == 0) {
-						System.out.print("processing file: "+fileNum+ "\t\n");
+                            LOGGER.info("processing file: " + fileNum + "\t\n");
                         }
 
                     } else if (PostTypeId == 2) {
                         String sQuestionID = ele.attribute("ParentId").getText();
-					String Body = ele.attribute("Body").getText() + "\n";
+                        // String body = ele.attribute("Body").getText() + "\n";
 
                         try {
-						tags = ele.attribute("Tags").getText().replace("<", "").replace(">", " ") ;
+                            tags = ele.attribute("Tags").getText().replace("<", "").replace(">", " ");
                             userId = ele.attribute("OwnerUserId").getText();
                         } catch (Exception e) {
-
                             continue;
                         }
 
                         // collect the tags associate to user
-					Question q = new Question(0, Integer.parseInt(sQuestionID), tags,date,null, null);
-					if(!userId_questionList_map.containsKey(userId)){
-						ArrayList<Question> questionList = new ArrayList();
+                        Question q = new Question(0, Integer.parseInt(sQuestionID), tags, date, null, null);
+                        if (!userId_questionList_map.containsKey(userId)) {
+                            ArrayList<Question> questionList = new ArrayList<>();
                             questionList.add(q);
-						userId_questionList_map.put(userId, questionList );
-					}else{
+                            userId_questionList_map.put(userId, questionList);
+                        } else {
                             userId_questionList_map.get(userId).add(q);
                         }
 
-					
                         if (!docMap.containsKey(sQuestionID)) {
 
                             StringBuffer metaSb = new StringBuffer();
                             metaSb.append(fileNum + "\t");
                             metaSb.append(sQuestionID + "\t");
                             metaSb.append("\n");
-						//outInfo.write(metaSb.toString().getBytes("utf-8"));
+                            // outInfo.write(metaSb.toString().getBytes("utf-8"));
                             docMap.put(sQuestionID, fileNum);
 
                             fileNum++;
@@ -289,28 +275,26 @@ public class QuestionInformationManager {
 
                         }
 
-					
                     }
 
                     docId_User_Map.put(String.valueOf(docId), userId);
 
                 } catch (DocumentException e) {
-				e.printStackTrace();
+                    LOGGER.error(e.getMessage(), e);
                     continue;
                 } catch (Exception e) {
-				e.printStackTrace();
+                    LOGGER.error(e.getMessage(), e);
                     continue;
                 }
             }
-		//outInfo.close();
-		br.close();
+        }
 		
-		System.out.println("End parsing xml file..........");
+		LOGGER.info("End parsing xml file..........");
 		
 		// output questions to files
 		
 		String outputFile = outputDir + "QuestionInformation.cvs";
-		
+		LOGGER.info("Write to " + outputFile);
 		BufferedWriter bw = new BufferedWriter(new FileWriter(outputFile));
 		bw.write("docId,questionId,createdDate,tags,content\n");
 		for(Question q : docId_question_map.values()){
@@ -321,10 +305,10 @@ public class QuestionInformationManager {
 		
 		// output tags information of each user/owner
 		outputFile = outputDir + "userTagInformation.txt";
-		
+	    LOGGER.info("Write to " + outputFile);
 		bw = new BufferedWriter(new FileWriter(outputFile));
 		for(String userId : userId_questionList_map.keySet()){
-			HashMap<String,Integer> tag_count = new HashMap();
+			HashMap<String,Integer> tag_count = new HashMap<>();
 			ArrayList<Question> list = userId_questionList_map.get(userId);
 			for(int i =0; i < list.size(); i++){
 				Question q= list.get(i);
@@ -350,24 +334,23 @@ public class QuestionInformationManager {
 			bw.write("avg:" + average+"\n");
 		}
 		bw.close();
-
 	}
 
-	public HashMap<String, Date> getInformationForUser(String userInfoFile) throws IOException {
+    public HashMap<String, Date> getInformationForUser(String userInfoFile) throws FileNotFoundException, IOException {
         // TODO Auto-generated method stub
         HashMap<String, Date> results = new HashMap<>();
-		BufferedReader br = new BufferedReader(new FileReader(userInfoFile));
+        try (BufferedReader br = new BufferedReader(new FileReader(userInfoFile))) {
             String temp = null;
             int j = 0;
-		while (((temp = br.readLine()) != null) ) 
+            while (((temp = br.readLine()) != null))
 
             {
                 j++;
                 if (j == 1 || j == 2) {
                     continue;
                 }
-			if(j%1000 ==0)
-				System.out.println(j);
+                if (j % 1000 == 0)
+                    LOGGER.info("j = " + j);
 
                 StringBuffer sb = new StringBuffer();
 
@@ -383,29 +366,30 @@ public class QuestionInformationManager {
                     Element ele = rootElt.element("row");
                     String userId = ele.attribute("Id").getText();
 
-				if(!this.user.contains(userId)){
+                    if (!this.user.contains(userId)) {
                         continue;
                     }
 
-                String sDate=ele.attribute("CreationDate").getText().replace("T", " ");
+                    String sDate = ele.attribute("CreationDate").getText().replace("T", " ");
                     DateFormat format = new SimpleDateFormat("yyyy-MM-dd H:m:s.S");
-                Date cDate =format.parse(sDate);
+                    Date cDate = format.parse(sDate);
 
                     results.put(userId, cDate);
 
-                if(results.size() == this.user.size() || j > 200000)
+                    if (results.size() == this.user.size() || j > 200000)
                         break;
-			}catch(Exception e){
+                } catch (Exception e) {
                     e.printStackTrace();
                 }
             }
+        }
         return results;
     }
-
 }
 
 
-class Question{
+class Question implements Serializable{
+    private static final long serialVersionUID = -440464054775600375L;
     private Date createdDate;
 	private int docId;
 	private int rowId;
diff --git a/tag_recommend_code/src/org/preprocess/readUserFromFreecodeJson.java b/tag_recommend_code/src/org/preprocess/ReadUserFromFreecodeJson.java
similarity index 93%
rename from tag_recommend_code/src/org/preprocess/readUserFromFreecodeJson.java
rename to tag_recommend_code/src/org/preprocess/ReadUserFromFreecodeJson.java
index e1de767..ce7472e 100644
--- a/tag_recommend_code/src/org/preprocess/readUserFromFreecodeJson.java
+++ b/tag_recommend_code/src/org/preprocess/ReadUserFromFreecodeJson.java
@@ -1,13 +1,12 @@
 package org.preprocess;
 
 import java.io.FileReader;
-import java.util.Iterator;
 import java.io.*;
 import org.json.simple.JSONArray;
 import org.json.simple.JSONObject;
 import org.json.simple.parser.JSONParser;
 
-public class readUserFromFreecodeJson {
+public class ReadUserFromFreecodeJson {
 
 	/**
 	 * @param args
diff --git a/tag_recommend_code/src/org/preprocess/TokenCleaner.java b/tag_recommend_code/src/org/preprocess/TokenCleaner.java
index 3294a1e..2fdd6d6 100644
--- a/tag_recommend_code/src/org/preprocess/TokenCleaner.java
+++ b/tag_recommend_code/src/org/preprocess/TokenCleaner.java
@@ -3,7 +3,6 @@ package org.preprocess;
 import java.io.BufferedWriter;
 import java.io.File;
 import java.io.FileInputStream;
-import java.io.FileNotFoundException;
 import java.io.FileWriter;
 
 public class TokenCleaner {
@@ -41,7 +40,7 @@ public class TokenCleaner {
 			in.close();
 			String s = new String(readBytes); 
 			s = s.replace("\n", " ").replace("\r", " ").replace(" +", " ");
-			String result =  dataFilter.filter_NL(s);
+			String result =  DataFilter.filter_NL(s);
 			BufferedWriter bw = new BufferedWriter(new FileWriter(outFile));
 			bw.write(result);
 			bw.close();
diff --git a/tag_recommend_code/src/org/preprocess/htmlFilter.java b/tag_recommend_code/src/org/preprocess/htmlFilter.java
deleted file mode 100644
index 96301cf..0000000
--- a/tag_recommend_code/src/org/preprocess/htmlFilter.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.preprocess;
-
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileNotFoundException;
-import java.io.FileWriter;
-
-public class htmlFilter {
-
-	/**
-	 * @param args
-	 */
-	public static void main(String[] args) {
-		// TODO Auto-generated method stub
-		//tokenCleanForDir("O:/shaowei/folksonomy/freecode/rawdata/", "O:/shaowei/folksonomy/freecode/htmlFilterred");
-		//tokenCleanForDir("G:/research/tag_recommendation/data_and_results/ChangeTagData/rawdata", "G:/research/tag_recommendation/data_and_results/ChangeTagData/htmlFilterred");
-		
-		if(args.length < 1){
-			System.out.println("usage [root path]");
-			return ;
-		}
-		String root = args[0];
-		tokenCleanForDir(root + "rawdata", root + "htmlFilterred");
-		
-	}
-	
-	public static void tokenCleanForDir(String inDir, String outDir){
-		File[] fileList = new File(inDir).listFiles();
-		if(!new File(outDir).exists())
-			new File(outDir).mkdirs();
-		
-			
-		int count = 0;
-		for(File f : fileList){
-			
-			tokenCleanerForFile(f.getAbsolutePath(), outDir+"/"+f.getName());
-			System.out.println(count++);
-		}
-	}
-	
-	
-	public static void tokenCleanerForFile(String inFile, String outFile){
-		FileInputStream in;
-		try {
-			in = new FileInputStream(inFile);
-			byte[] readBytes = new byte[in.available()];
-			in.read(readBytes);
-			in.close();
-			String s = new String(readBytes); 
-			s = s.replace("\n", " ").replace("\r", " ").replace(" +", " ");
-			String result =  dataFilter.filter_html(s);
-			BufferedWriter bw = new BufferedWriter(new FileWriter(outFile));
-			bw.write(result);
-			bw.close();
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-		
-	}
-	
-
-}
diff --git a/tag_recommend_code/src/org/preprocess/rawTextDataPreprocessor.java b/tag_recommend_code/src/org/preprocess/rawTextDataPreprocessor.java
index 3c15133..0f35298 100644
--- a/tag_recommend_code/src/org/preprocess/rawTextDataPreprocessor.java
+++ b/tag_recommend_code/src/org/preprocess/rawTextDataPreprocessor.java
@@ -5,62 +5,73 @@ import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.FileWriter;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
 
-import sun.usagetracker.UsageTrackerClient;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class rawTextDataPreprocessor {
+    private final static Logger LOGGER = LoggerFactory.getLogger(rawTextDataPreprocessor.class);
 
     /**
      * @param args
      */
     public static void main(String[] args) {
-		// TODO Auto-generated method stub
-		//tokenCleanForDir("O:/shaowei/folksonomy/freecode/rawdata/", "O:/shaowei/folksonomy/freecode/htmlFilterred");
-		//tokenCleanForDir("G:/research/tag_recommendation/data_and_results/ChangeTagData/rawdata", "G:/research/tag_recommendation/data_and_results/ChangeTagData/descriptionCleaned");
-		if(args.length <1){
-			System.out.println("usage [root of a dataset]" );
-			return ;
+        if (args.length < 1) {
+            System.out.println("usage [root of a dataset]");
+            return;
         }
 
         String root = args[0];
-		tokenCleanForDir(root + "rawdata",  root +"descriptionCleaned");
+        try {
+            tokenCleanForDir(root + "rawdata", root + "descriptionCleaned");
+        } catch (FileNotFoundException e) {
+            LOGGER.error(e.getMessage(), e);
+        } catch (IOException e) {
+            LOGGER.error(e.getMessage(), e);
+        }
+    }
 
+    public static void tokenCleanForDir(String inDir, String outDir) throws IOException {
+        if (!Files.exists(Paths.get(inDir)) || !Files.isDirectory(Paths.get(inDir))) {
+            LOGGER.error("Directory at " + inDir + " expected");
+            throw new IllegalStateException("Directory at " + inDir + " expected");
         }
         
-	public static void tokenCleanForDir(String inDir, String outDir){
         File[] fileList = new File(inDir).listFiles();
-		if(!new File(outDir).exists())
-			new File(outDir).mkdirs();
+        if (fileList == null) {
+            LOGGER.error("Failed to list directories or files in " + inDir);
+            throw new IllegalStateException("Failed to list directories or files in " + inDir);
+        }
         
+        if (!Files.exists(Paths.get(outDir))) {
+            Files.createDirectories(Paths.get(outDir));
+        }
 
         int count = 0;
-		for(File f : fileList){
-			
-			tokenCleanerForFile(f.getAbsolutePath(), outDir+"/"+f.getName());
-			System.out.println(count++);
+        for (File f : fileList) {
+            tokenCleanerForFile(f.getAbsolutePath(), outDir + "/" + f.getName());
+            LOGGER.info("counter = " + count++);
         }
     }
 
+    public static void tokenCleanerForFile(String inFile, String outFile) throws FileNotFoundException, IOException {
+        byte[] readBytes = null;
 
-	public static void tokenCleanerForFile(String inFile, String outFile){
-		FileInputStream in;
-		try {
-			in = new FileInputStream(inFile);
-			byte[] readBytes = new byte[in.available()];
+        try (FileInputStream in = new FileInputStream(inFile)) {
+            readBytes = new byte[in.available()];
             in.read(readBytes);
-			in.close();
+        }
+
         String s = new String(readBytes);
         s = s.replace("\n", " ").replace("\r", " ").replace(" +", " ");
-			String result =  dataFilter.filter_Code(s);
-			BufferedWriter bw = new BufferedWriter(new FileWriter(outFile));
+        String result = DataFilter.filter_Code(s);
+
+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(outFile))) {
             bw.write(result);
-			bw.close();
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
         }
-		
     }
 
-
 }
diff --git a/tag_recommend_code/src/queryExpansion/Query.java b/tag_recommend_code/src/queryExpansion/Query.java
index 818a69c..44c8290 100644
--- a/tag_recommend_code/src/queryExpansion/Query.java
+++ b/tag_recommend_code/src/queryExpansion/Query.java
@@ -34,15 +34,15 @@ public class Query {
 	
 	public Query(String original){
 		this.inferredTagText = original;
-		this.extendedTags = new HashMap();
-		this.inferedTagsFromLLDA = new HashMap();
-		this.termTagInferTags = new HashMap();
-		this.inferedTagsFromUser = new HashMap();
-		this.inferedTagsFromAsscoiatedRule = new HashMap();
+		this.extendedTags = new HashMap<>();
+		this.inferedTagsFromLLDA = new HashMap<>();
+		this.termTagInferTags = new HashMap<>();
+		this.inferedTagsFromUser = new HashMap<>();
+		this.inferedTagsFromAsscoiatedRule = new HashMap<>();
 	}
 	
 	public ArrayList<String> getTags(){
-		ArrayList<String> tags = new ArrayList();
+		ArrayList<String> tags = new ArrayList<>();
 		
 		for(Node n :inferedTagsFromLLDA.values()){
 			tags.add(n.getName());
@@ -189,17 +189,17 @@ public class Query {
 
 	public ArrayList<Node> getLinearCombineTopk(int topK, double[] paras) {
 		// TODO Auto-generated method stub
-		ArrayList<Node> result = new ArrayList();
+		ArrayList<Node> result = new ArrayList<>();
 		//
-		Set<String> set = new HashSet();
+		Set<String> set = new HashSet<>();
 		Set<String> set1 = this.termTagInferTags.keySet();
 		Set<String> set2 = this.inferedTagsFromLLDA.keySet();
 		Set<String> set3 = this.inferedTagsFromUser.keySet();
 		Set<String> set4 = this.inferedTagsFromAsscoiatedRule.keySet();
-		set.addAll(new ArrayList(set1));
-		set.addAll(new ArrayList(set2));
-		set.addAll(new ArrayList(set3));
-		set.addAll(new ArrayList(set4));
+		set.addAll(new ArrayList<>(set1));
+		set.addAll(new ArrayList<>(set2));
+		set.addAll(new ArrayList<>(set3));
+		set.addAll(new ArrayList<>(set4));
 		for(String tag : set){
 		//for(String tag : set1){
 			
@@ -243,17 +243,17 @@ public class Query {
 	
 	public ArrayList<Node> getLinearCombineTopk(int topK, double[] paras, boolean normalization) {
 		// TODO Auto-generated method stub
-		ArrayList<Node> result = new ArrayList();
+		ArrayList<Node> result = new ArrayList<>();
 		//
-		Set<String> set = new HashSet();
+		Set<String> set = new HashSet<>();
 		Set<String> set1 = this.termTagInferTags.keySet();
 		Set<String> set2 = this.inferedTagsFromLLDA.keySet();
 		Set<String> set3 = this.inferedTagsFromUser.keySet();
 		Set<String> set4 = this.inferedTagsFromAsscoiatedRule.keySet();
-		set.addAll(new ArrayList(set1));
-		set.addAll(new ArrayList(set2));
-		set.addAll(new ArrayList(set3));
-		set.addAll(new ArrayList(set4));
+		set.addAll(new ArrayList<>(set1));
+		set.addAll(new ArrayList<>(set2));
+		set.addAll(new ArrayList<>(set3));
+		set.addAll(new ArrayList<>(set4));
 		
 		// normalization
 		if(normalization){
@@ -347,14 +347,14 @@ public class Query {
 	// get kop 50 tags from  two list 
 	public ArrayList<String> getTopk(int k) {
 		// TODO Auto-generated method stub
-		ArrayList<Node> list1 = new ArrayList();
-		ArrayList<Node> list2 = new ArrayList();
+		ArrayList<Node> list1 = new ArrayList<>();
+		ArrayList<Node> list2 = new ArrayList<>();
 		list1.addAll(this.inferedTagsFromLLDA.values());
 		list2.addAll(this.termTagInferTags.values());
 		ArrayList<Node> topKNode1 =  Graph.getTopKTags(k, list1);
 		ArrayList<Node> topKNode2 =  Graph.getTopKTags(k, list2);
 		
-		ArrayList<String> results = new ArrayList();
+		ArrayList<String> results = new ArrayList<>();
 		
 		for(Node n : topKNode1){
 			results.add(n.getName());
diff --git a/tag_recommend_code/src/queryExpansion/run.java b/tag_recommend_code/src/queryExpansion/Run.java
similarity index 52%
rename from tag_recommend_code/src/queryExpansion/run.java
rename to tag_recommend_code/src/queryExpansion/Run.java
index fbbb91e..d8cf2dd 100644
--- a/tag_recommend_code/src/queryExpansion/run.java
+++ b/tag_recommend_code/src/queryExpansion/Run.java
@@ -1,16 +1,15 @@
 package queryExpansion;
 
 import java.io.BufferedReader;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.util.ArrayList;
 import java.util.HashMap;
 
-import Network.Graph;
-import topicinfer.TopicInfer;
+//import org.slf4j.Logger;
+//import org.slf4j.LoggerFactory;
 
-
-public class run {
+public class Run {
+//    private final static Logger LOGGER = LoggerFactory.getLogger(Run.class);
 	
 	// define the number of inferred number return
 	final static int infer_topK = 10;
@@ -50,19 +49,16 @@ public class run {
         
     }*/
     
-    public static void loadInferredTopic(String topicInferredOutPath,
-			ArrayList<Query> queryList) {
-		// TODO Auto-generated method stub
-    	try {
-			BufferedReader br = new BufferedReader(new FileReader(topicInferredOutPath));
+    public static void loadInferredTopic(String topicInferredOutPath, ArrayList<Query> queryList) {
+        try (BufferedReader br = new BufferedReader(new FileReader(topicInferredOutPath))) {
             String line = null;
 
-			while((line = br.readLine())!=null){
+            while ((line = br.readLine()) != null) {
                 String[] sparts = line.split(",");
                 Query q = queryList.get(Integer.parseInt(sparts[0]));
-				for(int i =1; i < sparts.length; i++){
+                for (int i = 1; i < sparts.length; i++) {
                     String[] strs = sparts[i].split(":");
-					q.addInfTags(strs[0],Double.parseDouble(strs[1]));
+                    q.addInfTags(strs[0], Double.parseDouble(strs[1]));
                 }
             }
         } catch (Exception e) {
@@ -73,19 +69,26 @@ public class run {
     }
     
 	public static ArrayList<Query> loadQueryFromFile(String path, String datapath){
-    	ArrayList<Query> ql = new ArrayList();
+    	ArrayList<Query> ql = new ArrayList<>();
     	HashMap<String, String> map = lineNum_queryid(datapath);
-    	try {
-			BufferedReader br = new BufferedReader(new FileReader(path));
+    	try (BufferedReader br = new BufferedReader(new FileReader(path))) {
 			String line = null;
 			
+//			int qNum = 0;
 			while((line = br.readLine())!=null){
+			    if (line.isEmpty()) continue;
+//			    LOGGER.info("line = " + line);
 				String[] spart = line.split(",");
 				String queryText = spart[1];
+//				LOGGER.info("spart[0] = " + spart[0]);
+//                LOGGER.info("spart[1] = " + spart[1]);
 				Query q = new Query(queryText);
 				q.id = spart[0];
 				q.query_id = map.get(q.id);
+//				LOGGER.info("(line, q.id, q.query_id) = (" + line + "," + q.id + "," + q.query_id + ")");
 				ql.add(q);
+//				LOGGER.info("Added query " + qNum);
+//				qNum ++;
 			}
 		} catch (Exception e) {
 			// TODO Auto-generated catch block
@@ -94,13 +97,12 @@ public class run {
     	return ql;
     }
     
-	public static HashMap<String, String> lineNum_queryid (String path){
-		HashMap<String, String> result = new HashMap();
-		try{
-		BufferedReader br = new BufferedReader(new FileReader(path));
+    public static HashMap<String, String> lineNum_queryid(String path) {
+        HashMap<String, String> result = new HashMap<>();
+        try (BufferedReader br = new BufferedReader(new FileReader(path))) {
             String line = null;
             int id = 0;
-		while((line = br.readLine())!=null){
+            while ((line = br.readLine()) != null) {
                 String[] sparts = line.split(",");
 
                 String queryid = sparts[0];
@@ -108,7 +110,7 @@ public class run {
 
                 id++;
             }
-		}catch (Exception e) {
+        } catch (Exception e) {
             // TODO Auto-generated catch block
             e.printStackTrace();
         }
diff --git a/tag_recommend_code/src/tagRecommend/Fre_Baye_IndividualApproachTest.java b/tag_recommend_code/src/tagRecommend/Fre_Baye_IndividualApproachTest.java
index baae8f7..bc60653 100644
--- a/tag_recommend_code/src/tagRecommend/Fre_Baye_IndividualApproachTest.java
+++ b/tag_recommend_code/src/tagRecommend/Fre_Baye_IndividualApproachTest.java
@@ -3,14 +3,12 @@ package tagRecommend;
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
 import java.io.File;
-import java.io.FileNotFoundException;
 import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 
-import edu.stanford.nlp.tmt.model.llda.*;
 import Network.Graph;
 import Network.Node;
 import Network.Tag;
@@ -18,7 +16,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +41,7 @@ public class Fre_Baye_IndividualApproachTest {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -81,8 +79,8 @@ public class Fre_Baye_IndividualApproachTest {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result = new HashMap();
-				HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result = new HashMap<>();
+//				HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -112,7 +110,7 @@ public class Fre_Baye_IndividualApproachTest {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -122,7 +120,7 @@ public class Fre_Baye_IndividualApproachTest {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -150,24 +148,24 @@ public class Fre_Baye_IndividualApproachTest {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -182,7 +180,7 @@ public class Fre_Baye_IndividualApproachTest {
 						// outputQuery(currentTestDir+"\\before_traindata",queryListForTraining);
 
 						// query expansion
-						int qid = 1;
+//						int qid = 1;
 						if (expansion) {
 							// generate network
 							String graphInputForTesting = currentTestDir
@@ -299,7 +297,7 @@ public class Fre_Baye_IndividualApproachTest {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -308,7 +306,7 @@ public class Fre_Baye_IndividualApproachTest {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -349,7 +347,7 @@ public class Fre_Baye_IndividualApproachTest {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -360,7 +358,7 @@ public class Fre_Baye_IndividualApproachTest {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -397,8 +395,8 @@ public class Fre_Baye_IndividualApproachTest {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -459,8 +457,8 @@ public class Fre_Baye_IndividualApproachTest {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -484,7 +482,7 @@ public class Fre_Baye_IndividualApproachTest {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -493,7 +491,7 @@ public class Fre_Baye_IndividualApproachTest {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -533,7 +531,7 @@ public class Fre_Baye_IndividualApproachTest {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/IndividualApproach.java b/tag_recommend_code/src/tagRecommend/IndividualApproach.java
index 041b8d3..7c9771f 100644
--- a/tag_recommend_code/src/tagRecommend/IndividualApproach.java
+++ b/tag_recommend_code/src/tagRecommend/IndividualApproach.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class IndividualApproach {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -82,8 +82,8 @@ public class IndividualApproach {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result = new HashMap();
-				HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result = new HashMap<>();
+				HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -113,7 +113,7 @@ public class IndividualApproach {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -123,7 +123,7 @@ public class IndividualApproach {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -151,24 +151,24 @@ public class IndividualApproach {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -300,7 +300,7 @@ public class IndividualApproach {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -309,7 +309,7 @@ public class IndividualApproach {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -350,7 +350,7 @@ public class IndividualApproach {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -361,7 +361,7 @@ public class IndividualApproach {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -398,8 +398,8 @@ public class IndividualApproach {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -460,8 +460,8 @@ public class IndividualApproach {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -485,7 +485,7 @@ public class IndividualApproach {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -494,7 +494,7 @@ public class IndividualApproach {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -534,7 +534,7 @@ public class IndividualApproach {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/LinearCombinationTrainerAndTester.java b/tag_recommend_code/src/tagRecommend/LinearCombinationTrainerAndTester.java
index 2bab0e2..7b2150e 100644
--- a/tag_recommend_code/src/tagRecommend/LinearCombinationTrainerAndTester.java
+++ b/tag_recommend_code/src/tagRecommend/LinearCombinationTrainerAndTester.java
@@ -22,7 +22,7 @@ public class LinearCombinationTrainerAndTester {
 	}
 	//return recalls
 	public HashMap<String, Double> performCrossValidation(int k){
-		HashMap<String, Double> recalls =new  HashMap();
+		HashMap<String, Double> recalls =new  HashMap<>();
 		ArrayList<Query> remainTest = (ArrayList<Query>) data.clone();
 		int[] testCaseNumberPerCross = new int[10];
 		int average = data.size() / k;
@@ -38,8 +38,8 @@ public class LinearCombinationTrainerAndTester {
 		// perform k cross validation
 		for (int i = 0; i < k; i++) {
 			// get test data
-			ArrayList<Query> test = new ArrayList();
-			ArrayList<Query> train = new ArrayList();
+			ArrayList<Query> test = new ArrayList<>();
+			ArrayList<Query> train = new ArrayList<>();
 			for(int j =0; j < testCaseNumberPerCross[i];j++){
 				Random rn = new Random();
 				int random =0;
@@ -199,7 +199,7 @@ public class LinearCombinationTrainerAndTester {
 		for(Query q : test){
 			
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(topK, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -221,11 +221,11 @@ public class LinearCombinationTrainerAndTester {
 	public HashMap<String,Double> calculateRecalls(double[] paras,ArrayList<Query> test, 
 			HashMap<String,List<String>> exclusiveTags) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			int exclusiveTagN = exclusiveTags.get(q.query_id).size();
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(exclusiveTagN+topK, paras,false);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -270,11 +270,11 @@ public class LinearCombinationTrainerAndTester {
 	
 	public HashMap<String,Double> calculateRecalls(double[] paras,ArrayList<Query> test) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(topK, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -296,11 +296,11 @@ public class LinearCombinationTrainerAndTester {
 	
 	public HashMap<String,Double> calculatPrecision(double[] paras,ArrayList<Query> test) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(topK, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -321,12 +321,12 @@ public class LinearCombinationTrainerAndTester {
 	public HashMap<String,Double> calculatPrecision(double[] paras,ArrayList<Query> test, 
 			HashMap<String, List<String>> exclusiveTags) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			
 			int exclusiveTagN = exclusiveTags.get(q.query_id).size();
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(topK + exclusiveTagN, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -363,12 +363,12 @@ public class LinearCombinationTrainerAndTester {
 	public HashMap<String,Double> calculatPrecision(double[] paras,ArrayList<Query> test, 
 			HashMap<String, List<String>> exclusiveTags, int exclusiveTagN) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			
 			
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(topK + exclusiveTagN, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
@@ -405,11 +405,11 @@ public class LinearCombinationTrainerAndTester {
 	
 	public HashMap<String,Double> calculateRecalls(double[] paras,ArrayList<Query> test, int cur_topk) {
 		// TODO Auto-generated method stub
-		HashMap<String,Double> result = new HashMap();
+		HashMap<String,Double> result = new HashMap<>();
 		for(Query q : test){
 			
 			ArrayList<Node> topNode =  q.getLinearCombineTopk(cur_topk, paras);
-			ArrayList<String> topTags = new ArrayList();
+			ArrayList<String> topTags = new ArrayList<>();
 			for(Node n : topNode)
 				topTags.add(n.getName());
 			String id = q.query_id;
diff --git a/tag_recommend_code/src/tagRecommend/RunKtimesForEffectSizeTest.java b/tag_recommend_code/src/tagRecommend/RunKtimesForEffectSizeTest.java
index 5b15263..63a5704 100644
--- a/tag_recommend_code/src/tagRecommend/RunKtimesForEffectSizeTest.java
+++ b/tag_recommend_code/src/tagRecommend/RunKtimesForEffectSizeTest.java
@@ -24,7 +24,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 import tagRecommend.*;
 
@@ -53,7 +53,7 @@ public class RunKtimesForEffectSizeTest {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) throws IOException {
 		// TODO Auto-generated method stub
@@ -79,7 +79,7 @@ public class RunKtimesForEffectSizeTest {
 
 			logFile = TestDir + "log_" + dateFormat.format(date) + ".txt";
 
-			HashMap<String, Double> final_results = new HashMap();
+			HashMap<String, Double> final_results = new HashMap<>();
 			// String root =
 			// "G:\\research\\tag_recommendation\\folksonomy\\freecode\\";
 			
@@ -99,12 +99,12 @@ public class RunKtimesForEffectSizeTest {
 								tenCrossValidation.crossNumber, TestDir);
 					}
 
-					HashMap<String, Double> result_recall = new HashMap();
-					HashMap<String, Double> result_precision = new HashMap();
+					HashMap<String, Double> result_recall = new HashMap<>();
+					HashMap<String, Double> result_precision = new HashMap<>();
 
 				
 
-					// HashMap<String,Double> result_10= new HashMap();
+					// HashMap<String,Double> result_10= new HashMap<>();
 					// perform cross validation
 					try {
 						for (int i = 0; i < crossNumber; i++) {
@@ -134,7 +134,7 @@ public class RunKtimesForEffectSizeTest {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									trainingData, inferTopicForTrainingPath,
 									infer_topK);
-							ArrayList<Query> queryListForTraining = run
+							ArrayList<Query> queryListForTraining = Run
 									.loadQueryFromFile(
 											inferTopicForTrainingPath,
 											trainingData);
@@ -146,7 +146,7 @@ public class RunKtimesForEffectSizeTest {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									testData, inferTopicPathForTesting,
 									infer_topK);
-							ArrayList<Query> queryListForTesting = run
+							ArrayList<Query> queryListForTesting = Run
 									.loadQueryFromFile(
 											inferTopicPathForTesting, testData);
 
@@ -176,9 +176,9 @@ public class RunKtimesForEffectSizeTest {
 
 							// infer topic llda
 							if (topicInfer) {
-								run.loadInferredTopic(inferTopicPathForTesting,
+								Run.loadInferredTopic(inferTopicPathForTesting,
 										queryListForTesting);
-								run.loadInferredTopic(
+								Run.loadInferredTopic(
 										inferTopicForTrainingPath,
 										queryListForTraining);
 							}
@@ -186,17 +186,17 @@ public class RunKtimesForEffectSizeTest {
 							// infer topic from term_tag_index
 							if (termTagIndex) {
 								TermTagIndex tti = new TermTagIndex();
-								tti.LoadIndexFromFile(root
+								tti.loadIndexFromFile(root
 										+ "term_tag_index.txt");
 								for (Query q : queryListForTesting) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
 								}
 
 								for (Query q : queryListForTraining) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
@@ -358,7 +358,7 @@ public class RunKtimesForEffectSizeTest {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -367,7 +367,7 @@ public class RunKtimesForEffectSizeTest {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -408,7 +408,7 @@ public class RunKtimesForEffectSizeTest {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -419,7 +419,7 @@ public class RunKtimesForEffectSizeTest {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -456,8 +456,8 @@ public class RunKtimesForEffectSizeTest {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -518,8 +518,8 @@ public class RunKtimesForEffectSizeTest {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -543,7 +543,7 @@ public class RunKtimesForEffectSizeTest {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -552,7 +552,7 @@ public class RunKtimesForEffectSizeTest {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -592,7 +592,7 @@ public class RunKtimesForEffectSizeTest {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/TestonKvaryWithOnefixed.java b/tag_recommend_code/src/tagRecommend/TestonKvaryWithOnefixed.java
index c067353..c6a989e 100644
--- a/tag_recommend_code/src/tagRecommend/TestonKvaryWithOnefixed.java
+++ b/tag_recommend_code/src/tagRecommend/TestonKvaryWithOnefixed.java
@@ -22,7 +22,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -47,7 +47,7 @@ public class TestonKvaryWithOnefixed {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -89,8 +89,8 @@ public class TestonKvaryWithOnefixed {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result = new HashMap();
-				HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result = new HashMap<>();
+				HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -122,7 +122,7 @@ public class TestonKvaryWithOnefixed {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -132,7 +132,7 @@ public class TestonKvaryWithOnefixed {
 								+ "/query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -163,9 +163,9 @@ public class TestonKvaryWithOnefixed {
 							System.out.println(dateFormat.format(cal.getTime()));
 							cal = Calendar.getInstance();
 							System.out.println("start topic infer");
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
@@ -175,15 +175,15 @@ public class TestonKvaryWithOnefixed {
 							cal = Calendar.getInstance();
 							System.out.println("start term infer");
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "/term_tag_index.txt");
+							tti.loadIndexFromFile(root + "/term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -319,7 +319,7 @@ public class TestonKvaryWithOnefixed {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -328,7 +328,7 @@ public class TestonKvaryWithOnefixed {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -369,7 +369,7 @@ public class TestonKvaryWithOnefixed {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -380,7 +380,7 @@ public class TestonKvaryWithOnefixed {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -417,8 +417,8 @@ public class TestonKvaryWithOnefixed {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -479,8 +479,8 @@ public class TestonKvaryWithOnefixed {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -504,7 +504,7 @@ public class TestonKvaryWithOnefixed {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -513,7 +513,7 @@ public class TestonKvaryWithOnefixed {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -553,7 +553,7 @@ public class TestonKvaryWithOnefixed {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/TestonKvary_cp2.java b/tag_recommend_code/src/tagRecommend/TestonKvary_cp2.java
index 0ca6852..1514ffa 100644
--- a/tag_recommend_code/src/tagRecommend/TestonKvary_cp2.java
+++ b/tag_recommend_code/src/tagRecommend/TestonKvary_cp2.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class TestonKvary_cp2 {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -70,8 +70,8 @@ public class TestonKvary_cp2 {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result = new HashMap();
-				HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result = new HashMap<>();
+				HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -101,7 +101,7 @@ public class TestonKvary_cp2 {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -111,7 +111,7 @@ public class TestonKvary_cp2 {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -139,24 +139,24 @@ public class TestonKvary_cp2 {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -288,7 +288,7 @@ public class TestonKvary_cp2 {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -297,7 +297,7 @@ public class TestonKvary_cp2 {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -338,7 +338,7 @@ public class TestonKvary_cp2 {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -349,7 +349,7 @@ public class TestonKvary_cp2 {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -386,8 +386,8 @@ public class TestonKvary_cp2 {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -448,8 +448,8 @@ public class TestonKvary_cp2 {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -473,7 +473,7 @@ public class TestonKvary_cp2 {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -482,7 +482,7 @@ public class TestonKvary_cp2 {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -522,7 +522,7 @@ public class TestonKvary_cp2 {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/TestonKvary_recall_precision.java b/tag_recommend_code/src/tagRecommend/TestonKvary_recall_precision.java
index 197b7cd..bf6549f 100644
--- a/tag_recommend_code/src/tagRecommend/TestonKvary_recall_precision.java
+++ b/tag_recommend_code/src/tagRecommend/TestonKvary_recall_precision.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class TestonKvary_recall_precision {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -69,9 +69,9 @@ public class TestonKvary_recall_precision {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result_recall = new HashMap();
-				HashMap<String,Double> result_precision = new HashMap();
-				//HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result_recall = new HashMap<>();
+				HashMap<String,Double> result_precision = new HashMap<>();
+				//HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -101,7 +101,7 @@ public class TestonKvary_recall_precision {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -111,7 +111,7 @@ public class TestonKvary_recall_precision {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -139,24 +139,24 @@ public class TestonKvary_recall_precision {
 
 						// infer topic llda
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
 						// infer topic from term_tag_index
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -313,7 +313,7 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -322,7 +322,7 @@ public class TestonKvary_recall_precision {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -363,7 +363,7 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -374,7 +374,7 @@ public class TestonKvary_recall_precision {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -411,8 +411,8 @@ public class TestonKvary_recall_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -473,8 +473,8 @@ public class TestonKvary_recall_precision {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -498,7 +498,7 @@ public class TestonKvary_recall_precision {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -507,7 +507,7 @@ public class TestonKvary_recall_precision {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -547,7 +547,7 @@ public class TestonKvary_recall_precision {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData.java b/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData.java
index 94863a8..2d94166 100644
--- a/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData.java
+++ b/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class TestvaryingTrainingData {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -59,7 +59,7 @@ public class TestvaryingTrainingData {
 		if (generateCrossData) {
 			generateTestData_Distribution(inputData, crossNumber, TestDir);
 		}
-		HashMap<String, Double> result = new HashMap();
+		HashMap<String, Double> result = new HashMap<>();
 
 		// perform cross validation
 		try {
@@ -92,7 +92,7 @@ public class TestvaryingTrainingData {
 						+ "\\query-training-out.csv";
 				TopicInfer.getTopKTopic(model, outputModelDir, trainingData,
 						inferTopicForTrainingPath, infer_topk);
-				ArrayList<Query> queryListForTraining = run.loadQueryFromFile(
+				ArrayList<Query> queryListForTraining = Run.loadQueryFromFile(
 						inferTopicForTrainingPath, trainingData);
 
 				// construct testing query
@@ -101,7 +101,7 @@ public class TestvaryingTrainingData {
 						+ "\\query-testing-out.csv";
 				TopicInfer.getTopKTopic(model, outputModelDir, testData,
 						inferTopicPathForTesting, infer_topk);
-				ArrayList<Query> queryListForTesting = run.loadQueryFromFile(
+				ArrayList<Query> queryListForTesting = Run.loadQueryFromFile(
 						inferTopicPathForTesting, testData);
 
 				// load testing data text
@@ -128,23 +128,23 @@ public class TestvaryingTrainingData {
 
 				// infer topic llda
 				if (topicInfer) {
-					run.loadInferredTopic(inferTopicPathForTesting,
+					Run.loadInferredTopic(inferTopicPathForTesting,
 							queryListForTesting);
-					run.loadInferredTopic(inferTopicForTrainingPath,
+					Run.loadInferredTopic(inferTopicForTrainingPath,
 							queryListForTraining);
 				}
 
 				// infer topic from term_tag_index
 				if (termTagIndex) {
 					TermTagIndex tti = new TermTagIndex();
-					tti.LoadIndexFromFile(root + "term_tag_index.txt");
+					tti.loadIndexFromFile(root + "term_tag_index.txt");
 					for (Query q : queryListForTesting) {
-						tti.AssignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+						tti.assignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 								termTagIndex_topK);
 					}
 
 					for (Query q : queryListForTraining) {
-						tti.AssignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+						tti.assignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 								termTagIndex_topK);
 					}
 
@@ -270,7 +270,7 @@ public class TestvaryingTrainingData {
 	}
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -279,7 +279,7 @@ public class TestvaryingTrainingData {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -320,7 +320,7 @@ public class TestvaryingTrainingData {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -331,7 +331,7 @@ public class TestvaryingTrainingData {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -364,8 +364,8 @@ public class TestvaryingTrainingData {
 				new File(subTestDir).mkdirs();
 			docList = loadDatasetbasedOnTag(dataset);
 			remainingTest = (HashMap<String, Doc>) docList.clone();
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 
 			// pick test set from each tag
 
@@ -415,8 +415,8 @@ public class TestvaryingTrainingData {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -440,7 +440,7 @@ public class TestvaryingTrainingData {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -449,7 +449,7 @@ public class TestvaryingTrainingData {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -489,7 +489,7 @@ public class TestvaryingTrainingData {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData_cp1.java b/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData_cp1.java
index fa8b7c1..24b8580 100644
--- a/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData_cp1.java
+++ b/tag_recommend_code/src/tagRecommend/TestvaryingTrainingData_cp1.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class TestvaryingTrainingData_cp1 {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -59,7 +59,7 @@ public class TestvaryingTrainingData_cp1 {
 		if (generateCrossData) {
 			generateTestData_Distribution(inputData, crossNumber, TestDir);
 		}
-		HashMap<String, Double> result = new HashMap();
+		HashMap<String, Double> result = new HashMap<>();
 
 		// perform cross validation
 		try {
@@ -92,7 +92,7 @@ public class TestvaryingTrainingData_cp1 {
 						+ "\\query-training-out.csv";
 				TopicInfer.getTopKTopic(model, outputModelDir, trainingData,
 						inferTopicForTrainingPath, infer_topk);
-				ArrayList<Query> queryListForTraining = run.loadQueryFromFile(
+				ArrayList<Query> queryListForTraining = Run.loadQueryFromFile(
 						inferTopicForTrainingPath, trainingData);
 
 				// construct testing query
@@ -101,7 +101,7 @@ public class TestvaryingTrainingData_cp1 {
 						+ "\\query-testing-out.csv";
 				TopicInfer.getTopKTopic(model, outputModelDir, testData,
 						inferTopicPathForTesting, infer_topk);
-				ArrayList<Query> queryListForTesting = run.loadQueryFromFile(
+				ArrayList<Query> queryListForTesting = Run.loadQueryFromFile(
 						inferTopicPathForTesting, testData);
 
 				// load testing data text
@@ -128,23 +128,23 @@ public class TestvaryingTrainingData_cp1 {
 
 				// infer topic llda
 				if (topicInfer) {
-					run.loadInferredTopic(inferTopicPathForTesting,
+					Run.loadInferredTopic(inferTopicPathForTesting,
 							queryListForTesting);
-					run.loadInferredTopic(inferTopicForTrainingPath,
+					Run.loadInferredTopic(inferTopicForTrainingPath,
 							queryListForTraining);
 				}
 
 				// infer topic from term_tag_index
 				if (termTagIndex) {
 					TermTagIndex tti = new TermTagIndex();
-					tti.LoadIndexFromFile(root + "term_tag_index.txt");
+					tti.loadIndexFromFile(root + "term_tag_index.txt");
 					for (Query q : queryListForTesting) {
-						tti.AssignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+						tti.assignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 								termTagIndex_topK);
 					}
 
 					for (Query q : queryListForTraining) {
-						tti.AssignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
+						tti.assignTags(q, TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 								termTagIndex_topK);
 					}
 
@@ -270,7 +270,7 @@ public class TestvaryingTrainingData_cp1 {
 	}
 	
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -279,7 +279,7 @@ public class TestvaryingTrainingData_cp1 {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -320,7 +320,7 @@ public class TestvaryingTrainingData_cp1 {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -331,7 +331,7 @@ public class TestvaryingTrainingData_cp1 {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -364,8 +364,8 @@ public class TestvaryingTrainingData_cp1 {
 				new File(subTestDir).mkdirs();
 			docList = loadDatasetbasedOnTag(dataset);
 			remainingTest = (HashMap<String, Doc>) docList.clone();
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 
 			// pick test set from each tag
 
@@ -415,8 +415,8 @@ public class TestvaryingTrainingData_cp1 {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -440,7 +440,7 @@ public class TestvaryingTrainingData_cp1 {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -449,7 +449,7 @@ public class TestvaryingTrainingData_cp1 {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -489,7 +489,7 @@ public class TestvaryingTrainingData_cp1 {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/b_f_individule_precision.java b/tag_recommend_code/src/tagRecommend/b_f_individule_precision.java
index cafb45a..3eb76e2 100644
--- a/tag_recommend_code/src/tagRecommend/b_f_individule_precision.java
+++ b/tag_recommend_code/src/tagRecommend/b_f_individule_precision.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -43,7 +43,7 @@ public class b_f_individule_precision {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -84,9 +84,9 @@ public class b_f_individule_precision {
 								tenCrossValidation.crossNumber, TestDir);
 					}
 
-					HashMap<String, Double> result_recall = new HashMap();
-					HashMap<String, Double> result_precision = new HashMap();
-					// HashMap<String,Double> result_10= new HashMap();
+					HashMap<String, Double> result_recall = new HashMap<>();
+					HashMap<String, Double> result_precision = new HashMap<>();
+					// HashMap<String,Double> result_10= new HashMap<>();
 					// perform cross validation
 					try {
 						for (int i = 0; i < crossNumber; i++) {
@@ -116,7 +116,7 @@ public class b_f_individule_precision {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									trainingData, inferTopicForTrainingPath,
 									infer_topK);
-							ArrayList<Query> queryListForTraining = run
+							ArrayList<Query> queryListForTraining = Run
 									.loadQueryFromFile(
 											inferTopicForTrainingPath,
 											trainingData);
@@ -128,7 +128,7 @@ public class b_f_individule_precision {
 							TopicInfer.getTopKTopic(model, outputModelDir,
 									testData, inferTopicPathForTesting,
 									infer_topK);
-							ArrayList<Query> queryListForTesting = run
+							ArrayList<Query> queryListForTesting = Run
 									.loadQueryFromFile(
 											inferTopicPathForTesting, testData);
 
@@ -156,9 +156,9 @@ public class b_f_individule_precision {
 
 							// infer topic llda
 							if (topicInfer) {
-								run.loadInferredTopic(inferTopicPathForTesting,
+								Run.loadInferredTopic(inferTopicPathForTesting,
 										queryListForTesting);
-								run.loadInferredTopic(
+								Run.loadInferredTopic(
 										inferTopicForTrainingPath,
 										queryListForTraining);
 							}
@@ -166,17 +166,17 @@ public class b_f_individule_precision {
 							// infer topic from term_tag_index
 							if (termTagIndex) {
 								TermTagIndex tti = new TermTagIndex();
-								tti.LoadIndexFromFile(root
+								tti.loadIndexFromFile(root
 										+ "term_tag_index.txt");
 								for (Query q : queryListForTesting) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
 								}
 
 								for (Query q : queryListForTraining) {
-									tti.AssignTags(
+									tti.assignTags(
 											q,
 											TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 											termTagIndex_topK);
@@ -324,7 +324,7 @@ public class b_f_individule_precision {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -333,7 +333,7 @@ public class b_f_individule_precision {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -374,7 +374,7 @@ public class b_f_individule_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -385,7 +385,7 @@ public class b_f_individule_precision {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -422,8 +422,8 @@ public class b_f_individule_precision {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -484,8 +484,8 @@ public class b_f_individule_precision {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -509,7 +509,7 @@ public class b_f_individule_precision {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -518,7 +518,7 @@ public class b_f_individule_precision {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -558,7 +558,7 @@ public class b_f_individule_precision {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/outputTag_distributions_with_two_approaches.java b/tag_recommend_code/src/tagRecommend/outputTag_distributions_with_two_approaches.java
index a5a095e..2a102ab 100644
--- a/tag_recommend_code/src/tagRecommend/outputTag_distributions_with_two_approaches.java
+++ b/tag_recommend_code/src/tagRecommend/outputTag_distributions_with_two_approaches.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import topicinfer.*;
 
 import edu.stanford.nlp.tmt.model.llda.CVB0LabeledLDA;
@@ -44,7 +44,7 @@ public class outputTag_distributions_with_two_approaches {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 		// TODO Auto-generated method stub
@@ -79,8 +79,8 @@ public class outputTag_distributions_with_two_approaches {
 							tenCrossValidation.crossNumber, TestDir);
 				}
 
-				HashMap<String, Double> result = new HashMap();
-				HashMap<String,Double> result_10= new HashMap();
+				HashMap<String, Double> result = new HashMap<>();
+				HashMap<String,Double> result_10= new HashMap<>();
 				// perform cross validation
 				try {
 					for (int i = 0; i < crossNumber; i++) {
@@ -110,7 +110,7 @@ public class outputTag_distributions_with_two_approaches {
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								trainingData, inferTopicForTrainingPath,
 								infer_topK);
-						ArrayList<Query> queryListForTraining = run
+						ArrayList<Query> queryListForTraining = Run
 								.loadQueryFromFile(inferTopicForTrainingPath,
 										trainingData);
 
@@ -120,7 +120,7 @@ public class outputTag_distributions_with_two_approaches {
 								+ "\\query-testing-out.csv";
 						TopicInfer.getTopKTopic(model, outputModelDir,
 								testData, inferTopicPathForTesting, infer_topK);
-						ArrayList<Query> queryListForTesting = run
+						ArrayList<Query> queryListForTesting = Run
 								.loadQueryFromFile(inferTopicPathForTesting,
 										testData);
 
@@ -149,9 +149,9 @@ public class outputTag_distributions_with_two_approaches {
 						// infer topic llda
 						System.out.println("topicinfering...");
 						if (topicInfer) {
-							run.loadInferredTopic(inferTopicPathForTesting,
+							Run.loadInferredTopic(inferTopicPathForTesting,
 									queryListForTesting);
-							run.loadInferredTopic(inferTopicForTrainingPath,
+							Run.loadInferredTopic(inferTopicForTrainingPath,
 									queryListForTraining);
 						}
 
@@ -159,15 +159,15 @@ public class outputTag_distributions_with_two_approaches {
 						System.out.println("term_tag inferring...");
 						if (termTagIndex) {
 							TermTagIndex tti = new TermTagIndex();
-							tti.LoadIndexFromFile(root + "term_tag_index.txt");
+							tti.loadIndexFromFile(root + "term_tag_index.txt");
 							for (Query q : queryListForTesting) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
 
 							for (Query q : queryListForTraining) {
-								tti.AssignTags(q,
+								tti.assignTags(q,
 										TermTagIndex.INFER_NON_UNIQUE_TOKEN,
 										termTagIndex_topK);
 							}
@@ -284,7 +284,7 @@ public class outputTag_distributions_with_two_approaches {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -293,7 +293,7 @@ public class outputTag_distributions_with_two_approaches {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -334,7 +334,7 @@ public class outputTag_distributions_with_two_approaches {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -345,7 +345,7 @@ public class outputTag_distributions_with_two_approaches {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -382,8 +382,8 @@ public class outputTag_distributions_with_two_approaches {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for (Doc d : remainingTest.values())
@@ -444,8 +444,8 @@ public class outputTag_distributions_with_two_approaches {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -469,7 +469,7 @@ public class outputTag_distributions_with_two_approaches {
 					else
 						System.out.println(tag + "," + id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -478,7 +478,7 @@ public class outputTag_distributions_with_two_approaches {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				for (String docId : tmp.get(key)) {
 					docmap.put(docId, "1");
 				}
@@ -518,7 +518,7 @@ public class outputTag_distributions_with_two_approaches {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/tenCrossValidation.java b/tag_recommend_code/src/tagRecommend/tenCrossValidation.java
index e48bcf3..5fe850d 100644
--- a/tag_recommend_code/src/tagRecommend/tenCrossValidation.java
+++ b/tag_recommend_code/src/tagRecommend/tenCrossValidation.java
@@ -18,7 +18,7 @@ import Network.generateNetwork;
 import TermTagIndex.TermTagIndex;
 import TermTagIndex.TermTagIndexBuilder;
 import queryExpansion.Query;
-import queryExpansion.run;
+import queryExpansion.Run;
 import scala.annotation.target.setter;
 import topicinfer.*;
 
@@ -53,7 +53,7 @@ public class tenCrossValidation {
 	static String train = "trainDataset_distr.csv";
 	static String test = "testDataset.csv";
 	static String golden = "goldenSet.csv";
-	static ArrayList<Tag> tag_doc = new ArrayList();
+	static ArrayList<Tag> tag_doc = new ArrayList<>();
 
 	public static void main(String[] args) {
 	   // set the sample data path
@@ -74,7 +74,7 @@ public class tenCrossValidation {
 				generateTestData_Distribution(inputData, tenCrossValidation.crossNumber,
 					TestDir);
 		}
-		HashMap<String, Double> result = new HashMap();
+		HashMap<String, Double> result = new HashMap<>();
 		
 		// perform cross validation
 		try {
@@ -100,7 +100,7 @@ public class tenCrossValidation {
 				String inferTopicForTrainingPath = currentTestDir + "\\query-training-out.csv";
 				TopicInfer.getTopKTopic(model,outputModelDir, trainingData,
 						inferTopicForTrainingPath, infer_topk);
-				ArrayList<Query> queryListForTraining = run
+				ArrayList<Query> queryListForTraining = Run
 						.loadQueryFromFile(inferTopicForTrainingPath, trainingData);
 				
 				// construct testing query
@@ -108,7 +108,7 @@ public class tenCrossValidation {
 				String inferTopicPathForTesting = currentTestDir + "\\query-testing-out.csv";
 				TopicInfer.getTopKTopic(model,outputModelDir, testData,
 						inferTopicPathForTesting, infer_topk);
-				ArrayList<Query> queryListForTesting = run
+				ArrayList<Query> queryListForTesting = Run
 						.loadQueryFromFile(inferTopicPathForTesting, testData);
 				
 			
@@ -134,20 +134,20 @@ public class tenCrossValidation {
 				
 				// infer topic llda
 				if(topicInfer){
-					run.loadInferredTopic(inferTopicPathForTesting, queryListForTesting);
-					run.loadInferredTopic(inferTopicForTrainingPath, queryListForTraining);
+					Run.loadInferredTopic(inferTopicPathForTesting, queryListForTesting);
+					Run.loadInferredTopic(inferTopicForTrainingPath, queryListForTraining);
 				}
 				
 				// infer topic from term_tag_index
 				if(termTagIndex){
 					TermTagIndex tti = new TermTagIndex(); 
-					tti.LoadIndexFromFile(root+"term_tag_index.txt");
+					tti.loadIndexFromFile(root+"term_tag_index.txt");
 					for(Query q : queryListForTesting){
-						tti.AssignTags(q,TermTagIndex.INFER_NON_UNIQUE_TOKEN,termTagIndex_topK);
+						tti.assignTags(q,TermTagIndex.INFER_NON_UNIQUE_TOKEN,termTagIndex_topK);
 					}
 					
 					for(Query q : queryListForTraining){
-						tti.AssignTags(q,TermTagIndex.INFER_NON_UNIQUE_TOKEN,termTagIndex_topK);
+						tti.assignTags(q,TermTagIndex.INFER_NON_UNIQUE_TOKEN,termTagIndex_topK);
 					}
 					
 				}
@@ -285,7 +285,7 @@ public class tenCrossValidation {
 	}
 
 	private static HashMap<String, Doc> loadGoldset(String path) {
-		HashMap<String, Doc> result = new HashMap();
+		HashMap<String, Doc> result = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(path));
 			String line = null;
@@ -294,7 +294,7 @@ public class tenCrossValidation {
 				String[] sparts = line.split(",");
 				Doc d = new Doc(sparts[0]);
 				String[] tagstr = sparts[1].split(" ");
-				ArrayList<String> tags = new ArrayList();
+				ArrayList<String> tags = new ArrayList<>();
 				for (int i = 0; i < tagstr.length; i++) {
 					tags.add(tagstr[i]);
 				}
@@ -335,7 +335,7 @@ public class tenCrossValidation {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 			// random pick test set
-			ArrayList<Doc> currentTestset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
 			for (int j = 0; j < testCaseNumberPerCross[i]; j++) {
 				int random = (int) (Math.random() * remainingTest.size());
 				// System.out.println(i+":" + random);
@@ -346,7 +346,7 @@ public class tenCrossValidation {
 				remainingTest.remove(random);
 			}
 			// get training set
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			for (int j = 0; j < docList.size(); j++) {
 				if (!currentTestset.contains(docList.get(j))) {
 					currentTrainset.add(docList.get(j));
@@ -383,8 +383,8 @@ public class tenCrossValidation {
 			if (!new File(subTestDir).isDirectory())
 				new File(subTestDir).mkdirs();
 
-			ArrayList<Doc> currentTestset = new ArrayList();
-			ArrayList<Doc> currentTrainset = new ArrayList();
+			ArrayList<Doc> currentTestset = new ArrayList<>();
+			ArrayList<Doc> currentTrainset = new ArrayList<>();
 			if (i == crossnumber - 1) {
 				// dump the remained doc to test case
 				for(Doc d: remainingTest.values())
@@ -449,8 +449,8 @@ public class tenCrossValidation {
 	}
 
 	private static HashMap<String, Doc> loadDatasetbasedOnTag(String dataset) {
-		HashMap<String, Doc> list = new HashMap();
-		HashMap<String, ArrayList<String>> tmp = new HashMap();
+		HashMap<String, Doc> list = new HashMap<>();
+		HashMap<String, ArrayList<String>> tmp = new HashMap<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
@@ -474,7 +474,7 @@ public class tenCrossValidation {
 					else
 						System.out.println(tag+","+id);
 				} else {
-					ArrayList<String> newlist = new ArrayList();
+					ArrayList<String> newlist = new ArrayList<>();
 					newlist.add(id);
 					tmp.put(tag, newlist);
 				}
@@ -483,7 +483,7 @@ public class tenCrossValidation {
 			br.close();
 			// dump to tag_doc
 			for (String key : tmp.keySet()) {
-				HashMap<String, String> docmap = new HashMap();
+				HashMap<String, String> docmap = new HashMap<>();
 				 for(String docId : tmp.get(key)){
 					 docmap.put(docId, "1");
 				 }
@@ -523,7 +523,7 @@ public class tenCrossValidation {
 
 	private static ArrayList<Doc> loadDataset(String dataset) {
 		// TODO Auto-generated method stub
-		ArrayList<Doc> list = new ArrayList();
+		ArrayList<Doc> list = new ArrayList<>();
 		try {
 			BufferedReader br = new BufferedReader(new FileReader(dataset));
 			String line = null;
diff --git a/tag_recommend_code/src/tagRecommend/test.scala b/tag_recommend_code/src/tagRecommend/test.scala
index 547b14d..38d9ff4 100644
--- a/tag_recommend_code/src/tagRecommend/test.scala
+++ b/tag_recommend_code/src/tagRecommend/test.scala
@@ -1,9 +1,9 @@
-package tagRecommend
-
-object test {
-
-  def main(args: Array[String]): Unit = {
-    println("help");
-  }
-
-}
\ No newline at end of file
+//package tagRecommend
+//
+//object test {
+//
+//  def main(args: Array[String]): Unit = {
+//    println("help");
+//  }
+//
+//}
\ No newline at end of file
diff --git a/tag_recommend_code/src/topicinfer/TopicInfer.scala b/tag_recommend_code/src/topicinfer/TopicInfer.scala
index 9cbefb9..61ee57a 100644
--- a/tag_recommend_code/src/topicinfer/TopicInfer.scala
+++ b/tag_recommend_code/src/topicinfer/TopicInfer.scala
@@ -1,5 +1,6 @@
 package topicinfer
 
+
 import scalanlp.io._
 import scalanlp.stage._
 import scalanlp.stage.text._
@@ -14,7 +15,7 @@ import java.io.PrintWriter
 
 
 
-case class Topic(name :String, score :Double) {
+class Topic(name :String, score :Double) {
 	override def toString  = name +":" +score
 	var Name :String = name;
 	var Score :Double = score;
